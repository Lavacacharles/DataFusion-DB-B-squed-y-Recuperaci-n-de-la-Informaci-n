{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Proyecto jiji"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: kagglehub in c:\\users\\ce mar\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (0.3.4)\n",
      "Requirement already satisfied: requests in c:\\users\\ce mar\\appdata\\roaming\\python\\python310\\site-packages (from kagglehub) (2.28.1)\n",
      "Requirement already satisfied: tqdm in c:\\users\\ce mar\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from kagglehub) (4.66.1)\n",
      "Requirement already satisfied: packaging in c:\\users\\ce mar\\appdata\\roaming\\python\\python310\\site-packages (from kagglehub) (23.2)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in c:\\users\\ce mar\\appdata\\roaming\\python\\python310\\site-packages (from requests->kagglehub) (2.1.1)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\ce mar\\appdata\\roaming\\python\\python310\\site-packages (from requests->kagglehub) (1.26.13)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\ce mar\\appdata\\roaming\\python\\python310\\site-packages (from requests->kagglehub) (2022.12.7)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\ce mar\\appdata\\roaming\\python\\python310\\site-packages (from requests->kagglehub) (3.4)\n",
      "Requirement already satisfied: colorama in c:\\users\\ce mar\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from tqdm->kagglehub) (0.4.6)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip available: 22.2.1 -> 24.3.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "!pip install kagglehub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to C:\\Users\\Ce\n",
      "[nltk_data]     mar\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "import numpy as np\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "nltk.download('punkt')\n",
    "import regex as re\n",
    "import os\n",
    "import pandas as pd\n",
    "stemmer = SnowballStemmer('english')\n",
    "import os\n",
    "import pickle\n",
    "import sys\n",
    "import math\n",
    "import dbm\n",
    "import time\n",
    "import heapq\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"stopwords-en.txt\", encoding=\"latin1\") as file:\n",
    "   stoplist = [line.rstrip().lower() for line in file]\n",
    "stoplist += ['?', '-', '.', ':', ',', '!', ';']\n",
    "\n",
    "def preprocesamiento(texto, stemming=True):\n",
    "  words = []\n",
    "  texto = str(texto)\n",
    "  texto = texto.lower()\n",
    "  texto = re.sub(r'[^a-zA-Z0-9_À-ÿ]', ' ', texto)\n",
    "  # tokenizar\n",
    "  words = nltk.word_tokenize(texto, language='english')\n",
    "  # filtrar stopwords\n",
    "  words = [word for word in words if word not in stoplist]\n",
    "  # reducir palabras (stemming)\n",
    "  if stemming:\n",
    "      words = [stemmer.stem(word) for word in words]\n",
    "  return words\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Funciones AUXILIARES para recuperar la información\n",
    "\n",
    "def getNumberWithAtributo(dataset_head, atributo):\n",
    "    if atributo in dataset_head.columns:\n",
    "        return dataset_head.columns.get_loc(atributo)  # Obtiene la posición del atributo\n",
    "    return -1  # Retorna -1 si no existe\n",
    " \n",
    "\n",
    "def getResultados(result, path, disk_limit):\n",
    "    res = []\n",
    "    for chunk in pd.read_csv(path, chunksize=disk_limit):\n",
    "        for doc, score in result:\n",
    "            if doc in chunk.index:\n",
    "                res.append((chunk.iloc[doc], score))\n",
    "    return pd.DataFrame(res, columns=['doc_id', 'score'])  \n",
    "def getResultadosDF(result, path, disk_limit):\n",
    "    res = []\n",
    "    for chunk in pd.read_csv(path, chunksize=disk_limit):\n",
    "        for doc, score in result:\n",
    "            row = chunk.iloc[doc].copy()\n",
    "            row['score'] = score  \n",
    "            res.append(row)\n",
    "    return pd.DataFrame(res) \n",
    "def get_dfTex_Cols(path, row, columnas): #We don´t use it is only ofr simple an very fast testing\n",
    "    dataset =  pd.read_csv(path)\n",
    "    res = [dataset.iloc[row, i] for i in columnas]\n",
    "    return ' '.join(res)  \n",
    "#Funciones auxiliares para el mergeo:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creando y ordenando cada bloque\n",
      "Iniciando con el merge\n",
      "Índice fusionado guardado en index_blocks_merge\\postings.bin\n",
      "Diccionario de términos guardado en index_blocks_merge\\term_dict.pkl\n",
      "Normas de documentos guardadas en index_blocks_merge\\document_norms.pkl\n",
      "\n",
      "Resumen de tiempos:\n",
      "Tiempo para la creación Creación  y el ordenamiento de los bloques: 28.40 segundos\n",
      "Tiempo para la ejecución del merge: 1.45 segundos\n",
      "Tiempo total para la creación y el mergeo: 29.85 segundos\n",
      "Se procesaron en total:   2000\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "class SPIMI:\n",
    "    def __init__(self, index_dataset_path=\"path\",bloques_dir=\"index_blocks\" , columnas = [\"track_name\",\"track_artist\",\"lyrics\", \"track_album_name\"]):\n",
    "        self.bloques_dir = bloques_dir  \n",
    "        self.dataset_path = index_dataset_path  \n",
    "        self.block_counter = 0      \n",
    "        self.doc_ids = None  \n",
    "        self.idf = {}\n",
    "        self.length = {}\n",
    "        self.disk_limit = 40000  \n",
    "        \n",
    "        #--Columnas:\n",
    "        self.columnas = columnas \n",
    "        # ----Direciones de memoria para los files--\n",
    "        self.dirIndex = self.bloques_dir + \"_merge\"\n",
    "        \n",
    "        self.term_dict_path = os.path.join(self.dirIndex, 'term_dict.pkl')\n",
    "        self.postings_file_path = os.path.join(self.dirIndex, 'postings.bin')\n",
    "        self.norms_file_path = os.path.join(self.dirIndex, 'document_norms.pkl')\n",
    "        #Guardar los ids\n",
    "        self.doc_count_path = os.path.join(self.dirIndex, 'doc_count.txt')  \n",
    "\n",
    "        if not os.path.exists(self.dirIndex):\n",
    "            os.makedirs(self.dirIndex)\n",
    "        if not os.path.exists(self.bloques_dir):\n",
    "            os.makedirs(self.bloques_dir)\n",
    "\n",
    "        if os.path.exists(self.doc_count_path): # Guardamos el total de docs en ves de usar un set\n",
    "            with open(self.doc_count_path, 'r') as f:\n",
    "                self.doc_ids = int(f.read().strip())\n",
    "\n",
    "        self.load_doc_ids()   \n",
    "\n",
    "    def load_doc_ids(self):\n",
    "        if os.path.exists(self.doc_count_path):\n",
    "            try:\n",
    "                with open(self.doc_count_path, 'r') as f:\n",
    "                    self.doc_ids = int(f.read().strip())\n",
    "                    print(f\"Documentos cargados: {self.doc_ids}\")\n",
    "            except (ValueError, FileNotFoundError):\n",
    "                self.doc_ids = 0  # Si el archivo está vacío o corrupto\n",
    "        else:\n",
    "            self.doc_ids = 0\n",
    " \n",
    "       \n",
    "        \n",
    "                  \n",
    "\n",
    "    def spimi_invert(self):\n",
    "        print(\"Creando y ordenando cada bloque\")\n",
    "        dictionary = {}\n",
    "        doc_ids = set()\n",
    "        time_spimiInvert_start = time.time()\n",
    "\n",
    "        #Get columna indeces: \n",
    "        with open(self.dataset_path, mode='r', encoding='utf-8') as file:\n",
    "            primera_linea = file.readline().strip()\n",
    "        #Get number columns\n",
    "        columnas = primera_linea.split(',')\n",
    "        columnas_numbers  = [i for i in range(len(columnas)) if columnas[i] in self.columnas]\n",
    "\n",
    "        # Creamos los diccionarios a los bloques\n",
    "        for chunk in pd.read_csv(self.dataset_path,chunksize= self.disk_limit): \n",
    "            for doc_id_, row in chunk.iterrows():\n",
    "                self.doc_ids +=1 # Actualizamos la cantidad de id\n",
    "                preFila = [str(row.iloc[i]) for i in columnas_numbers]\n",
    "                texto = ' '.join(item for item in preFila)\n",
    "                words = preprocesamiento(texto)\n",
    "                for text in words:\n",
    "                    doc_id = doc_id_\n",
    "                    token = text\n",
    "                    if token not in dictionary:\n",
    "                        dictionary[token] = {}  \n",
    "\n",
    "                    if doc_id not in dictionary[token]:\n",
    "                        dictionary[token][doc_id] = 1  \n",
    "                    else:\n",
    "                        dictionary[token][doc_id] += 1  \n",
    "\n",
    "                    dictionary_size = sys.getsizeof(dictionary)\n",
    "                    if dictionary_size >= self.disk_limit:\n",
    "                        self.write_block_to_disk(dictionary)\n",
    "                        dictionary.clear()\n",
    "\n",
    "        if dictionary:\n",
    "            self.write_block_to_disk(dictionary)  \n",
    "        \n",
    "        with open(self.doc_count_path, 'w') as f:\n",
    "            f.write(str(self.doc_ids))\n",
    "\n",
    "        time_spimiInvert = time.time()- time_spimiInvert_start\n",
    "\n",
    "        # MERGE\n",
    "        print(\"Iniciando con el merge\")\n",
    "        time_Merge_start = time.time()\n",
    "        self.mergeHeap()\n",
    "        time_Merge = time.time() - time_Merge_start\n",
    "\n",
    "\n",
    "        print(\"\\nResumen de tiempos:\")\n",
    "        print(f\"Tiempo para la creación Creación  y el ordenamiento de los bloques: {time_spimiInvert:.2f} segundos\")\n",
    "        print(f\"Tiempo para la ejecución del merge: {time_Merge:.2f} segundos\")\n",
    "        print(f\"Tiempo total para la creación y el mergeo: {time_Merge + time_spimiInvert:.2f} segundos\")\n",
    "    \n",
    "\n",
    "\n",
    "    def write_block_to_disk(self, dictionary): # Guardo los ordenados\n",
    "        # TO DO\n",
    "        # Probar con guardar y luego ordenar con hilos\n",
    "        sorted_terms = dict(sorted(dictionary.items())) \n",
    "        file_path = os.path.join(self.bloques_dir, f\"block_{self.block_counter}.txt\")\n",
    "        with open(file_path, \"w\", encoding=\"utf-8\") as f:\n",
    "            for term, postings in sorted_terms.items():\n",
    "                postings_str = json.dumps(postings)  \n",
    "                f.write(f\"{term}: {postings_str}\\n\")  \n",
    "        self.block_counter += 1\n",
    "\n",
    "\n",
    "    def load_block(self, filepath):\n",
    "        with open(filepath, \"rb\") as f:\n",
    "            return pickle.load(f)\n",
    "    def load_term_dict(self):\n",
    "        term_dict = {}\n",
    "        with open(self.term_dict_path, 'rb') as f:\n",
    "            while True:\n",
    "                try:\n",
    "                    term_entry = pickle.load(f)\n",
    "                    term_dict.update(term_entry)\n",
    "                except EOFError:\n",
    "                    break\n",
    "        return term_dict\n",
    "\n",
    "    def show_Block(self, index):\n",
    "        print(\"Imprimiendo contenido del bloque: \", index)\n",
    "        file_path = os.path.join(self.bloques_dir, f'block_{index}.pkl')\n",
    "        if not os.path.exists(file_path):\n",
    "            print(f\"El bloque {index} no existe en la ruta: {file_path}\")\n",
    "            return\n",
    "        try:\n",
    "            with open(file_path, 'rb') as f:\n",
    "                block_content = pickle.load(f)\n",
    "            print(block_content)\n",
    "        except Exception as e:\n",
    "            print(f\"Error al abrir el bloque {index}: {e}\")\n",
    "\n",
    "\n",
    "\n",
    "    # Ya se comprobó el correcto merge \n",
    "    # Ahora los archivo son terminos postings y salto de linea, asi se getea linea por línea \n",
    "    def mergeHeap(self):\n",
    "        lista_bloques = os.listdir(self.bloques_dir)\n",
    "        priority_queue = []\n",
    "        idx = 0  # Índice único para asegurar comparabilidad en el heap\n",
    "    \n",
    "        postings_file_path = os.path.join(self.dirIndex, 'postings.bin')\n",
    "        term_dict_path = os.path.join(self.dirIndex, 'term_dict.pkl')\n",
    "        norms_file_path = os.path.join(self.dirIndex, 'document_norms.pkl')\n",
    "    \n",
    "        open_files = {}\n",
    "        doc_norms_temp = {}  # Diccionario temporal para acumular las normas de cada documento\n",
    "    \n",
    "        try:\n",
    "            for bloque_path in lista_bloques:  # Referencias de bloques y pusheo de una línea (term, postings)\n",
    "                bloque_full_path = os.path.join(self.bloques_dir, bloque_path)\n",
    "                open_files[bloque_path] = open(bloque_full_path, \"r\", encoding=\"utf-8\")\n",
    "                term_postings = self.read_next_term(open_files[bloque_path])\n",
    "                if term_postings:\n",
    "                    term, postings = term_postings\n",
    "                    heapq.heappush(priority_queue, (term, idx, postings, bloque_path))\n",
    "                    idx += 1\n",
    "    \n",
    "            current_term = None\n",
    "            current_postings = {}\n",
    "            postings_file_position = 0\n",
    "    \n",
    "            with open(postings_file_path, 'wb') as postings_file, \\\n",
    "                 open(term_dict_path, 'ab') as term_dict_file:\n",
    "    \n",
    "                while priority_queue:\n",
    "                    term, _, postings, bloque_path = heapq.heappop(priority_queue)\n",
    "                    file = open_files[bloque_path]\n",
    "    \n",
    "                    if term == current_term:  # Caso en que el término es igual\n",
    "                        for doc_id, tf in postings.items():\n",
    "                            current_postings[doc_id] = current_postings.get(doc_id, 0) + tf\n",
    "                    else:  # Nuevo término\n",
    "                        if current_term is not None:\n",
    "                            # Calcular IDF\n",
    "                            idf = math.log10(self.doc_ids / (1 + len(current_postings)))\n",
    "    \n",
    "                            # Guardar postings (TF e IDF) en postings.bin\n",
    "                            tf_idf_postings = {}\n",
    "                            for doc_id, tf in current_postings.items():\n",
    "                                tf_weighted = math.log10(1 + tf)\n",
    "                                tfidf = tf_weighted * idf\n",
    "                                tf_idf_postings[doc_id] = {\"tf\": tf_weighted, \"idf\": idf}\n",
    "    \n",
    "                                # Acumular el cuadrado del TF-IDF en las normas temporales\n",
    "                                doc_norms_temp[doc_id] = doc_norms_temp.get(doc_id, 0) + tfidf ** 2\n",
    "    \n",
    "                            postings_data = pickle.dumps(tf_idf_postings)\n",
    "                            postings_file.write(postings_data)\n",
    "    \n",
    "                            # Guardar término en term_dict.pkl\n",
    "                            pickle.dump({current_term: (postings_file_position, len(postings_data))}, term_dict_file)\n",
    "                            postings_file_position += len(postings_data)\n",
    "    \n",
    "                        # Actualizar current_term y current_postings\n",
    "                        current_term = term\n",
    "                        current_postings = postings.copy()\n",
    "    \n",
    "                    next_term_postings = self.read_next_term(file)\n",
    "                    if next_term_postings:\n",
    "                        next_term, next_postings = next_term_postings\n",
    "                        heapq.heappush(priority_queue, (next_term, idx, next_postings, bloque_path))\n",
    "                        idx += 1\n",
    "    \n",
    "                # Procesar el último término\n",
    "                if current_term is not None:\n",
    "                    idf = math.log10(self.doc_ids / (1 + len(current_postings)))\n",
    "                    tf_idf_postings = {}\n",
    "                    for doc_id, tf in current_postings.items():\n",
    "                        tf_weighted = math.log10(1 + tf)\n",
    "                        tfidf = tf_weighted * idf\n",
    "                        tf_idf_postings[doc_id] = {\"tf\": tf_weighted, \"idf\": idf}\n",
    "    \n",
    "                        # Acumular el cuadrado del TF-IDF en las normas temporales\n",
    "                        doc_norms_temp[doc_id] = doc_norms_temp.get(doc_id, 0) + tfidf ** 2\n",
    "    \n",
    "                    postings_data = pickle.dumps(tf_idf_postings)\n",
    "                    postings_file.write(postings_data)\n",
    "                    pickle.dump({current_term: (postings_file_position, len(postings_data))}, term_dict_file)\n",
    "                    postings_file_position += len(postings_data)\n",
    "    \n",
    "        finally:\n",
    "            # Cerrar todos los archivos abiertos\n",
    "            for file in open_files.values():\n",
    "                file.close()\n",
    "    \n",
    "        # Guardar las normas finales tomando la raíz cuadrada\n",
    "        final_norms = {int(doc_id): math.sqrt(value) for doc_id, value in doc_norms_temp.items()}\n",
    "        with open(norms_file_path, 'wb') as norms_file:\n",
    "            pickle.dump(final_norms, norms_file)\n",
    "    \n",
    "        self.term_dict_path = term_dict_path\n",
    "        self.postings_file_path = postings_file_path\n",
    "        print(f\"Índice fusionado guardado en {postings_file_path}\")\n",
    "        print(f\"Diccionario de términos guardado en {term_dict_path}\")\n",
    "        print(f\"Normas de documentos guardadas en {norms_file_path}\")\n",
    "\n",
    "\n",
    "            # Realiza merge de todos los bloques:\n",
    "            # Manejaremos un archivo para ubicar los un indice general self.indexTerms donde se guarda la posición del termino\n",
    "            # En el otro archivo se guardaran completito tanto la palabra como el posting list pero ya ordenado\n",
    "    \n",
    "    def read_next_term(self, file):\n",
    "        line = file.readline()\n",
    "        if not line:  # Si llegamos al final del archivo\n",
    "            return None\n",
    "        term, postings_str = line.strip().split(\":\", 1)\n",
    "        postings = json.loads(postings_str)  # Convertir la lista de postings de JSON a un diccionario\n",
    "        return term, postings\n",
    "\n",
    "\n",
    "\n",
    "    #--Funciones para testing--\n",
    "    def print_all_norms(self):\n",
    "        if not os.path.exists(self.norms_file_path):\n",
    "            print(\"El archivo de normas no existe. Asegúrate de haber ejecutado 'spimi_invert' correctamente.\")\n",
    "            return\n",
    "\n",
    "        try:\n",
    "            with open(self.norms_file_path, 'rb') as norms_file:\n",
    "                norms = pickle.load(norms_file)\n",
    "                if not norms:\n",
    "                    print(\"El archivo de normas está vacío.\")\n",
    "                    return\n",
    "\n",
    "                print(\"Normas de los documentos:\")\n",
    "                for doc_id, norm in sorted(norms.items()):\n",
    "                    print(f\"Documento ID: {doc_id}, Norma: {norm}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error al leer o procesar las normas: {e}\")\n",
    "\n",
    "    def show_terms_and_positions(self):\n",
    "        term_dict = self.load_term_dict()\n",
    "        print(\"Términos y sus posiciones en el archivo de postings:\")\n",
    "\n",
    "        # for term, (position, length) in term_dict.items():\n",
    "        #     print(f\"Término: '{term}', Posición: {position}, Longitud: {length}\")\n",
    "\n",
    "        terms = list(term_dict.keys())\n",
    "        if terms == sorted(terms):\n",
    "            print(\"----->Yeeeei, Los términos están ordenados alfabéticamente en el diccionario final.\")\n",
    "        else:\n",
    "            print(\"Los términos NO están ordenados alfabéticamente en el diccionario final.\")\n",
    "\n",
    "\n",
    "    def show_terms_with_postings(self):\n",
    "        # Cargar el diccionario de términos desde el archivo term_dict.pkl\n",
    "        term_dict = self.load_term_dict()\n",
    "\n",
    "        if not os.path.exists(self.postings_file_path):\n",
    "            print(\"El archivo de postings no existe. Ejecuta 'mergeHeap' primero.\")\n",
    "            return\n",
    "\n",
    "        print(\"Términos y sus listas de postings:\")\n",
    "        try:\n",
    "            with open(self.postings_file_path, 'rb') as postings_file:\n",
    "                for term, (position, length) in term_dict.items():\n",
    "                    postings_file.seek(position)  \n",
    "                    postings_data = postings_file.read(length) \n",
    "                    postings = pickle.loads(postings_data) \n",
    "                    print(f\"Término: '{term}', Postings: {postings}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error al mostrar los términos y postings: {e}\")\n",
    "\n",
    "    def retrieval(self, query, k):\n",
    "        #Cargamos docs: \n",
    "        if self.doc_ids is None:\n",
    "            self.load_doc_ids()\n",
    "\n",
    "        if self.doc_ids == 0:  \n",
    "            raise ValueError(\"El índice no está construido o no contiene documentos. Ejecuta 'spimi_invert' antes de realizar consultas.\")\n",
    "        print(\"Se cargo en total: \", self.doc_ids, \" documentos\")\n",
    "\n",
    "        # Preprocesar la consulta\n",
    "        terms = preprocesamiento(query)\n",
    "\n",
    "        # Calcular el TF-IDF de la consulta\n",
    "        tf_query = {}\n",
    "        for term in terms:\n",
    "            if term in tf_query:\n",
    "                tf_query[term] += 1\n",
    "            else:\n",
    "                tf_query[term] = 1\n",
    "\n",
    "        tfidf_query = {}\n",
    "        norm_query = 0\n",
    "\n",
    "        term_dict = self.load_term_dict()\n",
    "\n",
    "        for term, tf in tf_query.items():\n",
    "            if term in term_dict:  # Solo considerar términos presentes en el índice\n",
    "                position, length = term_dict[term]\n",
    "                # Calcular IDF basado en postings\n",
    "                with open(self.postings_file_path, 'rb') as postings_file:\n",
    "                    postings_file.seek(position)\n",
    "                    postings_data = postings_file.read(length)\n",
    "                    postings = pickle.loads(postings_data)  # Cargar postings\n",
    "                    idf = math.log10(self.doc_ids / (1 + len(postings)))  # IDF\n",
    "\n",
    "                tfidf_query[term] = math.log10(1 + tf) * idf\n",
    "                norm_query += (tfidf_query[term]) ** 2\n",
    "        norm_query = math.sqrt(norm_query) \n",
    "\n",
    "        scores = [0] * self.doc_ids  \n",
    "\n",
    "        with open(self.postings_file_path, 'rb') as postings_file, \\\n",
    "             open(self.norms_file_path, 'rb') as norms_file:\n",
    "\n",
    "            for term, w_tq in tfidf_query.items():\n",
    "                if term in term_dict:\n",
    "                    position, length = term_dict[term]\n",
    "                    postings_file.seek(position)\n",
    "                    postings_data = postings_file.read(length)\n",
    "                    postings = pickle.loads(postings_data)  # Diccionario {doc_id: {'tf': tf, 'idf': idf}}\n",
    "\n",
    "                    for doc_id_str, values in postings.items():\n",
    "                        doc_id = int(doc_id_str)  # Convertimos la clave a entero\n",
    "                        tfidf_td = values['tf'] * values['idf']\n",
    "                        scores[doc_id] += tfidf_td * w_tq \n",
    "\n",
    "            # Normalizar las puntuaciones por la norma de los documentos\n",
    "            norms_file.seek(0)\n",
    "            norms = pickle.load(norms_file)\n",
    "            # Convertir las claves de normas a enteros\n",
    "            # norms = {int(k): v for k, v in norms.items()}\n",
    "\n",
    "            for doc_id in range(self.doc_ids):\n",
    "                doc_norm = norms.get(doc_id, 0)\n",
    "                # print(f\"Doc ID: {doc_id}, Score before normalization: {scores[doc_id]}, Doc Norm: {doc_norm}, Query Norm: {norm_query}\")\n",
    "                # print(f\"Tipo de doc_id en postings: {type(doc_id)}, Tipo de doc_id en normas: {type(next(iter(norms.keys())))}\")\n",
    "                if doc_norm != 0:\n",
    "                    scores[doc_id] /= (doc_norm * norm_query)\n",
    "\n",
    "        # Ordenar las puntuaciones en orden descendente y devolver los k más relevantes\n",
    "        result = sorted(enumerate(scores), key=lambda x: x[1], reverse=True)\n",
    "        return result[:k]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "path = \"spotify_2000.csv\"\n",
    "# ---------Creation---------\n",
    "s = SPIMI( path)  \n",
    "s.spimi_invert() \n",
    "\n",
    "# ---------End Creation---------\n",
    "print(\"Se procesaron en total:  \", s.doc_ids)\n",
    "\n",
    "\n",
    "#Prubas de merge\n",
    "# s.show_terms_and_positions()\n",
    "# s.show_terms_with_postings()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Se cargo en total:  2000  documentos\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>track_id</th>\n",
       "      <th>track_name</th>\n",
       "      <th>track_artist</th>\n",
       "      <th>lyrics</th>\n",
       "      <th>track_popularity</th>\n",
       "      <th>track_album_id</th>\n",
       "      <th>track_album_name</th>\n",
       "      <th>track_album_release_date</th>\n",
       "      <th>playlist_name</th>\n",
       "      <th>playlist_id</th>\n",
       "      <th>...</th>\n",
       "      <th>mode</th>\n",
       "      <th>speechiness</th>\n",
       "      <th>acousticness</th>\n",
       "      <th>instrumentalness</th>\n",
       "      <th>liveness</th>\n",
       "      <th>valence</th>\n",
       "      <th>tempo</th>\n",
       "      <th>duration_ms</th>\n",
       "      <th>language</th>\n",
       "      <th>score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>00emjlCv9azBN0fzuuyLqy</td>\n",
       "      <td>Dumb Litty</td>\n",
       "      <td>KARD</td>\n",
       "      <td>Get up out of my business You don't keep me fr...</td>\n",
       "      <td>65</td>\n",
       "      <td>7h5X3xhh3peIK9Y0qI5hbK</td>\n",
       "      <td>KARD 2nd Digital Single ‘Dumb Litty’</td>\n",
       "      <td>2019-09-22</td>\n",
       "      <td>K-Party Dance Mix</td>\n",
       "      <td>37i9dQZF1DX4RDXswvP6Mj</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0409</td>\n",
       "      <td>0.03700</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.1380</td>\n",
       "      <td>0.2400</td>\n",
       "      <td>130.018</td>\n",
       "      <td>193160</td>\n",
       "      <td>en</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1571</th>\n",
       "      <td>0PWqC3xDoCWyqy1IW3RiKE</td>\n",
       "      <td>Knockin'</td>\n",
       "      <td>Angel Grant</td>\n",
       "      <td>Kinda cute your look is real fly But what do y...</td>\n",
       "      <td>19</td>\n",
       "      <td>67BjkhyDC5KzhYRiKDVCcX</td>\n",
       "      <td>Album</td>\n",
       "      <td>1998-01-01</td>\n",
       "      <td>Neo Soul Music</td>\n",
       "      <td>78RRvktrPMSqAoCI21mNOe</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0369</td>\n",
       "      <td>0.55700</td>\n",
       "      <td>0.112000</td>\n",
       "      <td>0.0678</td>\n",
       "      <td>0.9080</td>\n",
       "      <td>105.919</td>\n",
       "      <td>294893</td>\n",
       "      <td>en</td>\n",
       "      <td>0.168544</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>755</th>\n",
       "      <td>0fbXFYtHPpq5fVNRGjxZrC</td>\n",
       "      <td>Scrub The Ground (feat. DJ Funk)</td>\n",
       "      <td>Chocolate Puma</td>\n",
       "      <td>I got that work I got that work I got that wor...</td>\n",
       "      <td>58</td>\n",
       "      <td>1wwwzZ0LYiLOct5BdvNpRb</td>\n",
       "      <td>Scrub The Ground (feat. DJ Funk)</td>\n",
       "      <td>2015-10-16</td>\n",
       "      <td>EDM 2019</td>\n",
       "      <td>1T0ed6Mg0QIruHvcoWWIKy</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0518</td>\n",
       "      <td>0.02630</td>\n",
       "      <td>0.812000</td>\n",
       "      <td>0.3380</td>\n",
       "      <td>0.6820</td>\n",
       "      <td>126.982</td>\n",
       "      <td>168937</td>\n",
       "      <td>en</td>\n",
       "      <td>0.137517</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>959</th>\n",
       "      <td>0HUeFoQFiMUPKxFApxyPwL</td>\n",
       "      <td>Euphoria</td>\n",
       "      <td>Destiny Rogers</td>\n",
       "      <td>All these feelings in my mind, I can't get it ...</td>\n",
       "      <td>53</td>\n",
       "      <td>2FHuZw1dqYbMHovcBF0E8L</td>\n",
       "      <td>Euphoria</td>\n",
       "      <td>2019-12-05</td>\n",
       "      <td>New R&amp;B‏‏​​   ‍</td>\n",
       "      <td>4I6rTSxqKl1LRvES2O9owQ</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0617</td>\n",
       "      <td>0.17900</td>\n",
       "      <td>0.000020</td>\n",
       "      <td>0.2080</td>\n",
       "      <td>0.6460</td>\n",
       "      <td>159.989</td>\n",
       "      <td>196749</td>\n",
       "      <td>en</td>\n",
       "      <td>0.136876</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1392</th>\n",
       "      <td>0nJW01T7XtvILxQgC5J7Wh</td>\n",
       "      <td>When I Was Your Man</td>\n",
       "      <td>Bruno Mars</td>\n",
       "      <td>Same bed but it feels just a little bit bigger...</td>\n",
       "      <td>82</td>\n",
       "      <td>58ufpQsJ1DS5kq4hhzQDiI</td>\n",
       "      <td>Unorthodox Jukebox</td>\n",
       "      <td>2012-12-07</td>\n",
       "      <td>Today's Hits 2000-Present</td>\n",
       "      <td>6a66cg3HcsjYkisYyQcov6</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0434</td>\n",
       "      <td>0.93200</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0880</td>\n",
       "      <td>0.3870</td>\n",
       "      <td>72.795</td>\n",
       "      <td>213827</td>\n",
       "      <td>en</td>\n",
       "      <td>0.119040</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1323</th>\n",
       "      <td>0MPegzGbqTvO2tw7BqYUU8</td>\n",
       "      <td>Go On Girl</td>\n",
       "      <td>Ne-Yo</td>\n",
       "      <td>I can't get it back But I don't want it back I...</td>\n",
       "      <td>2</td>\n",
       "      <td>48ben69fRtEbMTGqGYU8Qw</td>\n",
       "      <td>Because Of You</td>\n",
       "      <td>2007-01-01</td>\n",
       "      <td>Ultimate Throwbacks Collection</td>\n",
       "      <td>1dsaMvnC1hXPCNGC4aVtjj</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0.1090</td>\n",
       "      <td>0.15500</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0945</td>\n",
       "      <td>0.5620</td>\n",
       "      <td>83.896</td>\n",
       "      <td>261427</td>\n",
       "      <td>en</td>\n",
       "      <td>0.111420</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1749</th>\n",
       "      <td>0SDgPPSjRPdGD3hyrCTG2S</td>\n",
       "      <td>Don't Worry Bout Me - Alle Farben Remix</td>\n",
       "      <td>Zara Larsson</td>\n",
       "      <td>Everything, everything's cool now I wanted you...</td>\n",
       "      <td>48</td>\n",
       "      <td>3oEW0zWluwima7PXrGi8aF</td>\n",
       "      <td>Don't Worry Bout Me (Remixes)</td>\n",
       "      <td>2019-05-03</td>\n",
       "      <td>Dance Pop Tunes</td>\n",
       "      <td>4SdfG4cPG3skmTiQLozZGh</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0.2640</td>\n",
       "      <td>0.04470</td>\n",
       "      <td>0.000005</td>\n",
       "      <td>0.1780</td>\n",
       "      <td>0.3550</td>\n",
       "      <td>124.938</td>\n",
       "      <td>171914</td>\n",
       "      <td>en</td>\n",
       "      <td>0.100738</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1559</th>\n",
       "      <td>0puf9yIluy9W0vpMEUoAnN</td>\n",
       "      <td>Bang Bang</td>\n",
       "      <td>Jessie J</td>\n",
       "      <td>She got a body like an hourglass But I can giv...</td>\n",
       "      <td>78</td>\n",
       "      <td>2rDIivxBafNKpgRqlzIRSb</td>\n",
       "      <td>Sweet Talker (Deluxe Version)</td>\n",
       "      <td>2014-10-14</td>\n",
       "      <td>10er Playlist</td>\n",
       "      <td>1kEczIkZH8IgaWT2BiApxZ</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0909</td>\n",
       "      <td>0.26000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.3800</td>\n",
       "      <td>0.7490</td>\n",
       "      <td>150.035</td>\n",
       "      <td>199387</td>\n",
       "      <td>en</td>\n",
       "      <td>0.100399</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>179</th>\n",
       "      <td>05hXrqBkWkAdl4kG0iNksE</td>\n",
       "      <td>Feel Me (feat. Kanye West)</td>\n",
       "      <td>Tyga</td>\n",
       "      <td>Yeah, yeah, yeah, yeah, yeah Aye this shit got...</td>\n",
       "      <td>0</td>\n",
       "      <td>3iWD9oq8nfhu3Jta42rzfu</td>\n",
       "      <td>B*tch I'm the Sh*t 2</td>\n",
       "      <td>2017-07-21</td>\n",
       "      <td>RAP Gangsta</td>\n",
       "      <td>1Z1gW89x4MSBjkvVjGg7DQ</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0.1190</td>\n",
       "      <td>0.00362</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.9020</td>\n",
       "      <td>0.1750</td>\n",
       "      <td>142.162</td>\n",
       "      <td>199447</td>\n",
       "      <td>en</td>\n",
       "      <td>0.089845</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>977</th>\n",
       "      <td>0I3wlS5dlpw0hv5zhxN0pz</td>\n",
       "      <td>We Like To Party</td>\n",
       "      <td>RetroVision</td>\n",
       "      <td>Feel like to body play me, feel like to body p...</td>\n",
       "      <td>58</td>\n",
       "      <td>49QK3K9hkhbU3fBdFezvlP</td>\n",
       "      <td>We Like To Party</td>\n",
       "      <td>2019-05-17</td>\n",
       "      <td>Electro House - by Spinnin' Records</td>\n",
       "      <td>4pVZ70y8vzzkn2GVwQbQw8</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0.1010</td>\n",
       "      <td>0.00764</td>\n",
       "      <td>0.017500</td>\n",
       "      <td>0.2170</td>\n",
       "      <td>0.2410</td>\n",
       "      <td>126.046</td>\n",
       "      <td>172381</td>\n",
       "      <td>en</td>\n",
       "      <td>0.088652</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1855</th>\n",
       "      <td>0tBRo4P60DgKmg4jt48upm</td>\n",
       "      <td>Bumpy Ride</td>\n",
       "      <td>Mohombi</td>\n",
       "      <td>I wanna boom bang bang with your body-oWe gonn...</td>\n",
       "      <td>7</td>\n",
       "      <td>2i1sUf4TQEx2aR8a3jF68V</td>\n",
       "      <td>Bumpy Ride</td>\n",
       "      <td>2010-01-01</td>\n",
       "      <td>BALLARE - رقص</td>\n",
       "      <td>1CMvQ4Yr5DlYvYzI0Vc2UE</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0399</td>\n",
       "      <td>0.04700</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0724</td>\n",
       "      <td>0.8260</td>\n",
       "      <td>105.074</td>\n",
       "      <td>224402</td>\n",
       "      <td>en</td>\n",
       "      <td>0.086638</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1374</th>\n",
       "      <td>0NBiC3zLXoBQXBjsbnbwJq</td>\n",
       "      <td>Molly (feat. Brendon Urie of Panic at the Disco)</td>\n",
       "      <td>Lil Dicky</td>\n",
       "      <td>Yeah... man, this is the softest thing I ever ...</td>\n",
       "      <td>70</td>\n",
       "      <td>5eLVoIPq7P3Bu29lVgD4x0</td>\n",
       "      <td>Professional Rapper</td>\n",
       "      <td>2015-07-31</td>\n",
       "      <td>Hip Hop Controller</td>\n",
       "      <td>37i9dQZF1DWT5MrZnPU1zD</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0368</td>\n",
       "      <td>0.20300</td>\n",
       "      <td>0.000156</td>\n",
       "      <td>0.1150</td>\n",
       "      <td>0.3970</td>\n",
       "      <td>114.940</td>\n",
       "      <td>244340</td>\n",
       "      <td>en</td>\n",
       "      <td>0.085237</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1578</th>\n",
       "      <td>0pzWCDSuVRX7m95EFOxVa0</td>\n",
       "      <td>Drunk Dialing...LODT</td>\n",
       "      <td>Summer Walker</td>\n",
       "      <td>NA It's 3:34 and I don't need no more You can ...</td>\n",
       "      <td>69</td>\n",
       "      <td>1qgJNWnPIeK9rx7hF8JCPK</td>\n",
       "      <td>Over It</td>\n",
       "      <td>2019-10-04</td>\n",
       "      <td>Neo-Soul</td>\n",
       "      <td>1eqVgsNjaX6mxDPoefhocT</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0.1780</td>\n",
       "      <td>0.60400</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.2010</td>\n",
       "      <td>0.1120</td>\n",
       "      <td>125.662</td>\n",
       "      <td>134440</td>\n",
       "      <td>en</td>\n",
       "      <td>0.083684</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>979</th>\n",
       "      <td>0i5LBaRAugIBpBMEfNvN0e</td>\n",
       "      <td>Let Your Body Learn</td>\n",
       "      <td>Nitzer Ebb</td>\n",
       "      <td>Fast beat the feet Fast fall the hands Fast be...</td>\n",
       "      <td>16</td>\n",
       "      <td>73t8cKrie06UIq26e9WsD3</td>\n",
       "      <td>That Total Age</td>\n",
       "      <td>1987-01-01</td>\n",
       "      <td>Maxi Pop  GOLD (New Wave, Electropop, Synth Po...</td>\n",
       "      <td>2nRWtTI9a2LWjJ9Wy3JZs5</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0571</td>\n",
       "      <td>0.18000</td>\n",
       "      <td>0.003480</td>\n",
       "      <td>0.1230</td>\n",
       "      <td>0.0506</td>\n",
       "      <td>130.699</td>\n",
       "      <td>168640</td>\n",
       "      <td>en</td>\n",
       "      <td>0.081877</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>82</th>\n",
       "      <td>02jcEwywffn3Tsb48fXmlW</td>\n",
       "      <td>Get the Party Started</td>\n",
       "      <td>P!nk</td>\n",
       "      <td>I'm comin' up, so you better get this party st...</td>\n",
       "      <td>66</td>\n",
       "      <td>57wJKO7qrPw56iOEKhTmg2</td>\n",
       "      <td>M!ssundaztood (Expanded Edition)</td>\n",
       "      <td>2001</td>\n",
       "      <td>post-teen pop</td>\n",
       "      <td>4TvZA7Pml7mHLgbwNomlnm</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0459</td>\n",
       "      <td>0.00108</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.1730</td>\n",
       "      <td>0.9610</td>\n",
       "      <td>128.931</td>\n",
       "      <td>192533</td>\n",
       "      <td>en</td>\n",
       "      <td>0.080184</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>02rbSCTm2smTIZ6d8im2NP</td>\n",
       "      <td>This Is How We Party (with Icona Pop)</td>\n",
       "      <td>R3HAB</td>\n",
       "      <td>I jump the line And you're always waiting for ...</td>\n",
       "      <td>60</td>\n",
       "      <td>43qpL2ImleGrgxCEHLAA4p</td>\n",
       "      <td>This Is How We Party (with Icona Pop)</td>\n",
       "      <td>2019-02-08</td>\n",
       "      <td>Fresh EDM | Progressive House | Electro House ...</td>\n",
       "      <td>0FCHg9zJMNNiOokh3hVcxd</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0.1530</td>\n",
       "      <td>0.05720</td>\n",
       "      <td>0.000001</td>\n",
       "      <td>0.2690</td>\n",
       "      <td>0.3390</td>\n",
       "      <td>118.997</td>\n",
       "      <td>147605</td>\n",
       "      <td>en</td>\n",
       "      <td>0.079299</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1857</th>\n",
       "      <td>0tCD4um9Mbqli92icIS3Jo</td>\n",
       "      <td>Terrified</td>\n",
       "      <td>Terror Jr</td>\n",
       "      <td>Terrified T-Terrified I don't mind being alone...</td>\n",
       "      <td>54</td>\n",
       "      <td>6b2KfLOzzs3nOKKaPbi47J</td>\n",
       "      <td>Unfortunately, Terror Jr</td>\n",
       "      <td>2019-01-25</td>\n",
       "      <td>2019 in Indie Poptimism</td>\n",
       "      <td>16RNbqnNCCLlBJti7JU5nc</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0921</td>\n",
       "      <td>0.35300</td>\n",
       "      <td>0.000004</td>\n",
       "      <td>0.1220</td>\n",
       "      <td>0.6310</td>\n",
       "      <td>115.959</td>\n",
       "      <td>178298</td>\n",
       "      <td>en</td>\n",
       "      <td>0.072990</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1256</th>\n",
       "      <td>0LQmFbjbesF6XNwn427exG</td>\n",
       "      <td>Mad Love - Cheat Codes Remix</td>\n",
       "      <td>Sean Paul</td>\n",
       "      <td>Jiggle up yuh body Jiggle up yuh sinting Love ...</td>\n",
       "      <td>18</td>\n",
       "      <td>6oyGJmzNPyDLcEtzFcAEPC</td>\n",
       "      <td>Mad Love (Remixes)</td>\n",
       "      <td>2018-04-20</td>\n",
       "      <td>Pop EDM Remixes</td>\n",
       "      <td>4aUEH3uhbofktrFkXOOaKj</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0.2540</td>\n",
       "      <td>0.01950</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0776</td>\n",
       "      <td>0.3150</td>\n",
       "      <td>97.630</td>\n",
       "      <td>192245</td>\n",
       "      <td>en</td>\n",
       "      <td>0.069862</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>986</th>\n",
       "      <td>0IBUFi55OCMR5HrDHB1lEo</td>\n",
       "      <td>When Something Is Wrong With My Baby</td>\n",
       "      <td>Sam &amp; Dave</td>\n",
       "      <td>When something is wrong with my baby Something...</td>\n",
       "      <td>1</td>\n",
       "      <td>52Us0gT9wf2yGMNlwTATGB</td>\n",
       "      <td>Dead Presidents Volume II / Music From The Mot...</td>\n",
       "      <td>1995</td>\n",
       "      <td>Sexy Soul 2020</td>\n",
       "      <td>5EMARioe9z9eKOeWIAC2JW</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0296</td>\n",
       "      <td>0.47400</td>\n",
       "      <td>0.000715</td>\n",
       "      <td>0.0929</td>\n",
       "      <td>0.2450</td>\n",
       "      <td>137.529</td>\n",
       "      <td>195867</td>\n",
       "      <td>en</td>\n",
       "      <td>0.068452</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1779</th>\n",
       "      <td>0SKDPFbwzzai7lBD5gwSns</td>\n",
       "      <td>Get Up (Rattle) - Vocal Extended Version</td>\n",
       "      <td>Bingo Players</td>\n",
       "      <td>Yo This house party is crazy My crew is hella ...</td>\n",
       "      <td>24</td>\n",
       "      <td>0U8K1HuwQUmsQV1VqMAAWN</td>\n",
       "      <td>Get Up (Rattle)</td>\n",
       "      <td>2013-01-01</td>\n",
       "      <td>♥ EDM LOVE 2020</td>\n",
       "      <td>6jI1gFr6ANFtT8MmTvA2Ux</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0642</td>\n",
       "      <td>0.00380</td>\n",
       "      <td>0.001560</td>\n",
       "      <td>0.3420</td>\n",
       "      <td>0.4960</td>\n",
       "      <td>127.995</td>\n",
       "      <td>302147</td>\n",
       "      <td>en</td>\n",
       "      <td>0.068346</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>20 rows × 26 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                    track_id  \\\n",
       "4     00emjlCv9azBN0fzuuyLqy   \n",
       "1571  0PWqC3xDoCWyqy1IW3RiKE   \n",
       "755   0fbXFYtHPpq5fVNRGjxZrC   \n",
       "959   0HUeFoQFiMUPKxFApxyPwL   \n",
       "1392  0nJW01T7XtvILxQgC5J7Wh   \n",
       "1323  0MPegzGbqTvO2tw7BqYUU8   \n",
       "1749  0SDgPPSjRPdGD3hyrCTG2S   \n",
       "1559  0puf9yIluy9W0vpMEUoAnN   \n",
       "179   05hXrqBkWkAdl4kG0iNksE   \n",
       "977   0I3wlS5dlpw0hv5zhxN0pz   \n",
       "1855  0tBRo4P60DgKmg4jt48upm   \n",
       "1374  0NBiC3zLXoBQXBjsbnbwJq   \n",
       "1578  0pzWCDSuVRX7m95EFOxVa0   \n",
       "979   0i5LBaRAugIBpBMEfNvN0e   \n",
       "82    02jcEwywffn3Tsb48fXmlW   \n",
       "96    02rbSCTm2smTIZ6d8im2NP   \n",
       "1857  0tCD4um9Mbqli92icIS3Jo   \n",
       "1256  0LQmFbjbesF6XNwn427exG   \n",
       "986   0IBUFi55OCMR5HrDHB1lEo   \n",
       "1779  0SKDPFbwzzai7lBD5gwSns   \n",
       "\n",
       "                                            track_name    track_artist  \\\n",
       "4                                           Dumb Litty            KARD   \n",
       "1571                                          Knockin'     Angel Grant   \n",
       "755                   Scrub The Ground (feat. DJ Funk)  Chocolate Puma   \n",
       "959                                           Euphoria  Destiny Rogers   \n",
       "1392                               When I Was Your Man      Bruno Mars   \n",
       "1323                                        Go On Girl           Ne-Yo   \n",
       "1749           Don't Worry Bout Me - Alle Farben Remix    Zara Larsson   \n",
       "1559                                         Bang Bang        Jessie J   \n",
       "179                         Feel Me (feat. Kanye West)            Tyga   \n",
       "977                                   We Like To Party     RetroVision   \n",
       "1855                                        Bumpy Ride         Mohombi   \n",
       "1374  Molly (feat. Brendon Urie of Panic at the Disco)       Lil Dicky   \n",
       "1578                              Drunk Dialing...LODT   Summer Walker   \n",
       "979                                Let Your Body Learn      Nitzer Ebb   \n",
       "82                               Get the Party Started            P!nk   \n",
       "96               This Is How We Party (with Icona Pop)           R3HAB   \n",
       "1857                                         Terrified       Terror Jr   \n",
       "1256                      Mad Love - Cheat Codes Remix       Sean Paul   \n",
       "986               When Something Is Wrong With My Baby      Sam & Dave   \n",
       "1779          Get Up (Rattle) - Vocal Extended Version   Bingo Players   \n",
       "\n",
       "                                                 lyrics  track_popularity  \\\n",
       "4     Get up out of my business You don't keep me fr...                65   \n",
       "1571  Kinda cute your look is real fly But what do y...                19   \n",
       "755   I got that work I got that work I got that wor...                58   \n",
       "959   All these feelings in my mind, I can't get it ...                53   \n",
       "1392  Same bed but it feels just a little bit bigger...                82   \n",
       "1323  I can't get it back But I don't want it back I...                 2   \n",
       "1749  Everything, everything's cool now I wanted you...                48   \n",
       "1559  She got a body like an hourglass But I can giv...                78   \n",
       "179   Yeah, yeah, yeah, yeah, yeah Aye this shit got...                 0   \n",
       "977   Feel like to body play me, feel like to body p...                58   \n",
       "1855  I wanna boom bang bang with your body-oWe gonn...                 7   \n",
       "1374  Yeah... man, this is the softest thing I ever ...                70   \n",
       "1578  NA It's 3:34 and I don't need no more You can ...                69   \n",
       "979   Fast beat the feet Fast fall the hands Fast be...                16   \n",
       "82    I'm comin' up, so you better get this party st...                66   \n",
       "96    I jump the line And you're always waiting for ...                60   \n",
       "1857  Terrified T-Terrified I don't mind being alone...                54   \n",
       "1256  Jiggle up yuh body Jiggle up yuh sinting Love ...                18   \n",
       "986   When something is wrong with my baby Something...                 1   \n",
       "1779  Yo This house party is crazy My crew is hella ...                24   \n",
       "\n",
       "              track_album_id  \\\n",
       "4     7h5X3xhh3peIK9Y0qI5hbK   \n",
       "1571  67BjkhyDC5KzhYRiKDVCcX   \n",
       "755   1wwwzZ0LYiLOct5BdvNpRb   \n",
       "959   2FHuZw1dqYbMHovcBF0E8L   \n",
       "1392  58ufpQsJ1DS5kq4hhzQDiI   \n",
       "1323  48ben69fRtEbMTGqGYU8Qw   \n",
       "1749  3oEW0zWluwima7PXrGi8aF   \n",
       "1559  2rDIivxBafNKpgRqlzIRSb   \n",
       "179   3iWD9oq8nfhu3Jta42rzfu   \n",
       "977   49QK3K9hkhbU3fBdFezvlP   \n",
       "1855  2i1sUf4TQEx2aR8a3jF68V   \n",
       "1374  5eLVoIPq7P3Bu29lVgD4x0   \n",
       "1578  1qgJNWnPIeK9rx7hF8JCPK   \n",
       "979   73t8cKrie06UIq26e9WsD3   \n",
       "82    57wJKO7qrPw56iOEKhTmg2   \n",
       "96    43qpL2ImleGrgxCEHLAA4p   \n",
       "1857  6b2KfLOzzs3nOKKaPbi47J   \n",
       "1256  6oyGJmzNPyDLcEtzFcAEPC   \n",
       "986   52Us0gT9wf2yGMNlwTATGB   \n",
       "1779  0U8K1HuwQUmsQV1VqMAAWN   \n",
       "\n",
       "                                       track_album_name  \\\n",
       "4                  KARD 2nd Digital Single ‘Dumb Litty’   \n",
       "1571                                              Album   \n",
       "755                    Scrub The Ground (feat. DJ Funk)   \n",
       "959                                            Euphoria   \n",
       "1392                                 Unorthodox Jukebox   \n",
       "1323                                     Because Of You   \n",
       "1749                      Don't Worry Bout Me (Remixes)   \n",
       "1559                      Sweet Talker (Deluxe Version)   \n",
       "179                                B*tch I'm the Sh*t 2   \n",
       "977                                    We Like To Party   \n",
       "1855                                         Bumpy Ride   \n",
       "1374                                Professional Rapper   \n",
       "1578                                            Over It   \n",
       "979                                      That Total Age   \n",
       "82                     M!ssundaztood (Expanded Edition)   \n",
       "96                This Is How We Party (with Icona Pop)   \n",
       "1857                           Unfortunately, Terror Jr   \n",
       "1256                                 Mad Love (Remixes)   \n",
       "986   Dead Presidents Volume II / Music From The Mot...   \n",
       "1779                                    Get Up (Rattle)   \n",
       "\n",
       "     track_album_release_date  \\\n",
       "4                  2019-09-22   \n",
       "1571               1998-01-01   \n",
       "755                2015-10-16   \n",
       "959                2019-12-05   \n",
       "1392               2012-12-07   \n",
       "1323               2007-01-01   \n",
       "1749               2019-05-03   \n",
       "1559               2014-10-14   \n",
       "179                2017-07-21   \n",
       "977                2019-05-17   \n",
       "1855               2010-01-01   \n",
       "1374               2015-07-31   \n",
       "1578               2019-10-04   \n",
       "979                1987-01-01   \n",
       "82                       2001   \n",
       "96                 2019-02-08   \n",
       "1857               2019-01-25   \n",
       "1256               2018-04-20   \n",
       "986                      1995   \n",
       "1779               2013-01-01   \n",
       "\n",
       "                                          playlist_name  \\\n",
       "4                                     K-Party Dance Mix   \n",
       "1571                                     Neo Soul Music   \n",
       "755                                            EDM 2019   \n",
       "959                                     New R&B‏‏​​   ‍   \n",
       "1392                          Today's Hits 2000-Present   \n",
       "1323                     Ultimate Throwbacks Collection   \n",
       "1749                                    Dance Pop Tunes   \n",
       "1559                                      10er Playlist   \n",
       "179                                         RAP Gangsta   \n",
       "977                 Electro House - by Spinnin' Records   \n",
       "1855                                      BALLARE - رقص   \n",
       "1374                                 Hip Hop Controller   \n",
       "1578                                           Neo-Soul   \n",
       "979   Maxi Pop  GOLD (New Wave, Electropop, Synth Po...   \n",
       "82                                        post-teen pop   \n",
       "96    Fresh EDM | Progressive House | Electro House ...   \n",
       "1857                            2019 in Indie Poptimism   \n",
       "1256                                    Pop EDM Remixes   \n",
       "986                                      Sexy Soul 2020   \n",
       "1779                                    ♥ EDM LOVE 2020   \n",
       "\n",
       "                 playlist_id  ... mode speechiness  acousticness  \\\n",
       "4     37i9dQZF1DX4RDXswvP6Mj  ...    1      0.0409       0.03700   \n",
       "1571  78RRvktrPMSqAoCI21mNOe  ...    0      0.0369       0.55700   \n",
       "755   1T0ed6Mg0QIruHvcoWWIKy  ...    0      0.0518       0.02630   \n",
       "959   4I6rTSxqKl1LRvES2O9owQ  ...    0      0.0617       0.17900   \n",
       "1392  6a66cg3HcsjYkisYyQcov6  ...    1      0.0434       0.93200   \n",
       "1323  1dsaMvnC1hXPCNGC4aVtjj  ...    1      0.1090       0.15500   \n",
       "1749  4SdfG4cPG3skmTiQLozZGh  ...    1      0.2640       0.04470   \n",
       "1559  1kEczIkZH8IgaWT2BiApxZ  ...    0      0.0909       0.26000   \n",
       "179   1Z1gW89x4MSBjkvVjGg7DQ  ...    1      0.1190       0.00362   \n",
       "977   4pVZ70y8vzzkn2GVwQbQw8  ...    0      0.1010       0.00764   \n",
       "1855  1CMvQ4Yr5DlYvYzI0Vc2UE  ...    0      0.0399       0.04700   \n",
       "1374  37i9dQZF1DWT5MrZnPU1zD  ...    1      0.0368       0.20300   \n",
       "1578  1eqVgsNjaX6mxDPoefhocT  ...    1      0.1780       0.60400   \n",
       "979   2nRWtTI9a2LWjJ9Wy3JZs5  ...    1      0.0571       0.18000   \n",
       "82    4TvZA7Pml7mHLgbwNomlnm  ...    0      0.0459       0.00108   \n",
       "96    0FCHg9zJMNNiOokh3hVcxd  ...    1      0.1530       0.05720   \n",
       "1857  16RNbqnNCCLlBJti7JU5nc  ...    1      0.0921       0.35300   \n",
       "1256  4aUEH3uhbofktrFkXOOaKj  ...    0      0.2540       0.01950   \n",
       "986   5EMARioe9z9eKOeWIAC2JW  ...    1      0.0296       0.47400   \n",
       "1779  6jI1gFr6ANFtT8MmTvA2Ux  ...    1      0.0642       0.00380   \n",
       "\n",
       "      instrumentalness  liveness  valence    tempo  duration_ms  language  \\\n",
       "4             0.000000    0.1380   0.2400  130.018       193160        en   \n",
       "1571          0.112000    0.0678   0.9080  105.919       294893        en   \n",
       "755           0.812000    0.3380   0.6820  126.982       168937        en   \n",
       "959           0.000020    0.2080   0.6460  159.989       196749        en   \n",
       "1392          0.000000    0.0880   0.3870   72.795       213827        en   \n",
       "1323          0.000000    0.0945   0.5620   83.896       261427        en   \n",
       "1749          0.000005    0.1780   0.3550  124.938       171914        en   \n",
       "1559          0.000000    0.3800   0.7490  150.035       199387        en   \n",
       "179           0.000000    0.9020   0.1750  142.162       199447        en   \n",
       "977           0.017500    0.2170   0.2410  126.046       172381        en   \n",
       "1855          0.000000    0.0724   0.8260  105.074       224402        en   \n",
       "1374          0.000156    0.1150   0.3970  114.940       244340        en   \n",
       "1578          0.000000    0.2010   0.1120  125.662       134440        en   \n",
       "979           0.003480    0.1230   0.0506  130.699       168640        en   \n",
       "82            0.000000    0.1730   0.9610  128.931       192533        en   \n",
       "96            0.000001    0.2690   0.3390  118.997       147605        en   \n",
       "1857          0.000004    0.1220   0.6310  115.959       178298        en   \n",
       "1256          0.000000    0.0776   0.3150   97.630       192245        en   \n",
       "986           0.000715    0.0929   0.2450  137.529       195867        en   \n",
       "1779          0.001560    0.3420   0.4960  127.995       302147        en   \n",
       "\n",
       "         score  \n",
       "4     1.000000  \n",
       "1571  0.168544  \n",
       "755   0.137517  \n",
       "959   0.136876  \n",
       "1392  0.119040  \n",
       "1323  0.111420  \n",
       "1749  0.100738  \n",
       "1559  0.100399  \n",
       "179   0.089845  \n",
       "977   0.088652  \n",
       "1855  0.086638  \n",
       "1374  0.085237  \n",
       "1578  0.083684  \n",
       "979   0.081877  \n",
       "82    0.080184  \n",
       "96    0.079299  \n",
       "1857  0.072990  \n",
       "1256  0.069862  \n",
       "986   0.068452  \n",
       "1779  0.068346  \n",
       "\n",
       "[20 rows x 26 columns]"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\n",
    "# ----------Set Atributo--------\n",
    "query = \"this is my breath\"\n",
    "query = get_dfTex_Cols(path, 4, [1,2,3,6])\n",
    "top_k = 20\n",
    "\n",
    "result = s.retrieval(query, top_k)\n",
    "respuesta = getResultados(result, s.dataset_path, s.disk_limit) # La respuesta es un array pd de [fila del dataframe, score]\n",
    "#-----------End of query------------\n",
    "\n",
    "# s.print_all_norms()\n",
    "getResultadosDF(result, s.dataset_path, s.disk_limit)\n",
    "\n",
    "# print( \"Scores: \")\n",
    "# print( respuestadf[\"score\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# def verificar_termino(tf_file_dir, term):\n",
    "#     with dbm.open(tf_file_dir, \"r\") as tf_db:  \n",
    "#         try:\n",
    "#             term_key = term.encode()\n",
    "#             postings_list = pickle.loads(tf_db[term_key])\n",
    "#             print(f\"Postings list para '{term}': {postings_list}\")\n",
    "#         except KeyError:\n",
    "#             print(f\"El término '{term}' no se encuentra en el archivo DBM.\")\n",
    "\n",
    "# # verificar_termino(s.tf_file_dir, \"6\") "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
