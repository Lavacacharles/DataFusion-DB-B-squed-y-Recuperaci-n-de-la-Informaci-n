{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Proyecto jiji"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: kagglehub in c:\\users\\ce mar\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (0.3.4)"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip available: 22.2.1 -> 24.3.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Requirement already satisfied: requests in c:\\users\\ce mar\\appdata\\roaming\\python\\python310\\site-packages (from kagglehub) (2.28.1)\n",
      "Requirement already satisfied: tqdm in c:\\users\\ce mar\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from kagglehub) (4.66.1)\n",
      "Requirement already satisfied: packaging in c:\\users\\ce mar\\appdata\\roaming\\python\\python310\\site-packages (from kagglehub) (23.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\ce mar\\appdata\\roaming\\python\\python310\\site-packages (from requests->kagglehub) (2022.12.7)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in c:\\users\\ce mar\\appdata\\roaming\\python\\python310\\site-packages (from requests->kagglehub) (2.1.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\ce mar\\appdata\\roaming\\python\\python310\\site-packages (from requests->kagglehub) (3.4)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\ce mar\\appdata\\roaming\\python\\python310\\site-packages (from requests->kagglehub) (1.26.13)\n",
      "Requirement already satisfied: colorama in c:\\users\\ce mar\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from tqdm->kagglehub) (0.4.6)\n"
     ]
    }
   ],
   "source": [
    "!pip install kagglehub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to C:\\Users\\Ce\n",
      "[nltk_data]     mar\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "import numpy as np\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "nltk.download('punkt')\n",
    "import regex as re\n",
    "import os\n",
    "import pandas as pd\n",
    "stemmer = SnowballStemmer('english')\n",
    "import os\n",
    "import pickle\n",
    "import sys\n",
    "import math\n",
    "import dbm\n",
    "import time\n",
    "import heapq\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"stopwords-en.txt\", encoding=\"latin1\") as file:\n",
    "   stoplist = [line.rstrip().lower() for line in file]\n",
    "stoplist += ['?', '-', '.', ':', ',', '!', ';']\n",
    "\n",
    "def preprocesamiento(texto, stemming=True):\n",
    "  words = []\n",
    "  texto = str(texto)\n",
    "  texto = texto.lower()\n",
    "  texto = re.sub(r'[^a-zA-Z0-9_À-ÿ]', ' ', texto)\n",
    "  # tokenizar\n",
    "  words = nltk.word_tokenize(texto, language='english')\n",
    "  # filtrar stopwords\n",
    "  words = [word for word in words if word not in stoplist]\n",
    "  # reducir palabras (stemming)\n",
    "  if stemming:\n",
    "      words = [stemmer.stem(word) for word in words]\n",
    "  return words\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Funciones AUXILIARES para recuperar la información\n",
    "\n",
    "def getNumberWithAtributo(dataset_head, atributo):\n",
    "    if atributo in dataset_head.columns:\n",
    "        return dataset_head.columns.get_loc(atributo)  # Obtiene la posición del atributo\n",
    "    return -1  # Retorna -1 si no existe\n",
    " \n",
    "\n",
    "def getResultados(result, path, disk_limit):\n",
    "    res = []\n",
    "    for chunk in pd.read_csv(path, chunksize=disk_limit):\n",
    "        for doc, score in result:\n",
    "            if doc in chunk.index:\n",
    "                res.append((chunk.iloc[doc], score))\n",
    "    return pd.DataFrame(res, columns=['doc_id', 'score'])  \n",
    "def getResultadosDF(result, path, disk_limit):\n",
    "    res = []\n",
    "    for chunk in pd.read_csv(path, chunksize=disk_limit):\n",
    "        for doc, score in result:\n",
    "            row = chunk.iloc[doc].copy()\n",
    "            row['score'] = score  \n",
    "            res.append(row)\n",
    "    return pd.DataFrame(res) \n",
    "def get_dfTex_Cols(path, row, columnas): #We don´t use it is only ofr simple an very fast testing\n",
    "    dataset =  pd.read_csv(path)\n",
    "    res = [dataset.iloc[row, i] for i in columnas]\n",
    "    return ' '.join(res)  \n",
    "#Funciones auxiliares para el mergeo:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creando y ordenando cada bloque\n",
      "Iniciando con el merge\n",
      "Índice fusionado guardado en index_blocks_merge\\postings.bin\n",
      "Diccionario de términos guardado en index_blocks_merge\\term_dict.pkl\n",
      "Normas de documentos guardadas en index_blocks_merge\\document_norms.pkl\n",
      "\n",
      "Resumen de tiempos:\n",
      "Tiempo para la creación Creación  y el ordenamiento de los bloques: 30.63 segundos\n",
      "Tiempo para la ejecución del merge: 1.52 segundos\n",
      "Tiempo total para la creación y el mergeo: 32.15 segundos\n",
      "Se procesaron en total:   2000\n",
      "Términos y sus posiciones en el archivo de postings:\n",
      "----->Yeeeei, Los términos están ordenados alfabéticamente en el diccionario final.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "class SPIMI:\n",
    "    def __init__(self, index_dataset_path=\"path\",bloques_dir=\"index_blocks\" , columnas = [\"track_name\",\"track_artist\",\"lyrics\", \"track_album_name\"]):\n",
    "        self.bloques_dir = bloques_dir  \n",
    "        self.dataset_path = index_dataset_path  \n",
    "        self.block_counter = 0      \n",
    "        self.doc_ids = None  \n",
    "        self.idf = {}\n",
    "        self.length = {}\n",
    "        self.disk_limit = 40000  \n",
    "        \n",
    "        #--Columnas:\n",
    "        self.columnas = columnas \n",
    "        # ----Direciones de memoria para los files--\n",
    "        self.dirIndex = self.bloques_dir + \"_merge\"\n",
    "        \n",
    "        self.term_dict_path = os.path.join(self.dirIndex, 'term_dict.pkl')\n",
    "        self.postings_file_path = os.path.join(self.dirIndex, 'postings.bin')\n",
    "        self.norms_file_path = os.path.join(self.dirIndex, 'document_norms.pkl')\n",
    "        #Guardar los ids\n",
    "        self.doc_count_path = os.path.join(self.dirIndex, 'doc_count.txt')  \n",
    "\n",
    "        if not os.path.exists(self.dirIndex):\n",
    "            os.makedirs(self.dirIndex)\n",
    "        if not os.path.exists(self.bloques_dir):\n",
    "            os.makedirs(self.bloques_dir)\n",
    "\n",
    "        if os.path.exists(self.doc_count_path): # Guardamos el total de docs en ves de usar un set\n",
    "            with open(self.doc_count_path, 'r') as f:\n",
    "                self.doc_ids = int(f.read().strip())\n",
    "\n",
    "        self.load_doc_ids()   \n",
    "\n",
    "    def load_doc_ids(self):\n",
    "        if os.path.exists(self.doc_count_path):\n",
    "            try:\n",
    "                with open(self.doc_count_path, 'r') as f:\n",
    "                    self.doc_ids = int(f.read().strip())\n",
    "                    print(f\"Documentos cargados: {self.doc_ids}\")\n",
    "            except (ValueError, FileNotFoundError):\n",
    "                self.doc_ids = 0  # Si el archivo está vacío o corrupto\n",
    "        else:\n",
    "            self.doc_ids = 0\n",
    " \n",
    "       \n",
    "        \n",
    "                  \n",
    "\n",
    "    def spimi_invert(self):\n",
    "        print(\"Creando y ordenando cada bloque\")\n",
    "        dictionary = {}\n",
    "        doc_ids = set()\n",
    "        time_spimiInvert_start = time.time()\n",
    "\n",
    "        #Get columna indeces: \n",
    "        with open(self.dataset_path, mode='r', encoding='utf-8') as file:\n",
    "            primera_linea = file.readline().strip()\n",
    "        #Get number columns\n",
    "        columnas = primera_linea.split(',')\n",
    "        columnas_numbers  = [i for i in range(len(columnas)) if columnas[i] in self.columnas]\n",
    "\n",
    "        # Creamos los diccionarios a los bloques\n",
    "        for chunk in pd.read_csv(self.dataset_path,chunksize= self.disk_limit): \n",
    "            for doc_id_, row in chunk.iterrows():\n",
    "                self.doc_ids +=1 # Actualizamos la cantidad de id\n",
    "                preFila = [str(row.iloc[i]) for i in columnas_numbers]\n",
    "                texto = ' '.join(item for item in preFila)\n",
    "                words = preprocesamiento(texto)\n",
    "                for text in words:\n",
    "                    doc_id = doc_id_\n",
    "                    token = text\n",
    "                    if token not in dictionary:\n",
    "                        dictionary[token] = {}  \n",
    "\n",
    "                    if doc_id not in dictionary[token]:\n",
    "                        dictionary[token][doc_id] = 1  \n",
    "                    else:\n",
    "                        dictionary[token][doc_id] += 1  \n",
    "\n",
    "                    dictionary_size = sys.getsizeof(dictionary)\n",
    "                    if dictionary_size >= self.disk_limit:\n",
    "                        self.write_block_to_disk(dictionary)\n",
    "                        dictionary.clear()\n",
    "\n",
    "        if dictionary:\n",
    "            self.write_block_to_disk(dictionary)  \n",
    "        \n",
    "        with open(self.doc_count_path, 'w') as f:\n",
    "            f.write(str(self.doc_ids))\n",
    "\n",
    "        time_spimiInvert = time.time()- time_spimiInvert_start\n",
    "\n",
    "        # MERGE\n",
    "        print(\"Iniciando con el merge\")\n",
    "        time_Merge_start = time.time()\n",
    "        self.mergeHeap()\n",
    "        time_Merge = time.time() - time_Merge_start\n",
    "\n",
    "\n",
    "        print(\"\\nResumen de tiempos:\")\n",
    "        print(f\"Tiempo para la creación Creación  y el ordenamiento de los bloques: {time_spimiInvert:.2f} segundos\")\n",
    "        print(f\"Tiempo para la ejecución del merge: {time_Merge:.2f} segundos\")\n",
    "        print(f\"Tiempo total para la creación y el mergeo: {time_Merge + time_spimiInvert:.2f} segundos\")\n",
    "    \n",
    "\n",
    "\n",
    "    def write_block_to_disk(self, dictionary): # Guardo los ordenados\n",
    "        # TO DO\n",
    "        # Probar con guardar y luego ordenar con hilos\n",
    "        sorted_terms = dict(sorted(dictionary.items())) \n",
    "        file_path = os.path.join(self.bloques_dir, f\"block_{self.block_counter}.txt\")\n",
    "        with open(file_path, \"w\", encoding=\"utf-8\") as f:\n",
    "            for term, postings in sorted_terms.items():\n",
    "                postings_str = json.dumps(postings)  \n",
    "                f.write(f\"{term}: {postings_str}\\n\")  \n",
    "        self.block_counter += 1\n",
    "\n",
    "\n",
    "    def load_term_dict(self):\n",
    "        with open(self.term_dict_path, 'rb') as f:\n",
    "            while True:\n",
    "                try: #Yiel es un generador, en vez de un return yield mantienes q en piclek esta \n",
    "                    yield pickle.load(f) #Avanzamos bloque por bloque, es decir dicionario por diccionario\n",
    "                except EOFError:\n",
    "                    break\n",
    "\n",
    "\n",
    "    def show_Block(self, index):\n",
    "        print(\"Imprimiendo contenido del bloque: \", index)\n",
    "        file_path = os.path.join(self.bloques_dir, f'block_{index}.pkl')\n",
    "        if not os.path.exists(file_path):\n",
    "            print(f\"El bloque {index} no existe en la ruta: {file_path}\")\n",
    "            return\n",
    "        try:\n",
    "            with open(file_path, 'rb') as f:\n",
    "                block_content = pickle.load(f)\n",
    "            print(block_content)\n",
    "        except Exception as e:\n",
    "            print(f\"Error al abrir el bloque {index}: {e}\")\n",
    "\n",
    "\n",
    "\n",
    "    # Ya se comprobó el correcto merge \n",
    "    # Ahora los archivo son terminos postings y salto de linea, asi se getea linea por línea \n",
    "    def mergeHeap(self):\n",
    "        lista_bloques = os.listdir(self.bloques_dir)\n",
    "        priority_queue = []\n",
    "        idx = 0 \n",
    "    \n",
    "        postings_file_path = os.path.join(self.dirIndex, 'postings.bin')\n",
    "        term_dict_path = os.path.join(self.dirIndex, 'term_dict.pkl')\n",
    "        norms_file_path = os.path.join(self.dirIndex, 'document_norms.pkl')\n",
    "    \n",
    "        open_files = {}\n",
    "        doc_norms_temp = {}  \n",
    "    \n",
    "        #Para optimizar en término s de manejo de memoria secundaria a term_dict.pkl haremos lo siguiente: \n",
    "        # en un diccionario temporal y si se supera a disk_limit se guarda\n",
    "        # las el mañao del diccionario y le diccionario, de tal forma que luego se puda abrir por bloques\n",
    "        # En términos prácticos es (pos, tamanio) diccionario tempora ...\n",
    "        temp_term_dict = {}\n",
    "        temp_term_dict_position = 0\n",
    "\n",
    "        try:\n",
    "            for bloque_path in lista_bloques:  # Referencias de bloques y pusheo de una línea (term, postings)\n",
    "                bloque_full_path = os.path.join(self.bloques_dir, bloque_path)\n",
    "                open_files[bloque_path] = open(bloque_full_path, \"r\", encoding=\"utf-8\")\n",
    "                term_postings = self.read_next_term(open_files[bloque_path])\n",
    "                if term_postings:\n",
    "                    term, postings = term_postings\n",
    "                    heapq.heappush(priority_queue, (term, idx, postings, bloque_path))\n",
    "                    idx += 1\n",
    "    \n",
    "            current_term = None\n",
    "            current_postings = {}\n",
    "            postings_file_position = 0\n",
    "    \n",
    "            with open(postings_file_path, 'wb') as postings_file, \\\n",
    "                 open(term_dict_path, 'ab') as term_dict_file:\n",
    "    \n",
    "                while priority_queue:\n",
    "                    term, _, postings, bloque_path = heapq.heappop(priority_queue)\n",
    "                    file = open_files[bloque_path]\n",
    "    \n",
    "                    if term == current_term:  # Caso en que el término es igual\n",
    "                        for doc_id, tf in postings.items():\n",
    "                            current_postings[doc_id] = current_postings.get(doc_id, 0) + tf\n",
    "                    else:  # Nuevo término\n",
    "                        if current_term is not None:\n",
    "                            # Calcular IDF\n",
    "                            idf = math.log10(self.doc_ids / (1 + len(current_postings)))\n",
    "    \n",
    "                            # Guardar postings (TF e IDF) en postings.bin\n",
    "                            tf_idf_postings = {}\n",
    "                            for doc_id, tf in current_postings.items():\n",
    "                                tf_weighted = math.log10(1 + tf)\n",
    "                                tfidf = tf_weighted * idf\n",
    "                                tf_idf_postings[doc_id] = {\"tf\": tf_weighted, \"idf\": idf}\n",
    "    \n",
    "                                # Acumular el cuadrado del TF-IDF en las normas temporales\n",
    "                                doc_norms_temp[doc_id] = doc_norms_temp.get(doc_id, 0) + tfidf ** 2\n",
    "    \n",
    "                            postings_data = pickle.dumps(tf_idf_postings)\n",
    "                            postings_file.write(postings_data)\n",
    "    \n",
    "                            # Guardar término en term_dict_temporal: \n",
    "                            temp_term_dict[current_term] =  (postings_file_position, len(postings_data)) \n",
    "                            postings_file_position += len(postings_data)\n",
    "                        \n",
    "                        # Si se puera el limite ------><------\n",
    "                        dictionary_size = sys.getsizeof(temp_term_dict)\n",
    "                        if dictionary_size >= self.disk_limit: \n",
    "                            pickle.dump(temp_term_dict, term_dict_file) \n",
    "                            \n",
    "                            temp_term_dict.clear()\n",
    "                        \n",
    "                        # Actualizar current_term y current_postings\n",
    "                        current_term = term\n",
    "                        current_postings = postings.copy()\n",
    "\n",
    "                    next_term_postings = self.read_next_term(file)\n",
    "                    if next_term_postings:\n",
    "                        next_term, next_postings = next_term_postings\n",
    "                        heapq.heappush(priority_queue, (next_term, idx, next_postings, bloque_path))\n",
    "                        idx += 1\n",
    "    \n",
    "                \n",
    "                if current_term is not None:\n",
    "                    idf = math.log10(self.doc_ids / (1 + len(current_postings)))\n",
    "                    tf_idf_postings = {}\n",
    "                    for doc_id, tf in current_postings.items():\n",
    "                        tf_weighted = math.log10(1 + tf)\n",
    "                        tfidf = tf_weighted * idf\n",
    "                        tf_idf_postings[doc_id] = {\"tf\": tf_weighted, \"idf\": idf}\n",
    "    \n",
    "                        # Acumular el cuadrado del TF-IDF en las normas temporales\n",
    "                        doc_norms_temp[doc_id] = doc_norms_temp.get(doc_id, 0) + tfidf ** 2\n",
    "\n",
    "                    \n",
    "\n",
    "                    postings_data = pickle.dumps(tf_idf_postings)\n",
    "                    postings_file.write(postings_data)\n",
    "                    temp_term_dict[current_term] = (postings_file_position, len(postings_data))\n",
    "                    # pickle.dump({current_term: (postings_file_position, len(postings_data))}, term_dict_file)\n",
    "                    postings_file_position += len(postings_data)\n",
    "\n",
    "                    #Guardamos de frente temp_term_dict\n",
    "                    pickle.dump(temp_term_dict, term_dict_file) #Guardo el diccionario\n",
    "                    temp_term_dict.clear()\n",
    "\n",
    "                    sorted_terms = list(temp_term_dict.keys())\n",
    "                    \n",
    "                    temp_term_dict.clear()\n",
    "    \n",
    "        finally:\n",
    "            for file in open_files.values():\n",
    "                file.close()\n",
    "    \n",
    "        # Guardar las normas finales tomando la raíz cuadrada\n",
    "        final_norms = {int(doc_id): math.sqrt(value) for doc_id, value in doc_norms_temp.items()}\n",
    "        with open(norms_file_path, 'wb') as norms_file:\n",
    "            pickle.dump(final_norms, norms_file)\n",
    "    \n",
    "        self.term_dict_path = term_dict_path\n",
    "        self.postings_file_path = postings_file_path\n",
    "        print(f\"Índice fusionado guardado en {postings_file_path}\")\n",
    "        print(f\"Diccionario de términos guardado en {term_dict_path}\")\n",
    "        print(f\"Normas de documentos guardadas en {norms_file_path}\")\n",
    "            # Realiza merge de todos los bloques:\n",
    "            # Manejaremos un archivo para ubicar los un indice general self.indexTerms donde se guarda la posición del termino\n",
    "            # En el otro archivo se guardaran completito tanto la palabra como el posting list pero ya ordenado\n",
    "    \n",
    "    def read_next_term(self, file):\n",
    "        line = file.readline()\n",
    "        if not line:  # Si llegamos al final del archivo\n",
    "            return None\n",
    "        term, postings_str = line.strip().split(\":\", 1)\n",
    "        postings = json.loads(postings_str)  \n",
    "        return term, postings\n",
    "\n",
    "    #--Funciones para testing--\n",
    "    def print_all_norms(self):\n",
    "        if not os.path.exists(self.norms_file_path):\n",
    "            print(\"El archivo de normas no existe. Asegúrate de haber ejecutado 'spimi_invert' correctamente.\")\n",
    "            return\n",
    "\n",
    "        try:\n",
    "            with open(self.norms_file_path, 'rb') as norms_file:\n",
    "                norms = pickle.load(norms_file)\n",
    "                if not norms:\n",
    "                    print(\"El archivo de normas está vacío.\")\n",
    "                    return\n",
    "\n",
    "                print(\"Normas de los documentos:\")\n",
    "                for doc_id, norm in sorted(norms.items()):\n",
    "                    print(f\"Documento ID: {doc_id}, Norma: {norm}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error al leer o procesar las normas: {e}\")\n",
    "\n",
    "\n",
    "    def show_terms_and_positions(self):\n",
    "        print(\"Términos y sus posiciones en el archivo de postings:\")\n",
    "        all_terms = []  \n",
    "        # for term, (position, length) in term_dict.items():\n",
    "        #     print(f\"Término: '{term}', Posición: {position}, Longitud: {length}\")\n",
    "\n",
    "        for term_dict in self.load_term_dict():  \n",
    "            terms = list(term_dict.keys())\n",
    "            all_terms.extend(terms)\n",
    "        if all_terms == sorted(all_terms):\n",
    "            print(\"----->Yeeeei, Los términos están ordenados alfabéticamente en el diccionario final.\")\n",
    "        else:\n",
    "            print(\"Los términos NO están ordenados alfabéticamente en el diccionario final.\")\n",
    "\n",
    "\n",
    "    def show_terms_with_postings(self):\n",
    "        if not os.path.exists(self.postings_file_path):\n",
    "            print(\"El archivo de postings no existe. Ejecuta 'mergeHeap' primero.\")\n",
    "            return\n",
    "\n",
    "        print(\"Términos y sus listas de postings:\")\n",
    "        try:\n",
    "            with open(self.postings_file_path, 'rb') as postings_file:\n",
    "                for term_dict in self.load_term_dict(): \n",
    "                    for term, (position, length) in term_dict.items():\n",
    "                        postings_file.seek(position)  \n",
    "                        postings_data = postings_file.read(length) \n",
    "                        postings = pickle.loads(postings_data) \n",
    "                        print(f\"Término: '{term}', Postings: {postings}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error al mostrar los términos y postings: {e}\")\n",
    "\n",
    "\n",
    "    def retrieval(self, query, k):\n",
    "        # Cargamos doc_ids si no está ya cargado\n",
    "        if self.doc_ids is None:\n",
    "            self.load_doc_ids()\n",
    "    \n",
    "        if self.doc_ids == 0:  \n",
    "            raise ValueError(\"El índice no está construido o no contiene documentos. Ejecuta 'spimi_invert' antes de realizar consultas.\")\n",
    "        print(\"Se cargó en total: \", self.doc_ids, \" documentos\")\n",
    "    \n",
    "        # Preprocesar la consulta\n",
    "        terms = preprocesamiento(query)\n",
    "    \n",
    "        tf_query = {}\n",
    "        for term in terms:\n",
    "            if term in tf_query:\n",
    "                tf_query[term] += 1\n",
    "            else:\n",
    "                tf_query[term] = 1\n",
    "    \n",
    "        tfidf_query = {}\n",
    "        norm_query = 0\n",
    "    \n",
    "        scores = [0] * self.doc_ids  \n",
    "    \n",
    "        with open(self.postings_file_path, 'rb') as postings_file, \\\n",
    "             open(self.norms_file_path, 'rb') as norms_file:\n",
    "    \n",
    "            for term_dict_block in self.load_term_dict(): # bloque por bloque gracias a yield\n",
    "                for term, tf in tf_query.items():\n",
    "                    if term in term_dict_block:  # Considerar términos presentes en este bloque\n",
    "                        position, length = term_dict_block[term]\n",
    "    \n",
    "                        postings_file.seek(position)\n",
    "                        postings_data = postings_file.read(length)\n",
    "                        postings = pickle.loads(postings_data)  # Diccionario {doc_id: {'tf': tf, 'idf': idf}}\n",
    "                        idf = math.log10(self.doc_ids / (1 + len(postings)))\n",
    "    \n",
    "                        tfidf_query[term] = math.log10(1 + tf) * idf\n",
    "                        norm_query += (tfidf_query[term]) ** 2\n",
    "    \n",
    "                        w_tq = tfidf_query[term]\n",
    "                        for doc_id_str, values in postings.items():\n",
    "                            doc_id = int(doc_id_str)\n",
    "                            tfidf_td = values['tf'] * values['idf']\n",
    "                            scores[doc_id] += tfidf_td * w_tq \n",
    "    \n",
    "            norm_query = math.sqrt(norm_query)\n",
    "    \n",
    "            norms_file.seek(0)\n",
    "            norms = pickle.load(norms_file)\n",
    "    \n",
    "            for doc_id in range(self.doc_ids):\n",
    "                doc_norm = norms.get(doc_id, 0)\n",
    "                if doc_norm != 0:\n",
    "                    scores[doc_id] /= (doc_norm * norm_query)\n",
    "    \n",
    "        result = sorted(enumerate(scores), key=lambda x: x[1], reverse=True)\n",
    "        return result[:k]\n",
    "    \n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "path = \"spotify_2000.csv\"\n",
    "# ---------Creation---------\n",
    "columnas = [\"track_name\",\"track_artist\",\"lyrics\", \"track_album_name\"]\n",
    "s = SPIMI( path, columnas=columnas)  \n",
    "s.spimi_invert() \n",
    "\n",
    "# ---------End Creation---------\n",
    "print(\"Se procesaron en total:  \", s.doc_ids)\n",
    "\n",
    "\n",
    "#Prubas de merge\n",
    "s.show_terms_and_positions()\n",
    "# s.show_terms_with_postings()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Se cargó en total:  2000  documentos\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>track_id</th>\n",
       "      <th>track_name</th>\n",
       "      <th>track_artist</th>\n",
       "      <th>lyrics</th>\n",
       "      <th>track_popularity</th>\n",
       "      <th>track_album_id</th>\n",
       "      <th>track_album_name</th>\n",
       "      <th>track_album_release_date</th>\n",
       "      <th>playlist_name</th>\n",
       "      <th>playlist_id</th>\n",
       "      <th>...</th>\n",
       "      <th>mode</th>\n",
       "      <th>speechiness</th>\n",
       "      <th>acousticness</th>\n",
       "      <th>instrumentalness</th>\n",
       "      <th>liveness</th>\n",
       "      <th>valence</th>\n",
       "      <th>tempo</th>\n",
       "      <th>duration_ms</th>\n",
       "      <th>language</th>\n",
       "      <th>score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>00chLpzhgVjxs1zKC9UScL</td>\n",
       "      <td>Poison</td>\n",
       "      <td>Bell Biv DeVoe</td>\n",
       "      <td>NA Yeah, Spyderman and Freeze in full effect U...</td>\n",
       "      <td>0</td>\n",
       "      <td>6oZ6brjB8x3GoeSYdwJdPc</td>\n",
       "      <td>Gold</td>\n",
       "      <td>2005-01-01</td>\n",
       "      <td>Back in the day - R&amp;B, New Jack Swing, Swingbe...</td>\n",
       "      <td>3a9y4eeCJRmG9p4YKfqYIx</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0.2160</td>\n",
       "      <td>0.004320</td>\n",
       "      <td>0.007230</td>\n",
       "      <td>0.4890</td>\n",
       "      <td>0.650</td>\n",
       "      <td>111.904</td>\n",
       "      <td>262467</td>\n",
       "      <td>en</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>622</th>\n",
       "      <td>0dDifktDnbzmGwliFzI0Ld</td>\n",
       "      <td>Poison</td>\n",
       "      <td>Bell Biv DeVoe</td>\n",
       "      <td>NA Yeah, Spyderman and Freeze in full effect U...</td>\n",
       "      <td>0</td>\n",
       "      <td>5AR2bbUUDI2XEMarSbs8Cn</td>\n",
       "      <td>Bell Biv DeVoe Greatest Hits (Remastered)</td>\n",
       "      <td>2000</td>\n",
       "      <td>New Jack Swing</td>\n",
       "      <td>3ykXidKLz1eYPvuGoFlD1e</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0.2100</td>\n",
       "      <td>0.002060</td>\n",
       "      <td>0.001830</td>\n",
       "      <td>0.6340</td>\n",
       "      <td>0.783</td>\n",
       "      <td>111.879</td>\n",
       "      <td>261627</td>\n",
       "      <td>en</td>\n",
       "      <td>0.994159</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>237</th>\n",
       "      <td>06yHcjr58IKJnfEK7ko3sD</td>\n",
       "      <td>Poison</td>\n",
       "      <td>Bell Biv DeVoe</td>\n",
       "      <td>NA Yeah, Spyderman and Freeze in full effect U...</td>\n",
       "      <td>36</td>\n",
       "      <td>3ExqNzkCrQkkknQbrJFqMd</td>\n",
       "      <td>RnB 1990's</td>\n",
       "      <td>2019-07-19</td>\n",
       "      <td>CSR 103:9 (GTA: SA)</td>\n",
       "      <td>4sr2xEhXQR5VuZ0LZX8TQ8</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0.2100</td>\n",
       "      <td>0.001950</td>\n",
       "      <td>0.004170</td>\n",
       "      <td>0.6330</td>\n",
       "      <td>0.775</td>\n",
       "      <td>111.815</td>\n",
       "      <td>261853</td>\n",
       "      <td>en</td>\n",
       "      <td>0.983802</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1705</th>\n",
       "      <td>0RnDu3eYJqbFKz6MHv2ajd</td>\n",
       "      <td>Poison</td>\n",
       "      <td>Bell Biv DeVoe</td>\n",
       "      <td>NA Yeah, Spyderman and Freeze in full effect U...</td>\n",
       "      <td>48</td>\n",
       "      <td>2QC1IsQIUNdEz0zgWanPkN</td>\n",
       "      <td>20th Century Masters: The Millennium Collectio...</td>\n",
       "      <td>2002-01-01</td>\n",
       "      <td>New Jack Swing - 90s R&amp;B fused w Hip Hop</td>\n",
       "      <td>79xd4wnVuKZK4rJMsL2wPa</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0.1920</td>\n",
       "      <td>0.002150</td>\n",
       "      <td>0.002680</td>\n",
       "      <td>0.6270</td>\n",
       "      <td>0.802</td>\n",
       "      <td>111.857</td>\n",
       "      <td>262027</td>\n",
       "      <td>en</td>\n",
       "      <td>0.968680</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1199</th>\n",
       "      <td>0kYIsBXBR8bg8JN6xuqIDK</td>\n",
       "      <td>Since You Been Gone</td>\n",
       "      <td>RAINBOW</td>\n",
       "      <td>I get the same old dreams, same time every nig...</td>\n",
       "      <td>6</td>\n",
       "      <td>4saAsJgmmKqFWTzaTdiYzC</td>\n",
       "      <td>Rock</td>\n",
       "      <td>2003-01-01</td>\n",
       "      <td>70s Pop &amp; Rock Hits and Deep Tracks</td>\n",
       "      <td>1uKFRCQYci8kVgMy3xzTVH</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0370</td>\n",
       "      <td>0.362000</td>\n",
       "      <td>0.000004</td>\n",
       "      <td>0.5590</td>\n",
       "      <td>0.934</td>\n",
       "      <td>120.875</td>\n",
       "      <td>194000</td>\n",
       "      <td>en</td>\n",
       "      <td>0.120369</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>703</th>\n",
       "      <td>0EkB09i9ohIUnsw45jzEyf</td>\n",
       "      <td>Toxic</td>\n",
       "      <td>Yael Naim</td>\n",
       "      <td>Baby, can't you see I'm calling A guy like you...</td>\n",
       "      <td>4</td>\n",
       "      <td>09Cvd1XS0KwCXfMXukpvSS</td>\n",
       "      <td>Yael Naim</td>\n",
       "      <td>2007-10-22</td>\n",
       "      <td>Bluegrass Covers</td>\n",
       "      <td>37i9dQZF1DX56crgoe4TG3</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0632</td>\n",
       "      <td>0.605000</td>\n",
       "      <td>0.000056</td>\n",
       "      <td>0.0685</td>\n",
       "      <td>0.157</td>\n",
       "      <td>120.043</td>\n",
       "      <td>267226</td>\n",
       "      <td>en</td>\n",
       "      <td>0.111347</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1418</th>\n",
       "      <td>0nrWZWkJM62ftQJSI8fDc0</td>\n",
       "      <td>Diamond Heart</td>\n",
       "      <td>Alan Walker</td>\n",
       "      <td>Hello, sweet grief I know you'll be the death ...</td>\n",
       "      <td>69</td>\n",
       "      <td>3nzuGtN3nXARvvecier4K0</td>\n",
       "      <td>Different World</td>\n",
       "      <td>2018-12-14</td>\n",
       "      <td>Dance Pop Tunes</td>\n",
       "      <td>4SdfG4cPG3skmTiQLozZGh</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0345</td>\n",
       "      <td>0.123000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.4100</td>\n",
       "      <td>0.287</td>\n",
       "      <td>89.909</td>\n",
       "      <td>240333</td>\n",
       "      <td>en</td>\n",
       "      <td>0.110368</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>501</th>\n",
       "      <td>0bTjMx79m5NMCsHDTyAwBY</td>\n",
       "      <td>Already Dead</td>\n",
       "      <td>Hollywood Undead</td>\n",
       "      <td>It's the year of the snake Friends are overrat...</td>\n",
       "      <td>59</td>\n",
       "      <td>250u5qE6ny6ZYUUtlVGYre</td>\n",
       "      <td>Already Dead</td>\n",
       "      <td>2019-10-25</td>\n",
       "      <td>Rock Hard</td>\n",
       "      <td>37i9dQZF1DWWJOmJ7nRx0C</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0.4080</td>\n",
       "      <td>0.000439</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.1310</td>\n",
       "      <td>0.303</td>\n",
       "      <td>159.978</td>\n",
       "      <td>236734</td>\n",
       "      <td>en</td>\n",
       "      <td>0.109022</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1875</th>\n",
       "      <td>0TiC3GtlMCskf2hIUIBcDV</td>\n",
       "      <td>Crew Love</td>\n",
       "      <td>Drake</td>\n",
       "      <td>Take your nose off my keyboard What you bother...</td>\n",
       "      <td>51</td>\n",
       "      <td>63WdJvk8G9hxJn8u5rswNh</td>\n",
       "      <td>Take Care (Deluxe)</td>\n",
       "      <td>2011-11-15</td>\n",
       "      <td>Urban Contemporary</td>\n",
       "      <td>4Pbs84EQbuAblxlp6Chz0d</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0.2380</td>\n",
       "      <td>0.227000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.2290</td>\n",
       "      <td>0.294</td>\n",
       "      <td>160.152</td>\n",
       "      <td>208813</td>\n",
       "      <td>en</td>\n",
       "      <td>0.102159</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1994</th>\n",
       "      <td>0V4l4GQhgnWQGtCWpvA7va</td>\n",
       "      <td>Crew Love</td>\n",
       "      <td>Drake</td>\n",
       "      <td>Take your nose off my keyboard What you bother...</td>\n",
       "      <td>69</td>\n",
       "      <td>6X1x82kppWZmDzlXXK3y3q</td>\n",
       "      <td>Take Care (Deluxe)</td>\n",
       "      <td>2011-11-15</td>\n",
       "      <td>PROJECT: Contemporary</td>\n",
       "      <td>6HaCi9bqaiuSZEDfCEmwyo</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0.2380</td>\n",
       "      <td>0.227000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.2290</td>\n",
       "      <td>0.294</td>\n",
       "      <td>160.152</td>\n",
       "      <td>208813</td>\n",
       "      <td>en</td>\n",
       "      <td>0.102159</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>410</th>\n",
       "      <td>0arymh4a4YsUaps34DfA7L</td>\n",
       "      <td>When Can I See You</td>\n",
       "      <td>Babyface</td>\n",
       "      <td>When can my heart beat again? When does the pa...</td>\n",
       "      <td>13</td>\n",
       "      <td>5y4ohfVVgrOEwTM74TJbuS</td>\n",
       "      <td>For The Cool In You</td>\n",
       "      <td>1993-08-24</td>\n",
       "      <td>New Jack Swing</td>\n",
       "      <td>3ykXidKLz1eYPvuGoFlD1e</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0602</td>\n",
       "      <td>0.251000</td>\n",
       "      <td>0.000014</td>\n",
       "      <td>0.1150</td>\n",
       "      <td>0.573</td>\n",
       "      <td>84.612</td>\n",
       "      <td>229307</td>\n",
       "      <td>en</td>\n",
       "      <td>0.101216</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1783</th>\n",
       "      <td>0sLrHBMJNOxEcMhcseE3I4</td>\n",
       "      <td>Rollin' With Kid 'N Play</td>\n",
       "      <td>Kid 'N Play</td>\n",
       "      <td>Ho-la, ho-la, hey Ho-la, ho-la, hey Rolling ro...</td>\n",
       "      <td>43</td>\n",
       "      <td>1XN7cuhq8ZIvWsUUZ9xDzR</td>\n",
       "      <td>2 Hype</td>\n",
       "      <td>1988</td>\n",
       "      <td>Minitruckin Playlist</td>\n",
       "      <td>0VVH2Nzj6kBVGK3WIUQMAw</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0.1330</td>\n",
       "      <td>0.036200</td>\n",
       "      <td>0.004980</td>\n",
       "      <td>0.0461</td>\n",
       "      <td>0.739</td>\n",
       "      <td>103.013</td>\n",
       "      <td>243800</td>\n",
       "      <td>en</td>\n",
       "      <td>0.099471</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>458</th>\n",
       "      <td>0bIaAG656Kq0JGmw3N1TeS</td>\n",
       "      <td>One For the Money</td>\n",
       "      <td>Escape the Fate</td>\n",
       "      <td>Are you ready? Are you ready? Are you ready? A...</td>\n",
       "      <td>0</td>\n",
       "      <td>4FeqhARxrIYBYAZphCDz2Q</td>\n",
       "      <td>Ungrateful (Deluxe)</td>\n",
       "      <td>2013-05-14</td>\n",
       "      <td>New Hard Rock</td>\n",
       "      <td>64BvJcehegyvhqQtV82Ddz</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0554</td>\n",
       "      <td>0.000355</td>\n",
       "      <td>0.000004</td>\n",
       "      <td>0.0577</td>\n",
       "      <td>0.416</td>\n",
       "      <td>90.039</td>\n",
       "      <td>199960</td>\n",
       "      <td>en</td>\n",
       "      <td>0.098697</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>939</th>\n",
       "      <td>0hmxVkpxbcdHAXUoZ7DeCQ</td>\n",
       "      <td>Did My Best</td>\n",
       "      <td>The Voidz</td>\n",
       "      <td>Let me tell you a story'Bout the hazy good old...</td>\n",
       "      <td>60</td>\n",
       "      <td>5q9iV6CLu4ZEzhWKmJZxnc</td>\n",
       "      <td>Did My Best</td>\n",
       "      <td>2019-12-13</td>\n",
       "      <td>Modern Indie Rock // Alternative Rock / Garage...</td>\n",
       "      <td>1VnvyBDqoV5TCZAnXYferL</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0371</td>\n",
       "      <td>0.154000</td>\n",
       "      <td>0.888000</td>\n",
       "      <td>0.1060</td>\n",
       "      <td>0.623</td>\n",
       "      <td>120.209</td>\n",
       "      <td>296939</td>\n",
       "      <td>en</td>\n",
       "      <td>0.097655</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1142</th>\n",
       "      <td>0kFM6t9htbB53Dg8frGDGh</td>\n",
       "      <td>Kiss Me Deadly</td>\n",
       "      <td>Lita Ford</td>\n",
       "      <td>I went to a party last Saturday night I didn't...</td>\n",
       "      <td>54</td>\n",
       "      <td>5nfd1bXqze24U3EZXP1Qlk</td>\n",
       "      <td>Lita</td>\n",
       "      <td>1988-01-02</td>\n",
       "      <td>’80s Hard Rock</td>\n",
       "      <td>37i9dQZF1DX68H8ZujdnN7</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0326</td>\n",
       "      <td>0.008190</td>\n",
       "      <td>0.000079</td>\n",
       "      <td>0.3690</td>\n",
       "      <td>0.648</td>\n",
       "      <td>156.103</td>\n",
       "      <td>241800</td>\n",
       "      <td>en</td>\n",
       "      <td>0.097421</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1809</th>\n",
       "      <td>0STdgJGzSVSv3mhAvuT7dZ</td>\n",
       "      <td>Get It on Tonite - Remastered</td>\n",
       "      <td>Montell Jordan</td>\n",
       "      <td>Oh, ooh wee oh, oh When I'm lookin' at you I k...</td>\n",
       "      <td>6</td>\n",
       "      <td>5DyGKg106B5LCcjjbLzI3Z</td>\n",
       "      <td>New Jack Swing (The Ultimate Collection)</td>\n",
       "      <td>2005-05-02</td>\n",
       "      <td>Swingbeat (old skool), New Jack Swing, R&amp;B, Hi...</td>\n",
       "      <td>3krpccUV68nBGAQbvHEZDC</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0873</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.000053</td>\n",
       "      <td>0.0841</td>\n",
       "      <td>0.855</td>\n",
       "      <td>98.996</td>\n",
       "      <td>276267</td>\n",
       "      <td>en</td>\n",
       "      <td>0.093157</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>372</th>\n",
       "      <td>0AcLrSfAEBQcUnHOTm5pXg</td>\n",
       "      <td>Get It On Tonite</td>\n",
       "      <td>Montell Jordan</td>\n",
       "      <td>Oh, ooh wee oh, oh When I'm lookin' at you I k...</td>\n",
       "      <td>51</td>\n",
       "      <td>6J9fX2iXc9W7ILQUWvEhAx</td>\n",
       "      <td>Best Of Montell Jordan</td>\n",
       "      <td>2015-09-25</td>\n",
       "      <td>90s/00s Hip Hop &amp; RnB</td>\n",
       "      <td>0Ar0Ng9DlAWZtSPBvOQgOa</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0755</td>\n",
       "      <td>0.259000</td>\n",
       "      <td>0.000030</td>\n",
       "      <td>0.0618</td>\n",
       "      <td>0.857</td>\n",
       "      <td>99.001</td>\n",
       "      <td>277413</td>\n",
       "      <td>en</td>\n",
       "      <td>0.092600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1323</th>\n",
       "      <td>0MPegzGbqTvO2tw7BqYUU8</td>\n",
       "      <td>Go On Girl</td>\n",
       "      <td>Ne-Yo</td>\n",
       "      <td>I can't get it back But I don't want it back I...</td>\n",
       "      <td>2</td>\n",
       "      <td>48ben69fRtEbMTGqGYU8Qw</td>\n",
       "      <td>Because Of You</td>\n",
       "      <td>2007-01-01</td>\n",
       "      <td>Ultimate Throwbacks Collection</td>\n",
       "      <td>1dsaMvnC1hXPCNGC4aVtjj</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0.1090</td>\n",
       "      <td>0.155000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0945</td>\n",
       "      <td>0.562</td>\n",
       "      <td>83.896</td>\n",
       "      <td>261427</td>\n",
       "      <td>en</td>\n",
       "      <td>0.090622</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>365</th>\n",
       "      <td>0AA6zq5ArZ1sSH7VIMi4NK</td>\n",
       "      <td>This Is Why I'm Hot</td>\n",
       "      <td>MiMS</td>\n",
       "      <td>This is why I'm hot (hot!), this is why I'm ho...</td>\n",
       "      <td>57</td>\n",
       "      <td>42c0PgLPx6qRCZCzB8d7Pk</td>\n",
       "      <td>Music Is My Savior</td>\n",
       "      <td>2007-01-01</td>\n",
       "      <td>90s-2000s Southern Hip Hop / Crunk</td>\n",
       "      <td>5wsWBmQgDtKa8CEg7wTEMi</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0.5760</td>\n",
       "      <td>0.277000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.4950</td>\n",
       "      <td>0.485</td>\n",
       "      <td>80.021</td>\n",
       "      <td>253707</td>\n",
       "      <td>en</td>\n",
       "      <td>0.090051</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1378</th>\n",
       "      <td>0ND36delZlYUcwNaCk9Uf5</td>\n",
       "      <td>holy terrain</td>\n",
       "      <td>FKA twigs</td>\n",
       "      <td>Yeah, Pluto Lemme do my dance in this bitch, g...</td>\n",
       "      <td>57</td>\n",
       "      <td>1hDlZBsaWOz8St9iioQF0y</td>\n",
       "      <td>holy terrain</td>\n",
       "      <td>2019-09-09</td>\n",
       "      <td>New R&amp;B‏‏​​   ‍</td>\n",
       "      <td>4I6rTSxqKl1LRvES2O9owQ</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0.1230</td>\n",
       "      <td>0.366000</td>\n",
       "      <td>0.004320</td>\n",
       "      <td>0.1190</td>\n",
       "      <td>0.252</td>\n",
       "      <td>159.832</td>\n",
       "      <td>243000</td>\n",
       "      <td>en</td>\n",
       "      <td>0.086842</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>20 rows × 26 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                    track_id                     track_name      track_artist  \\\n",
       "2     00chLpzhgVjxs1zKC9UScL                         Poison    Bell Biv DeVoe   \n",
       "622   0dDifktDnbzmGwliFzI0Ld                         Poison    Bell Biv DeVoe   \n",
       "237   06yHcjr58IKJnfEK7ko3sD                         Poison    Bell Biv DeVoe   \n",
       "1705  0RnDu3eYJqbFKz6MHv2ajd                         Poison    Bell Biv DeVoe   \n",
       "1199  0kYIsBXBR8bg8JN6xuqIDK            Since You Been Gone           RAINBOW   \n",
       "703   0EkB09i9ohIUnsw45jzEyf                          Toxic         Yael Naim   \n",
       "1418  0nrWZWkJM62ftQJSI8fDc0                  Diamond Heart       Alan Walker   \n",
       "501   0bTjMx79m5NMCsHDTyAwBY                   Already Dead  Hollywood Undead   \n",
       "1875  0TiC3GtlMCskf2hIUIBcDV                      Crew Love             Drake   \n",
       "1994  0V4l4GQhgnWQGtCWpvA7va                      Crew Love             Drake   \n",
       "410   0arymh4a4YsUaps34DfA7L             When Can I See You          Babyface   \n",
       "1783  0sLrHBMJNOxEcMhcseE3I4       Rollin' With Kid 'N Play       Kid 'N Play   \n",
       "458   0bIaAG656Kq0JGmw3N1TeS              One For the Money   Escape the Fate   \n",
       "939   0hmxVkpxbcdHAXUoZ7DeCQ                    Did My Best         The Voidz   \n",
       "1142  0kFM6t9htbB53Dg8frGDGh                 Kiss Me Deadly         Lita Ford   \n",
       "1809  0STdgJGzSVSv3mhAvuT7dZ  Get It on Tonite - Remastered    Montell Jordan   \n",
       "372   0AcLrSfAEBQcUnHOTm5pXg               Get It On Tonite    Montell Jordan   \n",
       "1323  0MPegzGbqTvO2tw7BqYUU8                     Go On Girl             Ne-Yo   \n",
       "365   0AA6zq5ArZ1sSH7VIMi4NK            This Is Why I'm Hot              MiMS   \n",
       "1378  0ND36delZlYUcwNaCk9Uf5                   holy terrain         FKA twigs   \n",
       "\n",
       "                                                 lyrics  track_popularity  \\\n",
       "2     NA Yeah, Spyderman and Freeze in full effect U...                 0   \n",
       "622   NA Yeah, Spyderman and Freeze in full effect U...                 0   \n",
       "237   NA Yeah, Spyderman and Freeze in full effect U...                36   \n",
       "1705  NA Yeah, Spyderman and Freeze in full effect U...                48   \n",
       "1199  I get the same old dreams, same time every nig...                 6   \n",
       "703   Baby, can't you see I'm calling A guy like you...                 4   \n",
       "1418  Hello, sweet grief I know you'll be the death ...                69   \n",
       "501   It's the year of the snake Friends are overrat...                59   \n",
       "1875  Take your nose off my keyboard What you bother...                51   \n",
       "1994  Take your nose off my keyboard What you bother...                69   \n",
       "410   When can my heart beat again? When does the pa...                13   \n",
       "1783  Ho-la, ho-la, hey Ho-la, ho-la, hey Rolling ro...                43   \n",
       "458   Are you ready? Are you ready? Are you ready? A...                 0   \n",
       "939   Let me tell you a story'Bout the hazy good old...                60   \n",
       "1142  I went to a party last Saturday night I didn't...                54   \n",
       "1809  Oh, ooh wee oh, oh When I'm lookin' at you I k...                 6   \n",
       "372   Oh, ooh wee oh, oh When I'm lookin' at you I k...                51   \n",
       "1323  I can't get it back But I don't want it back I...                 2   \n",
       "365   This is why I'm hot (hot!), this is why I'm ho...                57   \n",
       "1378  Yeah, Pluto Lemme do my dance in this bitch, g...                57   \n",
       "\n",
       "              track_album_id  \\\n",
       "2     6oZ6brjB8x3GoeSYdwJdPc   \n",
       "622   5AR2bbUUDI2XEMarSbs8Cn   \n",
       "237   3ExqNzkCrQkkknQbrJFqMd   \n",
       "1705  2QC1IsQIUNdEz0zgWanPkN   \n",
       "1199  4saAsJgmmKqFWTzaTdiYzC   \n",
       "703   09Cvd1XS0KwCXfMXukpvSS   \n",
       "1418  3nzuGtN3nXARvvecier4K0   \n",
       "501   250u5qE6ny6ZYUUtlVGYre   \n",
       "1875  63WdJvk8G9hxJn8u5rswNh   \n",
       "1994  6X1x82kppWZmDzlXXK3y3q   \n",
       "410   5y4ohfVVgrOEwTM74TJbuS   \n",
       "1783  1XN7cuhq8ZIvWsUUZ9xDzR   \n",
       "458   4FeqhARxrIYBYAZphCDz2Q   \n",
       "939   5q9iV6CLu4ZEzhWKmJZxnc   \n",
       "1142  5nfd1bXqze24U3EZXP1Qlk   \n",
       "1809  5DyGKg106B5LCcjjbLzI3Z   \n",
       "372   6J9fX2iXc9W7ILQUWvEhAx   \n",
       "1323  48ben69fRtEbMTGqGYU8Qw   \n",
       "365   42c0PgLPx6qRCZCzB8d7Pk   \n",
       "1378  1hDlZBsaWOz8St9iioQF0y   \n",
       "\n",
       "                                       track_album_name  \\\n",
       "2                                                  Gold   \n",
       "622           Bell Biv DeVoe Greatest Hits (Remastered)   \n",
       "237                                          RnB 1990's   \n",
       "1705  20th Century Masters: The Millennium Collectio...   \n",
       "1199                                               Rock   \n",
       "703                                           Yael Naim   \n",
       "1418                                    Different World   \n",
       "501                                        Already Dead   \n",
       "1875                                 Take Care (Deluxe)   \n",
       "1994                                 Take Care (Deluxe)   \n",
       "410                                 For The Cool In You   \n",
       "1783                                             2 Hype   \n",
       "458                                 Ungrateful (Deluxe)   \n",
       "939                                         Did My Best   \n",
       "1142                                               Lita   \n",
       "1809           New Jack Swing (The Ultimate Collection)   \n",
       "372                              Best Of Montell Jordan   \n",
       "1323                                     Because Of You   \n",
       "365                                  Music Is My Savior   \n",
       "1378                                       holy terrain   \n",
       "\n",
       "     track_album_release_date  \\\n",
       "2                  2005-01-01   \n",
       "622                      2000   \n",
       "237                2019-07-19   \n",
       "1705               2002-01-01   \n",
       "1199               2003-01-01   \n",
       "703                2007-10-22   \n",
       "1418               2018-12-14   \n",
       "501                2019-10-25   \n",
       "1875               2011-11-15   \n",
       "1994               2011-11-15   \n",
       "410                1993-08-24   \n",
       "1783                     1988   \n",
       "458                2013-05-14   \n",
       "939                2019-12-13   \n",
       "1142               1988-01-02   \n",
       "1809               2005-05-02   \n",
       "372                2015-09-25   \n",
       "1323               2007-01-01   \n",
       "365                2007-01-01   \n",
       "1378               2019-09-09   \n",
       "\n",
       "                                          playlist_name  \\\n",
       "2     Back in the day - R&B, New Jack Swing, Swingbe...   \n",
       "622                                      New Jack Swing   \n",
       "237                                 CSR 103:9 (GTA: SA)   \n",
       "1705           New Jack Swing - 90s R&B fused w Hip Hop   \n",
       "1199                70s Pop & Rock Hits and Deep Tracks   \n",
       "703                                    Bluegrass Covers   \n",
       "1418                                    Dance Pop Tunes   \n",
       "501                                           Rock Hard   \n",
       "1875                                 Urban Contemporary   \n",
       "1994                              PROJECT: Contemporary   \n",
       "410                                      New Jack Swing   \n",
       "1783                               Minitruckin Playlist   \n",
       "458                                       New Hard Rock   \n",
       "939   Modern Indie Rock // Alternative Rock / Garage...   \n",
       "1142                                     ’80s Hard Rock   \n",
       "1809  Swingbeat (old skool), New Jack Swing, R&B, Hi...   \n",
       "372                               90s/00s Hip Hop & RnB   \n",
       "1323                     Ultimate Throwbacks Collection   \n",
       "365                  90s-2000s Southern Hip Hop / Crunk   \n",
       "1378                                    New R&B‏‏​​   ‍   \n",
       "\n",
       "                 playlist_id  ... mode speechiness  acousticness  \\\n",
       "2     3a9y4eeCJRmG9p4YKfqYIx  ...    0      0.2160      0.004320   \n",
       "622   3ykXidKLz1eYPvuGoFlD1e  ...    0      0.2100      0.002060   \n",
       "237   4sr2xEhXQR5VuZ0LZX8TQ8  ...    0      0.2100      0.001950   \n",
       "1705  79xd4wnVuKZK4rJMsL2wPa  ...    0      0.1920      0.002150   \n",
       "1199  1uKFRCQYci8kVgMy3xzTVH  ...    1      0.0370      0.362000   \n",
       "703   37i9dQZF1DX56crgoe4TG3  ...    1      0.0632      0.605000   \n",
       "1418  4SdfG4cPG3skmTiQLozZGh  ...    1      0.0345      0.123000   \n",
       "501   37i9dQZF1DWWJOmJ7nRx0C  ...    1      0.4080      0.000439   \n",
       "1875  4Pbs84EQbuAblxlp6Chz0d  ...    0      0.2380      0.227000   \n",
       "1994  6HaCi9bqaiuSZEDfCEmwyo  ...    0      0.2380      0.227000   \n",
       "410   3ykXidKLz1eYPvuGoFlD1e  ...    1      0.0602      0.251000   \n",
       "1783  0VVH2Nzj6kBVGK3WIUQMAw  ...    1      0.1330      0.036200   \n",
       "458   64BvJcehegyvhqQtV82Ddz  ...    0      0.0554      0.000355   \n",
       "939   1VnvyBDqoV5TCZAnXYferL  ...    0      0.0371      0.154000   \n",
       "1142  37i9dQZF1DX68H8ZujdnN7  ...    1      0.0326      0.008190   \n",
       "1809  3krpccUV68nBGAQbvHEZDC  ...    0      0.0873      0.250000   \n",
       "372   0Ar0Ng9DlAWZtSPBvOQgOa  ...    0      0.0755      0.259000   \n",
       "1323  1dsaMvnC1hXPCNGC4aVtjj  ...    1      0.1090      0.155000   \n",
       "365   5wsWBmQgDtKa8CEg7wTEMi  ...    1      0.5760      0.277000   \n",
       "1378  4I6rTSxqKl1LRvES2O9owQ  ...    0      0.1230      0.366000   \n",
       "\n",
       "      instrumentalness  liveness  valence    tempo  duration_ms  language  \\\n",
       "2             0.007230    0.4890    0.650  111.904       262467        en   \n",
       "622           0.001830    0.6340    0.783  111.879       261627        en   \n",
       "237           0.004170    0.6330    0.775  111.815       261853        en   \n",
       "1705          0.002680    0.6270    0.802  111.857       262027        en   \n",
       "1199          0.000004    0.5590    0.934  120.875       194000        en   \n",
       "703           0.000056    0.0685    0.157  120.043       267226        en   \n",
       "1418          0.000000    0.4100    0.287   89.909       240333        en   \n",
       "501           0.000000    0.1310    0.303  159.978       236734        en   \n",
       "1875          0.000000    0.2290    0.294  160.152       208813        en   \n",
       "1994          0.000000    0.2290    0.294  160.152       208813        en   \n",
       "410           0.000014    0.1150    0.573   84.612       229307        en   \n",
       "1783          0.004980    0.0461    0.739  103.013       243800        en   \n",
       "458           0.000004    0.0577    0.416   90.039       199960        en   \n",
       "939           0.888000    0.1060    0.623  120.209       296939        en   \n",
       "1142          0.000079    0.3690    0.648  156.103       241800        en   \n",
       "1809          0.000053    0.0841    0.855   98.996       276267        en   \n",
       "372           0.000030    0.0618    0.857   99.001       277413        en   \n",
       "1323          0.000000    0.0945    0.562   83.896       261427        en   \n",
       "365           0.000000    0.4950    0.485   80.021       253707        en   \n",
       "1378          0.004320    0.1190    0.252  159.832       243000        en   \n",
       "\n",
       "         score  \n",
       "2     1.000000  \n",
       "622   0.994159  \n",
       "237   0.983802  \n",
       "1705  0.968680  \n",
       "1199  0.120369  \n",
       "703   0.111347  \n",
       "1418  0.110368  \n",
       "501   0.109022  \n",
       "1875  0.102159  \n",
       "1994  0.102159  \n",
       "410   0.101216  \n",
       "1783  0.099471  \n",
       "458   0.098697  \n",
       "939   0.097655  \n",
       "1142  0.097421  \n",
       "1809  0.093157  \n",
       "372   0.092600  \n",
       "1323  0.090622  \n",
       "365   0.090051  \n",
       "1378  0.086842  \n",
       "\n",
       "[20 rows x 26 columns]"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\n",
    "# ----------Set Atributo--------\n",
    "query = \"this is my breath\"\n",
    "query = get_dfTex_Cols(path, 2, [1,2,3,6])\n",
    "top_k = 20\n",
    "\n",
    "result = s.retrieval(query, top_k)\n",
    "respuesta = getResultados(result, s.dataset_path, s.disk_limit) # La respuesta es un array pd de [fila del dataframe, score]\n",
    "#-----------End of query------------\n",
    "\n",
    "# s.print_all_norms()\n",
    "getResultadosDF(result, s.dataset_path, s.disk_limit)\n",
    "\n",
    "# print( \"Scores: \")\n",
    "# print( respuestadf[\"score\"])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
