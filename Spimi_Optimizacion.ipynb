{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Proyecto jiji"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: kagglehub in c:\\users\\ce mar\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (0.3.4)\n",
      "Requirement already satisfied: packaging in c:\\users\\ce mar\\appdata\\roaming\\python\\python310\\site-packages (from kagglehub) (23.2)\n",
      "Requirement already satisfied: tqdm in c:\\users\\ce mar\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from kagglehub) (4.66.1)\n",
      "Requirement already satisfied: requests in c:\\users\\ce mar\\appdata\\roaming\\python\\python310\\site-packages (from kagglehub) (2.28.1)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\ce mar\\appdata\\roaming\\python\\python310\\site-packages (from requests->kagglehub) (1.26.13)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in c:\\users\\ce mar\\appdata\\roaming\\python\\python310\\site-packages (from requests->kagglehub) (2.1.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\ce mar\\appdata\\roaming\\python\\python310\\site-packages (from requests->kagglehub) (3.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\ce mar\\appdata\\roaming\\python\\python310\\site-packages (from requests->kagglehub) (2022.12.7)\n",
      "Requirement already satisfied: colorama in c:\\users\\ce mar\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from tqdm->kagglehub) (0.4.6)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip available: 22.2.1 -> 24.3.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "!pip install kagglehub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Ce mar\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\scipy\\__init__.py:177: UserWarning: A NumPy version >=1.18.5 and <1.26.0 is required for this version of SciPy (detected version 1.26.2\n",
      "  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n",
      "[nltk_data] Downloading package punkt to C:\\Users\\Ce\n",
      "[nltk_data]     mar\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "import numpy as np\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "nltk.download('punkt')\n",
    "import regex as re\n",
    "import os\n",
    "import pandas as pd\n",
    "stemmer = SnowballStemmer('english')\n",
    "import os\n",
    "import pickle\n",
    "import sys\n",
    "import math\n",
    "import dbm\n",
    "import time\n",
    "import heapq\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"stopwords-en.txt\", encoding=\"latin1\") as file:\n",
    "   stoplist = [line.rstrip().lower() for line in file]\n",
    "stoplist += ['?', '-', '.', ':', ',', '!', ';']\n",
    "\n",
    "def preprocesamiento(texto, stemming=True):\n",
    "  words = []\n",
    "  texto = str(texto)\n",
    "  texto = texto.lower()\n",
    "  texto = re.sub(r'[^a-zA-Z0-9_√Ä-√ø]', ' ', texto)\n",
    "  # tokenizar\n",
    "  words = nltk.word_tokenize(texto, language='english')\n",
    "  # filtrar stopwords\n",
    "  words = [word for word in words if word not in stoplist]\n",
    "  # reducir palabras (stemming)\n",
    "  if stemming:\n",
    "      words = [stemmer.stem(word) for word in words]\n",
    "  return words\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Funciones AUXILIARES para recuperar la informaci√≥n\n",
    "\n",
    "def getNumberWithAtributo(dataset_head, atributo):\n",
    "    if atributo in dataset_head.columns:\n",
    "        return dataset_head.columns.get_loc(atributo)  # Obtiene la posici√≥n del atributo\n",
    "    return -1  # Retorna -1 si no existe\n",
    " \n",
    "\n",
    "def getResultados(result, path, disk_limit):\n",
    "    res = []\n",
    "    for chunk in pd.read_csv(path, chunksize=disk_limit):\n",
    "        for doc, score in result:\n",
    "            if doc in chunk.index:\n",
    "                res.append((chunk.iloc[doc], score))\n",
    "    return pd.DataFrame(res, columns=['doc_id', 'score'])  \n",
    "def getResultadosDF(result, path, disk_limit):\n",
    "    res = []\n",
    "    for chunk in pd.read_csv(path, chunksize=disk_limit):\n",
    "        for doc, score in result:\n",
    "            row = chunk.iloc[doc].copy()\n",
    "            row['score'] = score  \n",
    "            res.append(row)\n",
    "    return pd.DataFrame(res) \n",
    "def get_dfTex_Cols(path, row, columnas): #We don¬¥t use it is only ofr simple an very fast testing\n",
    "    dataset =  pd.read_csv(path)\n",
    "    res = [dataset.iloc[row, i] for i in columnas]\n",
    "    return ' '.join(res)  \n",
    "#Funciones auxiliares para el mergeo:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "class SPIMI:\n",
    "    def __init__(self, index_dataset_path=\"path\",bloques_dir=\"index_blocks\" , columnas = [\"track_name\",\"track_artist\",\"lyrics\", \"track_album_name\"]):\n",
    "        self.bloques_dir = bloques_dir  \n",
    "        self.dataset_path = index_dataset_path  \n",
    "        self.block_counter = 0      \n",
    "        self.doc_ids = None  \n",
    "        self.idf = {}\n",
    "        self.length = {}\n",
    "        self.disk_limit = 40000  \n",
    "        \n",
    "        #--Columnas:\n",
    "        self.columnas = columnas \n",
    "        # ----Direciones de memoria para los files--\n",
    "        self.dirIndex = self.bloques_dir + \"_merge\"\n",
    "        \n",
    "        self.term_dict_path = os.path.join(self.dirIndex, 'term_dict.pkl')\n",
    "        self.postings_file_path = os.path.join(self.dirIndex, 'postings.bin')\n",
    "        self.norms_file_path = os.path.join(self.dirIndex, 'document_norms.pkl')\n",
    "        #Guardar los ids\n",
    "        self.doc_count_path = os.path.join(self.dirIndex, 'doc_count.txt')  \n",
    "\n",
    "        if not os.path.exists(self.dirIndex):\n",
    "            os.makedirs(self.dirIndex)\n",
    "        if not os.path.exists(self.bloques_dir):\n",
    "            os.makedirs(self.bloques_dir)\n",
    "\n",
    "        if os.path.exists(self.doc_count_path): # Guardamos el total de docs en ves de usar un set\n",
    "            with open(self.doc_count_path, 'r') as f:\n",
    "                self.doc_ids = int(f.read().strip())\n",
    "\n",
    "        self.load_doc_ids()   \n",
    "\n",
    "    def load_doc_ids(self):\n",
    "        if os.path.exists(self.doc_count_path):\n",
    "            try:\n",
    "                with open(self.doc_count_path, 'r') as f:\n",
    "                    self.doc_ids = int(f.read().strip())\n",
    "                    print(f\"Documentos cargados: {self.doc_ids}\")\n",
    "            except (ValueError, FileNotFoundError):\n",
    "                self.doc_ids = 0  # Si el archivo est√° vac√≠o o corrupto\n",
    "        else:\n",
    "            self.doc_ids = 0\n",
    " \n",
    "       \n",
    "        \n",
    "                  \n",
    "\n",
    "    def spimi_invert(self):\n",
    "        print(\"Creando y ordenando cada bloque\")\n",
    "        dictionary = {}\n",
    "        doc_ids = set()\n",
    "        time_spimiInvert_start = time.time()\n",
    "\n",
    "        #Get columna indeces: \n",
    "        with open(self.dataset_path, mode='r', encoding='utf-8') as file:\n",
    "            primera_linea = file.readline().strip()\n",
    "        #Get number columns\n",
    "        columnas = primera_linea.split(',')\n",
    "        columnas_numbers  = [i for i in range(len(columnas)) if columnas[i] in self.columnas]\n",
    "\n",
    "        # Creamos los diccionarios a los bloques\n",
    "        for chunk in pd.read_csv(self.dataset_path,chunksize= self.disk_limit): \n",
    "            for doc_id_, row in chunk.iterrows():\n",
    "                self.doc_ids +=1 # Actualizamos la cantidad de id\n",
    "                preFila = [str(row.iloc[i]) for i in columnas_numbers]\n",
    "                texto = ' '.join(item for item in preFila)\n",
    "                words = preprocesamiento(texto)\n",
    "                for text in words:\n",
    "                    doc_id = doc_id_\n",
    "                    token = text\n",
    "                    if token not in dictionary:\n",
    "                        dictionary[token] = {}  \n",
    "\n",
    "                    if doc_id not in dictionary[token]:\n",
    "                        dictionary[token][doc_id] = 1  \n",
    "                    else:\n",
    "                        dictionary[token][doc_id] += 1  \n",
    "\n",
    "                    dictionary_size = sys.getsizeof(dictionary)\n",
    "                    if dictionary_size >= self.disk_limit:\n",
    "                        self.write_block_to_disk(dictionary)\n",
    "                        dictionary.clear()\n",
    "\n",
    "        if dictionary:\n",
    "            self.write_block_to_disk(dictionary)  \n",
    "        \n",
    "        with open(self.doc_count_path, 'w') as f:\n",
    "            f.write(str(self.doc_ids))\n",
    "\n",
    "        time_spimiInvert = time.time()- time_spimiInvert_start\n",
    "\n",
    "        # MERGE\n",
    "        print(\"Iniciando con el merge\")\n",
    "        time_Merge_start = time.time()\n",
    "        self.mergeHeap()\n",
    "        time_Merge = time.time() - time_Merge_start\n",
    "\n",
    "\n",
    "        print(\"\\nResumen de tiempos:\")\n",
    "        print(f\"Tiempo para la creaci√≥n Creaci√≥n  y el ordenamiento de los bloques: {time_spimiInvert:.2f} segundos\")\n",
    "        print(f\"Tiempo para la ejecuci√≥n del merge: {time_Merge:.2f} segundos\")\n",
    "        print(f\"Tiempo total para la creaci√≥n y el mergeo: {time_Merge + time_spimiInvert:.2f} segundos\")\n",
    "    \n",
    "\n",
    "\n",
    "    def write_block_to_disk(self, dictionary): # Guardo los ordenados\n",
    "        # TO DO\n",
    "        # Probar con guardar y luego ordenar con hilos\n",
    "        sorted_terms = dict(sorted(dictionary.items())) \n",
    "        file_path = os.path.join(self.bloques_dir, f\"block_{self.block_counter}.txt\")\n",
    "        with open(file_path, \"w\", encoding=\"utf-8\") as f:\n",
    "            for term, postings in sorted_terms.items():\n",
    "                postings_str = json.dumps(postings)  \n",
    "                f.write(f\"{term}: {postings_str}\\n\")  \n",
    "        self.block_counter += 1\n",
    "\n",
    "\n",
    "    def load_term_dict(self):\n",
    "        with open(self.term_dict_path, 'rb') as f:\n",
    "            while True:\n",
    "                try: #Yiel es un generador, en vez de un return yield mantienes q en piclek esta \n",
    "                    yield pickle.load(f) #Avanzamos bloque por bloque, es decir dicionario por diccionario\n",
    "                except EOFError:\n",
    "                    break\n",
    "\n",
    "\n",
    "    def show_Block(self, index):\n",
    "        print(\"Imprimiendo contenido del bloque: \", index)\n",
    "        file_path = os.path.join(self.bloques_dir, f'block_{index}.pkl')\n",
    "        if not os.path.exists(file_path):\n",
    "            print(f\"El bloque {index} no existe en la ruta: {file_path}\")\n",
    "            return\n",
    "        try:\n",
    "            with open(file_path, 'rb') as f:\n",
    "                block_content = pickle.load(f)\n",
    "            print(block_content)\n",
    "        except Exception as e:\n",
    "            print(f\"Error al abrir el bloque {index}: {e}\")\n",
    "\n",
    "\n",
    "\n",
    "    # Ya se comprob√≥ el correcto merge \n",
    "    # Ahora los archivo son terminos postings y salto de linea, asi se getea linea por l√≠nea \n",
    "    def mergeHeap(self):\n",
    "        lista_bloques = os.listdir(self.bloques_dir)\n",
    "        priority_queue = []\n",
    "        idx = 0 \n",
    "    \n",
    "        postings_file_path = os.path.join(self.dirIndex, 'postings.bin')\n",
    "        term_dict_path = os.path.join(self.dirIndex, 'term_dict.pkl')\n",
    "        norms_file_path = os.path.join(self.dirIndex, 'document_norms.pkl')\n",
    "    \n",
    "        open_files = {}\n",
    "        doc_norms_temp = {}  \n",
    "    \n",
    "        #Para optimizar en t√©rmino s de manejo de memoria secundaria a term_dict.pkl haremos lo siguiente: \n",
    "        # en un diccionario temporal y si se supera a disk_limit se guarda\n",
    "        # las el ma√±ao del diccionario y le diccionario, de tal forma que luego se puda abrir por bloques\n",
    "        # En t√©rminos pr√°cticos es (pos, tamanio) diccionario tempora ...\n",
    "        temp_term_dict = {}\n",
    "        temp_term_dict_position = 0\n",
    "\n",
    "        try:\n",
    "            for bloque_path in lista_bloques:  # Referencias de bloques y pusheo de una l√≠nea (term, postings)\n",
    "                bloque_full_path = os.path.join(self.bloques_dir, bloque_path)\n",
    "                open_files[bloque_path] = open(bloque_full_path, \"r\", encoding=\"utf-8\")\n",
    "                term_postings = self.read_next_term(open_files[bloque_path])\n",
    "                if term_postings:\n",
    "                    term, postings = term_postings\n",
    "                    heapq.heappush(priority_queue, (term, idx, postings, bloque_path))\n",
    "                    idx += 1\n",
    "    \n",
    "            current_term = None\n",
    "            current_postings = {}\n",
    "            postings_file_position = 0\n",
    "    \n",
    "            with open(postings_file_path, 'wb') as postings_file, \\\n",
    "                 open(term_dict_path, 'ab') as term_dict_file:\n",
    "    \n",
    "                while priority_queue:\n",
    "                    term, _, postings, bloque_path = heapq.heappop(priority_queue)\n",
    "                    file = open_files[bloque_path]\n",
    "    \n",
    "                    if term == current_term:  # Caso en que el t√©rmino es igual\n",
    "                        for doc_id, tf in postings.items():\n",
    "                            current_postings[doc_id] = current_postings.get(doc_id, 0) + tf\n",
    "                    else:  # Nuevo t√©rmino\n",
    "                        if current_term is not None:\n",
    "                            # Calcular IDF\n",
    "                            idf = math.log10(self.doc_ids / (1 + len(current_postings)))\n",
    "    \n",
    "                            # Guardar postings (TF e IDF) en postings.bin\n",
    "                            tf_idf_postings = {}\n",
    "                            for doc_id, tf in current_postings.items():\n",
    "                                tf_weighted = math.log10(1 + tf)\n",
    "                                tfidf = tf_weighted * idf\n",
    "                                tf_idf_postings[doc_id] = {\"tf\": tf_weighted, \"idf\": idf}\n",
    "    \n",
    "                                # Acumular el cuadrado del TF-IDF en las normas temporales\n",
    "                                doc_norms_temp[doc_id] = doc_norms_temp.get(doc_id, 0) + tfidf ** 2\n",
    "    \n",
    "                            postings_data = pickle.dumps(tf_idf_postings)\n",
    "                            postings_file.write(postings_data)\n",
    "    \n",
    "                            # Guardar t√©rmino en term_dict_temporal: \n",
    "                            temp_term_dict[current_term] =  (postings_file_position, len(postings_data)) \n",
    "                            postings_file_position += len(postings_data)\n",
    "                        \n",
    "                        # Si se puera el limite ------><------\n",
    "                        dictionary_size = sys.getsizeof(temp_term_dict)\n",
    "                        if dictionary_size >= self.disk_limit: \n",
    "                            pickle.dump(temp_term_dict, term_dict_file) \n",
    "                            \n",
    "                            temp_term_dict.clear()\n",
    "                        \n",
    "                        # Actualizar current_term y current_postings\n",
    "                        current_term = term\n",
    "                        current_postings = postings.copy()\n",
    "\n",
    "                    next_term_postings = self.read_next_term(file)\n",
    "                    if next_term_postings:\n",
    "                        next_term, next_postings = next_term_postings\n",
    "                        heapq.heappush(priority_queue, (next_term, idx, next_postings, bloque_path))\n",
    "                        idx += 1\n",
    "    \n",
    "                \n",
    "                if current_term is not None:\n",
    "                    idf = math.log10(self.doc_ids / (1 + len(current_postings)))\n",
    "                    tf_idf_postings = {}\n",
    "                    for doc_id, tf in current_postings.items():\n",
    "                        tf_weighted = math.log10(1 + tf)\n",
    "                        tfidf = tf_weighted * idf\n",
    "                        tf_idf_postings[doc_id] = {\"tf\": tf_weighted, \"idf\": idf}\n",
    "    \n",
    "                        # Acumular el cuadrado del TF-IDF en las normas temporales\n",
    "                        doc_norms_temp[doc_id] = doc_norms_temp.get(doc_id, 0) + tfidf ** 2\n",
    "\n",
    "                    \n",
    "\n",
    "                    postings_data = pickle.dumps(tf_idf_postings)\n",
    "                    postings_file.write(postings_data)\n",
    "                    temp_term_dict[current_term] = (postings_file_position, len(postings_data))\n",
    "                    # pickle.dump({current_term: (postings_file_position, len(postings_data))}, term_dict_file)\n",
    "                    postings_file_position += len(postings_data)\n",
    "\n",
    "                    #Guardamos de frente temp_term_dict\n",
    "                    pickle.dump(temp_term_dict, term_dict_file) #Guardo el diccionario\n",
    "                    temp_term_dict.clear()\n",
    "\n",
    "                    sorted_terms = list(temp_term_dict.keys())\n",
    "                    \n",
    "                    temp_term_dict.clear()\n",
    "    \n",
    "        finally:\n",
    "            for file in open_files.values():\n",
    "                file.close()\n",
    "    \n",
    "        # Guardar las normas finales tomando la ra√≠z cuadrada\n",
    "        final_norms = {int(doc_id): math.sqrt(value) for doc_id, value in doc_norms_temp.items()}\n",
    "        with open(norms_file_path, 'wb') as norms_file:\n",
    "            pickle.dump(final_norms, norms_file)\n",
    "    \n",
    "        self.term_dict_path = term_dict_path\n",
    "        self.postings_file_path = postings_file_path\n",
    "        print(f\"√çndice fusionado guardado en {postings_file_path}\")\n",
    "        print(f\"Diccionario de t√©rminos guardado en {term_dict_path}\")\n",
    "        print(f\"Normas de documentos guardadas en {norms_file_path}\")\n",
    "            # Realiza merge de todos los bloques:\n",
    "            # Manejaremos un archivo para ubicar los un indice general self.indexTerms donde se guarda la posici√≥n del termino\n",
    "            # En el otro archivo se guardaran completito tanto la palabra como el posting list pero ya ordenado\n",
    "    \n",
    "    def read_next_term(self, file):\n",
    "        line = file.readline()\n",
    "        if not line:  # Si llegamos al final del archivo\n",
    "            return None\n",
    "        term, postings_str = line.strip().split(\":\", 1)\n",
    "        postings = json.loads(postings_str)  \n",
    "        return term, postings\n",
    "\n",
    "    #--Funciones para testing--\n",
    "    def print_all_norms(self):\n",
    "        if not os.path.exists(self.norms_file_path):\n",
    "            print(\"El archivo de normas no existe. Aseg√∫rate de haber ejecutado 'spimi_invert' correctamente.\")\n",
    "            return\n",
    "\n",
    "        try:\n",
    "            with open(self.norms_file_path, 'rb') as norms_file:\n",
    "                norms = pickle.load(norms_file)\n",
    "                if not norms:\n",
    "                    print(\"El archivo de normas est√° vac√≠o.\")\n",
    "                    return\n",
    "\n",
    "                print(\"Normas de los documentos:\")\n",
    "                for doc_id, norm in sorted(norms.items()):\n",
    "                    print(f\"Documento ID: {doc_id}, Norma: {norm}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error al leer o procesar las normas: {e}\")\n",
    "\n",
    "\n",
    "    def show_terms_and_positions(self):\n",
    "        print(\"T√©rminos y sus posiciones en el archivo de postings:\")\n",
    "        all_terms = []  \n",
    "        # for term, (position, length) in term_dict.items():\n",
    "        #     print(f\"T√©rmino: '{term}', Posici√≥n: {position}, Longitud: {length}\")\n",
    "\n",
    "        for term_dict in self.load_term_dict():  \n",
    "            terms = list(term_dict.keys())\n",
    "            all_terms.extend(terms)\n",
    "        if all_terms == sorted(all_terms):\n",
    "            print(\"----->Yeeeei, Los t√©rminos est√°n ordenados alfab√©ticamente en el diccionario final.\")\n",
    "        else:\n",
    "            print(\"Los t√©rminos NO est√°n ordenados alfab√©ticamente en el diccionario final.\")\n",
    "\n",
    "\n",
    "    def show_terms_with_postings(self):\n",
    "        if not os.path.exists(self.postings_file_path):\n",
    "            print(\"El archivo de postings no existe. Ejecuta 'mergeHeap' primero.\")\n",
    "            return\n",
    "\n",
    "        print(\"T√©rminos y sus listas de postings:\")\n",
    "        try:\n",
    "            with open(self.postings_file_path, 'rb') as postings_file:\n",
    "                for term_dict in self.load_term_dict(): \n",
    "                    for term, (position, length) in term_dict.items():\n",
    "                        postings_file.seek(position)  \n",
    "                        postings_data = postings_file.read(length) \n",
    "                        postings = pickle.loads(postings_data) \n",
    "                        print(f\"T√©rmino: '{term}', Postings: {postings}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error al mostrar los t√©rminos y postings: {e}\")\n",
    "\n",
    "\n",
    "    def retrieval(self, query, k):\n",
    "        # Cargamos doc_ids si no est√° ya cargado\n",
    "        if self.doc_ids is None:\n",
    "            self.load_doc_ids()\n",
    "    \n",
    "        if self.doc_ids == 0:  \n",
    "            raise ValueError(\"El √≠ndice no est√° construido o no contiene documentos. Ejecuta 'spimi_invert' antes de realizar consultas.\")\n",
    "        print(\"Se carg√≥ en total: \", self.doc_ids, \" documentos\")\n",
    "    \n",
    "        # Preprocesar la consulta\n",
    "        terms = preprocesamiento(query)\n",
    "    \n",
    "        tf_query = {}\n",
    "        for term in terms:\n",
    "            if term in tf_query:\n",
    "                tf_query[term] += 1\n",
    "            else:\n",
    "                tf_query[term] = 1\n",
    "    \n",
    "        tfidf_query = {}\n",
    "        norm_query = 0\n",
    "    \n",
    "        scores = [0] * self.doc_ids  \n",
    "    \n",
    "        with open(self.postings_file_path, 'rb') as postings_file, \\\n",
    "             open(self.norms_file_path, 'rb') as norms_file:\n",
    "    \n",
    "            for term_dict_block in self.load_term_dict(): # bloque por bloque gracias a yield\n",
    "                for term, tf in tf_query.items():\n",
    "                    if term in term_dict_block:  # Considerar t√©rminos presentes en este bloque\n",
    "                        position, length = term_dict_block[term]\n",
    "    \n",
    "                        postings_file.seek(position)\n",
    "                        postings_data = postings_file.read(length)\n",
    "                        postings = pickle.loads(postings_data)  # Diccionario {doc_id: {'tf': tf, 'idf': idf}}\n",
    "                        idf = math.log10(self.doc_ids / (1 + len(postings)))\n",
    "    \n",
    "                        tfidf_query[term] = math.log10(1 + tf) * idf\n",
    "                        norm_query += (tfidf_query[term]) ** 2\n",
    "    \n",
    "                        w_tq = tfidf_query[term]\n",
    "                        for doc_id_str, values in postings.items():\n",
    "                            doc_id = int(doc_id_str)\n",
    "                            tfidf_td = values['tf'] * values['idf']\n",
    "                            scores[doc_id] += tfidf_td * w_tq \n",
    "    \n",
    "            norm_query = math.sqrt(norm_query)\n",
    "    \n",
    "            norms_file.seek(0)\n",
    "            norms = pickle.load(norms_file)\n",
    "    \n",
    "            for doc_id in range(self.doc_ids):\n",
    "                doc_norm = norms.get(doc_id, 0)\n",
    "                if doc_norm != 0:\n",
    "                    scores[doc_id] /= (doc_norm * norm_query)\n",
    "    \n",
    "        result = sorted(enumerate(scores), key=lambda x: x[1], reverse=True)\n",
    "        return result[:k]\n",
    "    \n",
    "    \n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creando y ordenando cada bloque\n",
      "Iniciando con el merge\n",
      "√çndice fusionado guardado en index_blocks_merge\\postings.bin\n",
      "Diccionario de t√©rminos guardado en index_blocks_merge\\term_dict.pkl\n",
      "Normas de documentos guardadas en index_blocks_merge\\document_norms.pkl\n",
      "\n",
      "Resumen de tiempos:\n",
      "Tiempo para la creaci√≥n Creaci√≥n  y el ordenamiento de los bloques: 28.62 segundos\n",
      "Tiempo para la ejecuci√≥n del merge: 1.63 segundos\n",
      "Tiempo total para la creaci√≥n y el mergeo: 30.24 segundos\n"
     ]
    }
   ],
   "source": [
    "# Path\n",
    "\n",
    "# Create table tablita from path \"takataka\"\n",
    "# ---------Creation---------\n",
    "path = \"spotify_2000.csv\"\n",
    "#--Columnas de indexacion: \n",
    "# Select \"track_name\",\"track_artist\",\"lyrics\", \"track_album_name\"\n",
    "columnas = [\"track_name\",\"track_artist\",\"lyrics\", \"track_album_name\"]\n",
    "\n",
    "s = SPIMI(path, columnas=columnas)  \n",
    "s.spimi_invert() \n",
    "\n",
    "# ---------End Creation---------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Se carg√≥ en total:  2000  documentos\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>track_id</th>\n",
       "      <th>track_name</th>\n",
       "      <th>track_artist</th>\n",
       "      <th>lyrics</th>\n",
       "      <th>track_popularity</th>\n",
       "      <th>track_album_id</th>\n",
       "      <th>track_album_name</th>\n",
       "      <th>track_album_release_date</th>\n",
       "      <th>playlist_name</th>\n",
       "      <th>playlist_id</th>\n",
       "      <th>...</th>\n",
       "      <th>mode</th>\n",
       "      <th>speechiness</th>\n",
       "      <th>acousticness</th>\n",
       "      <th>instrumentalness</th>\n",
       "      <th>liveness</th>\n",
       "      <th>valence</th>\n",
       "      <th>tempo</th>\n",
       "      <th>duration_ms</th>\n",
       "      <th>language</th>\n",
       "      <th>score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>371</th>\n",
       "      <td>0aCjE4cpIQxLQcaNAj1xgW</td>\n",
       "      <td>Ronca</td>\n",
       "      <td>Don Omar</td>\n",
       "      <td>El mejor DJ del genero del reggaeton... Mejor ...</td>\n",
       "      <td>3</td>\n",
       "      <td>3hKDEHUzH1GmdohemezsEw</td>\n",
       "      <td>The Gold Series \"The Last Don\"</td>\n",
       "      <td>2004-01-01</td>\n",
       "      <td>This Is: Don Omar</td>\n",
       "      <td>37i9dQZF1DWYBUdckfg1va</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0.1990</td>\n",
       "      <td>0.21900</td>\n",
       "      <td>0.002050</td>\n",
       "      <td>0.0536</td>\n",
       "      <td>0.715</td>\n",
       "      <td>173.976</td>\n",
       "      <td>301133</td>\n",
       "      <td>es</td>\n",
       "      <td>0.073541</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1228</th>\n",
       "      <td>0LcnZnhSQsoa9Dc51tj3wa</td>\n",
       "      <td>Quien La Vio Llorar</td>\n",
       "      <td>Don Omar</td>\n",
       "      <td>Contigo sufrio Conmigo vivio Y con tus mentira...</td>\n",
       "      <td>3</td>\n",
       "      <td>3hKDEHUzH1GmdohemezsEw</td>\n",
       "      <td>The Gold Series \"The Last Don\"</td>\n",
       "      <td>2004-01-01</td>\n",
       "      <td>This Is: Don Omar</td>\n",
       "      <td>37i9dQZF1DWYBUdckfg1va</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0361</td>\n",
       "      <td>0.10100</td>\n",
       "      <td>0.000040</td>\n",
       "      <td>0.3540</td>\n",
       "      <td>0.376</td>\n",
       "      <td>79.993</td>\n",
       "      <td>185613</td>\n",
       "      <td>es</td>\n",
       "      <td>0.070400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>552</th>\n",
       "      <td>0cGKcnXQmiyU5WKubrEmDh</td>\n",
       "      <td>Belly Danza - Remastered 2016</td>\n",
       "      <td>Don Omar</td>\n",
       "      <td>Ohhhhh no Ohhhhh no Ohhhhh na na na na na na n...</td>\n",
       "      <td>46</td>\n",
       "      <td>1jvfABr5t081rCAjajFQey</td>\n",
       "      <td>King Of Kings 10th Anniversary (Remastered)</td>\n",
       "      <td>2016-11-11</td>\n",
       "      <td>This Is: Don Omar</td>\n",
       "      <td>37i9dQZF1DWYBUdckfg1va</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0.2380</td>\n",
       "      <td>0.04180</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0818</td>\n",
       "      <td>0.913</td>\n",
       "      <td>109.987</td>\n",
       "      <td>245827</td>\n",
       "      <td>en</td>\n",
       "      <td>0.070194</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>127</th>\n",
       "      <td>03wOwEbTS3JR2Y2U0TxkFx</td>\n",
       "      <td>Encanto</td>\n",
       "      <td>Don Omar</td>\n",
       "      <td>NA Desde que te habl√© sab√≠a que eras especial,...</td>\n",
       "      <td>49</td>\n",
       "      <td>0n2jgERAc9G8viAvRDwaJh</td>\n",
       "      <td>Encanto</td>\n",
       "      <td>2017-03-24</td>\n",
       "      <td>This Is: Don Omar</td>\n",
       "      <td>37i9dQZF1DWYBUdckfg1va</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0499</td>\n",
       "      <td>0.14500</td>\n",
       "      <td>0.000503</td>\n",
       "      <td>0.0752</td>\n",
       "      <td>0.902</td>\n",
       "      <td>172.030</td>\n",
       "      <td>195400</td>\n",
       "      <td>es</td>\n",
       "      <td>0.068827</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>476</th>\n",
       "      <td>0bMRpUsk1IWi5O2NIkWsqR</td>\n",
       "      <td>Mayor Que Yo 3</td>\n",
       "      <td>Luny Tunes</td>\n",
       "      <td>Letra de \"Mayor Que Yo 3\" ft. Don Omar, Wisin,...</td>\n",
       "      <td>52</td>\n",
       "      <td>4GMgJaHCYCH8YVqEYZ0NxA</td>\n",
       "      <td>Dance Latin # 1 Hits</td>\n",
       "      <td>2016-04-22</td>\n",
       "      <td>This Is: Don Omar</td>\n",
       "      <td>37i9dQZF1DWYBUdckfg1va</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0.1130</td>\n",
       "      <td>0.09180</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.2120</td>\n",
       "      <td>0.458</td>\n",
       "      <td>92.006</td>\n",
       "      <td>309267</td>\n",
       "      <td>es</td>\n",
       "      <td>0.066932</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>450</th>\n",
       "      <td>0BenZn1xELuxhb7Pjwyjxv</td>\n",
       "      <td>Hooka</td>\n",
       "      <td>Don Omar</td>\n",
       "      <td>Letra de \"Hookah\" ft. Plan B ¬°Hooka! (plo-plo-...</td>\n",
       "      <td>60</td>\n",
       "      <td>6mGDfbDErYIJsmSewvccWm</td>\n",
       "      <td>Meet The Orphans</td>\n",
       "      <td>2010-01-01</td>\n",
       "      <td>This Is: Don Omar</td>\n",
       "      <td>37i9dQZF1DWYBUdckfg1va</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0.1540</td>\n",
       "      <td>0.03730</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.4910</td>\n",
       "      <td>0.818</td>\n",
       "      <td>170.014</td>\n",
       "      <td>237973</td>\n",
       "      <td>es</td>\n",
       "      <td>0.064318</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>01FBBpA6WBl3arX7pxTFwO</td>\n",
       "      <td>Not To Much - Remastered 2016</td>\n",
       "      <td>Don Omar</td>\n",
       "      <td>Ella empezo de espacio ganandome terreno Y yo ...</td>\n",
       "      <td>44</td>\n",
       "      <td>1jvfABr5t081rCAjajFQey</td>\n",
       "      <td>King Of Kings 10th Anniversary (Remastered)</td>\n",
       "      <td>2016-11-11</td>\n",
       "      <td>This Is: Don Omar</td>\n",
       "      <td>37i9dQZF1DWYBUdckfg1va</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0564</td>\n",
       "      <td>0.01970</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.2030</td>\n",
       "      <td>0.261</td>\n",
       "      <td>92.004</td>\n",
       "      <td>211520</td>\n",
       "      <td>es</td>\n",
       "      <td>0.060960</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1309</th>\n",
       "      <td>0Mk48GaMgw0qz0auSymuHd</td>\n",
       "      <td>Perdido En Tus Ojos</td>\n",
       "      <td>Don Omar</td>\n",
       "      <td>NA Todo comenz√≥ cuando nos miramos, hubo una q...</td>\n",
       "      <td>58</td>\n",
       "      <td>57mWSm5UtRGT08KeJuyZqu</td>\n",
       "      <td>The Last Don II</td>\n",
       "      <td>2015-06-16</td>\n",
       "      <td>Fiesta Latina Mix üéàüéâüíÉüèªüï∫üèª‚òÄÔ∏èüèñ</td>\n",
       "      <td>2kY6lVc5EcVfI5WNKmPQQG</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0312</td>\n",
       "      <td>0.16900</td>\n",
       "      <td>0.000002</td>\n",
       "      <td>0.2260</td>\n",
       "      <td>0.787</td>\n",
       "      <td>95.986</td>\n",
       "      <td>234080</td>\n",
       "      <td>es</td>\n",
       "      <td>0.037183</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>364</th>\n",
       "      <td>0A7uLa2SqVK5s5Y2V3kCyW</td>\n",
       "      <td>Good Looking</td>\n",
       "      <td>Don Omar</td>\n",
       "      <td>Don Danny, Danny Fornaris Luny Tunes El Orfana...</td>\n",
       "      <td>56</td>\n",
       "      <td>6mGDfbDErYIJsmSewvccWm</td>\n",
       "      <td>Meet The Orphans</td>\n",
       "      <td>2010-01-01</td>\n",
       "      <td>This Is: Don Omar</td>\n",
       "      <td>37i9dQZF1DWYBUdckfg1va</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0.1560</td>\n",
       "      <td>0.00879</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.8140</td>\n",
       "      <td>0.757</td>\n",
       "      <td>93.013</td>\n",
       "      <td>258613</td>\n",
       "      <td>es</td>\n",
       "      <td>0.035139</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0017A6SJgTbfQVU2EtsPNo</td>\n",
       "      <td>Pangarap</td>\n",
       "      <td>Barbie's Cradle</td>\n",
       "      <td>Minsan pa Nang ako'y napalingon Hindi ko alam ...</td>\n",
       "      <td>41</td>\n",
       "      <td>1srJQ0njEQgd8w4XSqI4JQ</td>\n",
       "      <td>Trip</td>\n",
       "      <td>2001-01-01</td>\n",
       "      <td>Pinoy Classic Rock</td>\n",
       "      <td>37i9dQZF1DWYDQ8wBxd7xt</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0236</td>\n",
       "      <td>0.27900</td>\n",
       "      <td>0.011700</td>\n",
       "      <td>0.0887</td>\n",
       "      <td>0.566</td>\n",
       "      <td>97.091</td>\n",
       "      <td>235440</td>\n",
       "      <td>tl</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>004s3t0ONYlzxII9PLgU6z</td>\n",
       "      <td>I Feel Alive</td>\n",
       "      <td>Steady Rollin</td>\n",
       "      <td>The trees, are singing in the wind The sky blu...</td>\n",
       "      <td>28</td>\n",
       "      <td>3z04Lb9Dsilqw68SHt6jLB</td>\n",
       "      <td>Love &amp; Loss</td>\n",
       "      <td>2017-11-21</td>\n",
       "      <td>Hard Rock Workout</td>\n",
       "      <td>3YouF0u7waJnolytf9JCXf</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0442</td>\n",
       "      <td>0.01170</td>\n",
       "      <td>0.009940</td>\n",
       "      <td>0.3470</td>\n",
       "      <td>0.404</td>\n",
       "      <td>135.225</td>\n",
       "      <td>373512</td>\n",
       "      <td>en</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>00chLpzhgVjxs1zKC9UScL</td>\n",
       "      <td>Poison</td>\n",
       "      <td>Bell Biv DeVoe</td>\n",
       "      <td>NA Yeah, Spyderman and Freeze in full effect U...</td>\n",
       "      <td>0</td>\n",
       "      <td>6oZ6brjB8x3GoeSYdwJdPc</td>\n",
       "      <td>Gold</td>\n",
       "      <td>2005-01-01</td>\n",
       "      <td>Back in the day - R&amp;B, New Jack Swing, Swingbe...</td>\n",
       "      <td>3a9y4eeCJRmG9p4YKfqYIx</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0.2160</td>\n",
       "      <td>0.00432</td>\n",
       "      <td>0.007230</td>\n",
       "      <td>0.4890</td>\n",
       "      <td>0.650</td>\n",
       "      <td>111.904</td>\n",
       "      <td>262467</td>\n",
       "      <td>en</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>00cqd6ZsSkLZqGMlQCR0Zo</td>\n",
       "      <td>Baby It's Cold Outside (feat. Christina Aguilera)</td>\n",
       "      <td>CeeLo Green</td>\n",
       "      <td>I really can't stay Baby it's cold outside I'v...</td>\n",
       "      <td>41</td>\n",
       "      <td>3ssspRe42CXkhPxdc12xcp</td>\n",
       "      <td>CeeLo's Magic Moment</td>\n",
       "      <td>2012-10-29</td>\n",
       "      <td>Christmas Soul</td>\n",
       "      <td>6FZYc2BvF7tColxO8PBShV</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0341</td>\n",
       "      <td>0.68900</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0664</td>\n",
       "      <td>0.405</td>\n",
       "      <td>118.593</td>\n",
       "      <td>243067</td>\n",
       "      <td>en</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>00emjlCv9azBN0fzuuyLqy</td>\n",
       "      <td>Dumb Litty</td>\n",
       "      <td>KARD</td>\n",
       "      <td>Get up out of my business You don't keep me fr...</td>\n",
       "      <td>65</td>\n",
       "      <td>7h5X3xhh3peIK9Y0qI5hbK</td>\n",
       "      <td>KARD 2nd Digital Single ‚ÄòDumb Litty‚Äô</td>\n",
       "      <td>2019-09-22</td>\n",
       "      <td>K-Party Dance Mix</td>\n",
       "      <td>37i9dQZF1DX4RDXswvP6Mj</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0409</td>\n",
       "      <td>0.03700</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.1380</td>\n",
       "      <td>0.240</td>\n",
       "      <td>130.018</td>\n",
       "      <td>193160</td>\n",
       "      <td>en</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>00f9VGHfQhAHMCQ2bSjg3D</td>\n",
       "      <td>Soldier</td>\n",
       "      <td>James TW</td>\n",
       "      <td>Hold your breath, don't look down, keep trying...</td>\n",
       "      <td>70</td>\n",
       "      <td>3GNzXsFbzdwM0WKCZtgeNP</td>\n",
       "      <td>Chapters</td>\n",
       "      <td>2019-04-26</td>\n",
       "      <td>urban contemporary</td>\n",
       "      <td>4WiB26kw0INKwbzfb5M6Tv</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0550</td>\n",
       "      <td>0.28000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0975</td>\n",
       "      <td>0.305</td>\n",
       "      <td>147.764</td>\n",
       "      <td>224720</td>\n",
       "      <td>en</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>00FROhC5g4iJdax5US8jRr</td>\n",
       "      <td>Satisfy You</td>\n",
       "      <td>Diddy</td>\n",
       "      <td>All I want is somebody who's gonna love me for...</td>\n",
       "      <td>52</td>\n",
       "      <td>2dHr0LpUe6CNV5lNsr8x0W</td>\n",
       "      <td>Forever</td>\n",
       "      <td>1999-08-24</td>\n",
       "      <td>Swingbeat (old skool), New Jack Swing, R&amp;B, Hi...</td>\n",
       "      <td>3krpccUV68nBGAQbvHEZDC</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0.1850</td>\n",
       "      <td>0.59100</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.1450</td>\n",
       "      <td>0.695</td>\n",
       "      <td>87.261</td>\n",
       "      <td>286441</td>\n",
       "      <td>en</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>00GfGwzlSB8DoA0cDP2Eit</td>\n",
       "      <td>Tender Lover</td>\n",
       "      <td>Babyface</td>\n",
       "      <td>Feels good Everybody Tender lover Tender love ...</td>\n",
       "      <td>36</td>\n",
       "      <td>51fAXJ5bMn7DRSunXQ6PMb</td>\n",
       "      <td>Tender Lover</td>\n",
       "      <td>1989-07-07</td>\n",
       "      <td>New Jack Swing</td>\n",
       "      <td>3ykXidKLz1eYPvuGoFlD1e</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0445</td>\n",
       "      <td>0.22600</td>\n",
       "      <td>0.000422</td>\n",
       "      <td>0.0513</td>\n",
       "      <td>0.687</td>\n",
       "      <td>102.459</td>\n",
       "      <td>259267</td>\n",
       "      <td>en</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>00Gu3RMpDW2vO9PjlMVFDL</td>\n",
       "      <td>Hide Away (feat. Envy Monroe)</td>\n",
       "      <td>Blasterjaxx</td>\n",
       "      <td>Don't run away, it's getting colder Our hearts...</td>\n",
       "      <td>42</td>\n",
       "      <td>5pqG85igfoeWcCDIsSi9x7</td>\n",
       "      <td>Hide Away (feat. Envy Monroe)</td>\n",
       "      <td>2019-06-21</td>\n",
       "      <td>Big Room EDM - by Spinnin' Records</td>\n",
       "      <td>7xWdFCrU5Gka6qp1ODrSdK</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0421</td>\n",
       "      <td>0.02490</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.3610</td>\n",
       "      <td>0.134</td>\n",
       "      <td>130.001</td>\n",
       "      <td>188000</td>\n",
       "      <td>en</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>00GxbkrW4m1Tac5xySEJ4M</td>\n",
       "      <td>Ti volevo dedicare (feat. J-AX &amp; Boomdabash)</td>\n",
       "      <td>Rocco Hunt</td>\n",
       "      <td>Ho una cosa da dirti da tempo Ma‚Ää non ho mai t...</td>\n",
       "      <td>78</td>\n",
       "      <td>57L1NgMlfxscOxHhmfLjqg</td>\n",
       "      <td>Libert√†</td>\n",
       "      <td>2019-08-30</td>\n",
       "      <td>Musica Italiana 2020 - Playlist Pop &amp; Hip-Hop ...</td>\n",
       "      <td>6kVFIQBhLT4003iw2WWEv1</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0661</td>\n",
       "      <td>0.01040</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.1920</td>\n",
       "      <td>0.271</td>\n",
       "      <td>120.002</td>\n",
       "      <td>208133</td>\n",
       "      <td>it</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>00hdjyXt6MohKnCyDmhxOL</td>\n",
       "      <td>Una Vaina Loca</td>\n",
       "      <td>Fuego</td>\n",
       "      <td>Fuego Uoh uoh uoh La musiica del futuroo Yeah!...</td>\n",
       "      <td>1</td>\n",
       "      <td>01nV3KuocS1NJHTsJbPkTO</td>\n",
       "      <td>Una Vaina Loca</td>\n",
       "      <td>2011-11-02</td>\n",
       "      <td>MIX LATIN POP¬∞</td>\n",
       "      <td>6IS6XTdbS9qJZgfjNKgpB8</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0361</td>\n",
       "      <td>0.11400</td>\n",
       "      <td>0.000080</td>\n",
       "      <td>0.0620</td>\n",
       "      <td>0.642</td>\n",
       "      <td>117.009</td>\n",
       "      <td>188213</td>\n",
       "      <td>es</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>20 rows √ó 26 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                    track_id  \\\n",
       "371   0aCjE4cpIQxLQcaNAj1xgW   \n",
       "1228  0LcnZnhSQsoa9Dc51tj3wa   \n",
       "552   0cGKcnXQmiyU5WKubrEmDh   \n",
       "127   03wOwEbTS3JR2Y2U0TxkFx   \n",
       "476   0bMRpUsk1IWi5O2NIkWsqR   \n",
       "450   0BenZn1xELuxhb7Pjwyjxv   \n",
       "46    01FBBpA6WBl3arX7pxTFwO   \n",
       "1309  0Mk48GaMgw0qz0auSymuHd   \n",
       "364   0A7uLa2SqVK5s5Y2V3kCyW   \n",
       "0     0017A6SJgTbfQVU2EtsPNo   \n",
       "1     004s3t0ONYlzxII9PLgU6z   \n",
       "2     00chLpzhgVjxs1zKC9UScL   \n",
       "3     00cqd6ZsSkLZqGMlQCR0Zo   \n",
       "4     00emjlCv9azBN0fzuuyLqy   \n",
       "5     00f9VGHfQhAHMCQ2bSjg3D   \n",
       "6     00FROhC5g4iJdax5US8jRr   \n",
       "7     00GfGwzlSB8DoA0cDP2Eit   \n",
       "8     00Gu3RMpDW2vO9PjlMVFDL   \n",
       "9     00GxbkrW4m1Tac5xySEJ4M   \n",
       "10    00hdjyXt6MohKnCyDmhxOL   \n",
       "\n",
       "                                             track_name     track_artist  \\\n",
       "371                                               Ronca         Don Omar   \n",
       "1228                                Quien La Vio Llorar         Don Omar   \n",
       "552                       Belly Danza - Remastered 2016         Don Omar   \n",
       "127                                             Encanto         Don Omar   \n",
       "476                                      Mayor Que Yo 3       Luny Tunes   \n",
       "450                                               Hooka         Don Omar   \n",
       "46                        Not To Much - Remastered 2016         Don Omar   \n",
       "1309                                Perdido En Tus Ojos         Don Omar   \n",
       "364                                        Good Looking         Don Omar   \n",
       "0                                              Pangarap  Barbie's Cradle   \n",
       "1                                          I Feel Alive    Steady Rollin   \n",
       "2                                                Poison   Bell Biv DeVoe   \n",
       "3     Baby It's Cold Outside (feat. Christina Aguilera)      CeeLo Green   \n",
       "4                                            Dumb Litty             KARD   \n",
       "5                                               Soldier         James TW   \n",
       "6                                           Satisfy You            Diddy   \n",
       "7                                          Tender Lover         Babyface   \n",
       "8                         Hide Away (feat. Envy Monroe)      Blasterjaxx   \n",
       "9          Ti volevo dedicare (feat. J-AX & Boomdabash)       Rocco Hunt   \n",
       "10                                       Una Vaina Loca            Fuego   \n",
       "\n",
       "                                                 lyrics  track_popularity  \\\n",
       "371   El mejor DJ del genero del reggaeton... Mejor ...                 3   \n",
       "1228  Contigo sufrio Conmigo vivio Y con tus mentira...                 3   \n",
       "552   Ohhhhh no Ohhhhh no Ohhhhh na na na na na na n...                46   \n",
       "127   NA Desde que te habl√© sab√≠a que eras especial,...                49   \n",
       "476   Letra de \"Mayor Que Yo 3\" ft. Don Omar, Wisin,...                52   \n",
       "450   Letra de \"Hookah\" ft. Plan B ¬°Hooka! (plo-plo-...                60   \n",
       "46    Ella empezo de espacio ganandome terreno Y yo ...                44   \n",
       "1309  NA Todo comenz√≥ cuando nos miramos, hubo una q...                58   \n",
       "364   Don Danny, Danny Fornaris Luny Tunes El Orfana...                56   \n",
       "0     Minsan pa Nang ako'y napalingon Hindi ko alam ...                41   \n",
       "1     The trees, are singing in the wind The sky blu...                28   \n",
       "2     NA Yeah, Spyderman and Freeze in full effect U...                 0   \n",
       "3     I really can't stay Baby it's cold outside I'v...                41   \n",
       "4     Get up out of my business You don't keep me fr...                65   \n",
       "5     Hold your breath, don't look down, keep trying...                70   \n",
       "6     All I want is somebody who's gonna love me for...                52   \n",
       "7     Feels good Everybody Tender lover Tender love ...                36   \n",
       "8     Don't run away, it's getting colder Our hearts...                42   \n",
       "9     Ho una cosa da dirti da tempo Ma‚Ää non ho mai t...                78   \n",
       "10    Fuego Uoh uoh uoh La musiica del futuroo Yeah!...                 1   \n",
       "\n",
       "              track_album_id                             track_album_name  \\\n",
       "371   3hKDEHUzH1GmdohemezsEw               The Gold Series \"The Last Don\"   \n",
       "1228  3hKDEHUzH1GmdohemezsEw               The Gold Series \"The Last Don\"   \n",
       "552   1jvfABr5t081rCAjajFQey  King Of Kings 10th Anniversary (Remastered)   \n",
       "127   0n2jgERAc9G8viAvRDwaJh                                      Encanto   \n",
       "476   4GMgJaHCYCH8YVqEYZ0NxA                         Dance Latin # 1 Hits   \n",
       "450   6mGDfbDErYIJsmSewvccWm                             Meet The Orphans   \n",
       "46    1jvfABr5t081rCAjajFQey  King Of Kings 10th Anniversary (Remastered)   \n",
       "1309  57mWSm5UtRGT08KeJuyZqu                              The Last Don II   \n",
       "364   6mGDfbDErYIJsmSewvccWm                             Meet The Orphans   \n",
       "0     1srJQ0njEQgd8w4XSqI4JQ                                         Trip   \n",
       "1     3z04Lb9Dsilqw68SHt6jLB                                  Love & Loss   \n",
       "2     6oZ6brjB8x3GoeSYdwJdPc                                         Gold   \n",
       "3     3ssspRe42CXkhPxdc12xcp                         CeeLo's Magic Moment   \n",
       "4     7h5X3xhh3peIK9Y0qI5hbK         KARD 2nd Digital Single ‚ÄòDumb Litty‚Äô   \n",
       "5     3GNzXsFbzdwM0WKCZtgeNP                                     Chapters   \n",
       "6     2dHr0LpUe6CNV5lNsr8x0W                                      Forever   \n",
       "7     51fAXJ5bMn7DRSunXQ6PMb                                 Tender Lover   \n",
       "8     5pqG85igfoeWcCDIsSi9x7                Hide Away (feat. Envy Monroe)   \n",
       "9     57L1NgMlfxscOxHhmfLjqg                                      Libert√†   \n",
       "10    01nV3KuocS1NJHTsJbPkTO                               Una Vaina Loca   \n",
       "\n",
       "     track_album_release_date  \\\n",
       "371                2004-01-01   \n",
       "1228               2004-01-01   \n",
       "552                2016-11-11   \n",
       "127                2017-03-24   \n",
       "476                2016-04-22   \n",
       "450                2010-01-01   \n",
       "46                 2016-11-11   \n",
       "1309               2015-06-16   \n",
       "364                2010-01-01   \n",
       "0                  2001-01-01   \n",
       "1                  2017-11-21   \n",
       "2                  2005-01-01   \n",
       "3                  2012-10-29   \n",
       "4                  2019-09-22   \n",
       "5                  2019-04-26   \n",
       "6                  1999-08-24   \n",
       "7                  1989-07-07   \n",
       "8                  2019-06-21   \n",
       "9                  2019-08-30   \n",
       "10                 2011-11-02   \n",
       "\n",
       "                                          playlist_name  \\\n",
       "371                                   This Is: Don Omar   \n",
       "1228                                  This Is: Don Omar   \n",
       "552                                   This Is: Don Omar   \n",
       "127                                   This Is: Don Omar   \n",
       "476                                   This Is: Don Omar   \n",
       "450                                   This Is: Don Omar   \n",
       "46                                    This Is: Don Omar   \n",
       "1309                        Fiesta Latina Mix üéàüéâüíÉüèªüï∫üèª‚òÄÔ∏èüèñ   \n",
       "364                                   This Is: Don Omar   \n",
       "0                                    Pinoy Classic Rock   \n",
       "1                                     Hard Rock Workout   \n",
       "2     Back in the day - R&B, New Jack Swing, Swingbe...   \n",
       "3                                        Christmas Soul   \n",
       "4                                     K-Party Dance Mix   \n",
       "5                                    urban contemporary   \n",
       "6     Swingbeat (old skool), New Jack Swing, R&B, Hi...   \n",
       "7                                        New Jack Swing   \n",
       "8                    Big Room EDM - by Spinnin' Records   \n",
       "9     Musica Italiana 2020 - Playlist Pop & Hip-Hop ...   \n",
       "10                                       MIX LATIN POP¬∞   \n",
       "\n",
       "                 playlist_id  ... mode speechiness  acousticness  \\\n",
       "371   37i9dQZF1DWYBUdckfg1va  ...    1      0.1990       0.21900   \n",
       "1228  37i9dQZF1DWYBUdckfg1va  ...    0      0.0361       0.10100   \n",
       "552   37i9dQZF1DWYBUdckfg1va  ...    0      0.2380       0.04180   \n",
       "127   37i9dQZF1DWYBUdckfg1va  ...    1      0.0499       0.14500   \n",
       "476   37i9dQZF1DWYBUdckfg1va  ...    0      0.1130       0.09180   \n",
       "450   37i9dQZF1DWYBUdckfg1va  ...    1      0.1540       0.03730   \n",
       "46    37i9dQZF1DWYBUdckfg1va  ...    0      0.0564       0.01970   \n",
       "1309  2kY6lVc5EcVfI5WNKmPQQG  ...    1      0.0312       0.16900   \n",
       "364   37i9dQZF1DWYBUdckfg1va  ...    1      0.1560       0.00879   \n",
       "0     37i9dQZF1DWYDQ8wBxd7xt  ...    1      0.0236       0.27900   \n",
       "1     3YouF0u7waJnolytf9JCXf  ...    1      0.0442       0.01170   \n",
       "2     3a9y4eeCJRmG9p4YKfqYIx  ...    0      0.2160       0.00432   \n",
       "3     6FZYc2BvF7tColxO8PBShV  ...    0      0.0341       0.68900   \n",
       "4     37i9dQZF1DX4RDXswvP6Mj  ...    1      0.0409       0.03700   \n",
       "5     4WiB26kw0INKwbzfb5M6Tv  ...    1      0.0550       0.28000   \n",
       "6     3krpccUV68nBGAQbvHEZDC  ...    1      0.1850       0.59100   \n",
       "7     3ykXidKLz1eYPvuGoFlD1e  ...    1      0.0445       0.22600   \n",
       "8     7xWdFCrU5Gka6qp1ODrSdK  ...    1      0.0421       0.02490   \n",
       "9     6kVFIQBhLT4003iw2WWEv1  ...    1      0.0661       0.01040   \n",
       "10    6IS6XTdbS9qJZgfjNKgpB8  ...    1      0.0361       0.11400   \n",
       "\n",
       "      instrumentalness  liveness  valence    tempo  duration_ms  language  \\\n",
       "371           0.002050    0.0536    0.715  173.976       301133        es   \n",
       "1228          0.000040    0.3540    0.376   79.993       185613        es   \n",
       "552           0.000000    0.0818    0.913  109.987       245827        en   \n",
       "127           0.000503    0.0752    0.902  172.030       195400        es   \n",
       "476           0.000000    0.2120    0.458   92.006       309267        es   \n",
       "450           0.000000    0.4910    0.818  170.014       237973        es   \n",
       "46            0.000000    0.2030    0.261   92.004       211520        es   \n",
       "1309          0.000002    0.2260    0.787   95.986       234080        es   \n",
       "364           0.000000    0.8140    0.757   93.013       258613        es   \n",
       "0             0.011700    0.0887    0.566   97.091       235440        tl   \n",
       "1             0.009940    0.3470    0.404  135.225       373512        en   \n",
       "2             0.007230    0.4890    0.650  111.904       262467        en   \n",
       "3             0.000000    0.0664    0.405  118.593       243067        en   \n",
       "4             0.000000    0.1380    0.240  130.018       193160        en   \n",
       "5             0.000000    0.0975    0.305  147.764       224720        en   \n",
       "6             0.000000    0.1450    0.695   87.261       286441        en   \n",
       "7             0.000422    0.0513    0.687  102.459       259267        en   \n",
       "8             0.000000    0.3610    0.134  130.001       188000        en   \n",
       "9             0.000000    0.1920    0.271  120.002       208133        it   \n",
       "10            0.000080    0.0620    0.642  117.009       188213        es   \n",
       "\n",
       "         score  \n",
       "371   0.073541  \n",
       "1228  0.070400  \n",
       "552   0.070194  \n",
       "127   0.068827  \n",
       "476   0.066932  \n",
       "450   0.064318  \n",
       "46    0.060960  \n",
       "1309  0.037183  \n",
       "364   0.035139  \n",
       "0     0.000000  \n",
       "1     0.000000  \n",
       "2     0.000000  \n",
       "3     0.000000  \n",
       "4     0.000000  \n",
       "5     0.000000  \n",
       "6     0.000000  \n",
       "7     0.000000  \n",
       "8     0.000000  \n",
       "9     0.000000  \n",
       "10    0.000000  \n",
       "\n",
       "[20 rows x 26 columns]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\n",
    "query = \"don omar\"\n",
    "top_k = 20\n",
    "\n",
    "result = s.retrieval(query, top_k)\n",
    "\n",
    "getResultadosDF(result, s.dataset_path, s.disk_limit) \n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
