{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Proyecto jiji"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: kagglehub in c:\\users\\ce mar\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (0.3.4)\n",
      "Requirement already satisfied: tqdm in c:\\users\\ce mar\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from kagglehub) (4.66.1)\n",
      "Requirement already satisfied: requests in c:\\users\\ce mar\\appdata\\roaming\\python\\python310\\site-packages (from kagglehub) (2.28.1)\n",
      "Requirement already satisfied: packaging in c:\\users\\ce mar\\appdata\\roaming\\python\\python310\\site-packages (from kagglehub) (23.2)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in c:\\users\\ce mar\\appdata\\roaming\\python\\python310\\site-packages (from requests->kagglehub) (2.1.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\ce mar\\appdata\\roaming\\python\\python310\\site-packages (from requests->kagglehub) (2022.12.7)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\ce mar\\appdata\\roaming\\python\\python310\\site-packages (from requests->kagglehub) (1.26.13)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\ce mar\\appdata\\roaming\\python\\python310\\site-packages (from requests->kagglehub) (3.4)\n",
      "Requirement already satisfied: colorama in c:\\users\\ce mar\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from tqdm->kagglehub) (0.4.6)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip available: 22.2.1 -> 24.3.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "!pip install kagglehub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to C:\\Users\\Ce\n",
      "[nltk_data]     mar\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "import numpy as np\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "nltk.download('punkt')\n",
    "import regex as re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import math\n",
    "import sys\n",
    "import dbm\n",
    "stemmer = SnowballStemmer('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"stopwords-en.txt\", encoding=\"latin1\") as file:\n",
    "   stoplist = [line.rstrip().lower() for line in file]\n",
    "stoplist += ['?', '-', '.', ':', ',', '!', ';']\n",
    "\n",
    "def preprocesamiento(texto, stemming=True):\n",
    "  words = []\n",
    "  texto = str(texto)\n",
    "  texto = texto.lower()\n",
    "  texto = re.sub(r'[^a-zA-Z0-9_À-ÿ]', ' ', texto)\n",
    "  # tokenizar\n",
    "  words = nltk.word_tokenize(texto, language='spanish')\n",
    "  # filtrar stopwords\n",
    "  words = [word for word in words if word not in stoplist]\n",
    "  # reducir palabras (stemming)\n",
    "  if stemming:\n",
    "      words = [stemmer.stem(word) for word in words]\n",
    "  return words\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "def OrdenarPorBloques(dir_blocks, n_blocks):\n",
    "    for i in range(n_blocks):\n",
    "        file_path = os.path.join(dir_blocks, 'block_{}.pkl'.format(i))\n",
    "        with open(file_path, 'rb') as f:\n",
    "            tuplas_ordenadas = pickle.load(f)\n",
    "        tuplas_ordenadas = sorted(list(tuplas_ordenadas.items()), key=lambda x: x[0])\n",
    "        # tuplas_ordenadas = [par for par in tuplas_ordenadas if not par[0].isdigit()]\n",
    "        tuplas_ordenadas = [(term[0], list(term[1].items())) for term in tuplas_ordenadas]\n",
    "        with open(file_path, 'wb') as f:\n",
    "            pickle.dump(tuplas_ordenadas, f)\n",
    "\n",
    "def mergeSortAux(dir_bloques, l, r):\n",
    "    if l == r:\n",
    "        bloque = leer_bloque(dir_bloques, l)\n",
    "        unicos = set()    \n",
    "        for par in bloque:\n",
    "            unicos.add(par[0])\n",
    "        return list(unicos)\n",
    "    \n",
    "    if (l < r):\n",
    "        mid = int(math.ceil((r + l)/2.0))\n",
    "        unique_l = mergeSortAux(dir_bloques, l, mid - 1)\n",
    "        unique_r = mergeSortAux(dir_bloques, mid, r)\n",
    "        unicos = set()    \n",
    "        for term in unique_l:\n",
    "            unicos.add(term)\n",
    "        for term in unique_r:\n",
    "            unicos.add(term)\n",
    "        unicos = list(unicos)\n",
    "        merge_v2(dir_bloques, l, r, mid, len(unicos))\n",
    "        return list(unicos)\n",
    "        \n",
    "    return []\n",
    "\n",
    "def escribir_bloque(dir_bloques, block, idx_insert_block, buffer_limit = 2000):\n",
    "    with open(os.path.join(dir_bloques, \"block_{}_v2.pkl\".format(idx_insert_block)), 'wb') as f:\n",
    "        pickle.dump(block, f)    \n",
    "def leer_bloque(dir_bloques, it):\n",
    "    file_path = os.path.join(dir_bloques, f\"block_{it}.pkl\")\n",
    "    with open(file_path, \"rb\") as f:\n",
    "        buffer = pickle.load(f)\n",
    "    return buffer\n",
    "def merge_v2(dir_bloques, l, r, mid, num_terms):\n",
    "    idx_insert_block = l\n",
    "    new_block = []\n",
    "    mezclar_n_bloques = r - l + 1\n",
    "    unique_terms_per_block = int(math.ceil(num_terms/mezclar_n_bloques))\n",
    "    unique_terms_current_block = 0\n",
    "\n",
    "    it_l = l\n",
    "    it_r = mid\n",
    "    term_dic_l = leer_bloque(dir_bloques, it_l)\n",
    "    term_dic_r = leer_bloque(dir_bloques, it_r)\n",
    "    \n",
    "    idx_term_l = 0\n",
    "    idx_term_r = 0\n",
    "\n",
    "    idx_doc_l = 0\n",
    "    idx_doc_r = 0\n",
    "    new_block = []\n",
    "    while(it_l < mid and it_r < r + 1):\n",
    "        while(idx_term_l < len(term_dic_l) and idx_term_r < len(term_dic_r)): # moverme entre palabras de dos bloques\n",
    "            new_term = []\n",
    "            if(term_dic_l[idx_term_l][0] < term_dic_r[idx_term_r][0]):\n",
    "                new_term = term_dic_l[idx_term_l]\n",
    "                idx_term_l += 1\n",
    "            elif(term_dic_l[idx_term_l][0] > term_dic_r[idx_term_r][0]):\n",
    "                new_term = term_dic_r[idx_term_r]\n",
    "                idx_term_r += 1\n",
    "            else:\n",
    "                idx_doc_l = 0\n",
    "                idx_doc_r = 0\n",
    "                while(idx_doc_l < len(term_dic_l[idx_term_l][1]) and idx_doc_r < len(term_dic_r[idx_term_r][1])):\n",
    "                    # print(f\"Toma 2 terminos iguales con tf = {term_dic_l[idx_term_l][1]} y {term_dic_r[idx_term_r][1]}\")\n",
    "                    if term_dic_l[idx_term_l][1][idx_doc_l][0] > term_dic_r[idx_term_r][1][idx_doc_r][0]:\n",
    "                        pushear_doc = term_dic_r[idx_term_r][1][idx_doc_r]\n",
    "                        idx_doc_r += 1\n",
    "                    elif term_dic_l[idx_term_l][1][idx_doc_l][0] < term_dic_r[idx_term_r][1][idx_doc_r][0]:\n",
    "                        pushear_doc = term_dic_l[idx_term_l][1][idx_doc_l]\n",
    "                        idx_doc_l += 1\n",
    "                    else:\n",
    "                        pushear_doc = (term_dic_l[idx_term_l][1][idx_doc_l][0], term_dic_l[idx_term_l][1][idx_doc_l][1] + term_dic_r[idx_term_r][1][idx_doc_r][1])\n",
    "                        idx_doc_l += 1\n",
    "                        idx_doc_r += 1\n",
    "                    new_term.append(pushear_doc)\n",
    "                while(idx_doc_l < len(term_dic_l[idx_term_l][1])):\n",
    "                    pushear_doc = term_dic_l[idx_term_l][1][idx_doc_l]\n",
    "                    idx_doc_l += 1\n",
    "                    new_term.append(pushear_doc)\n",
    "                while(idx_doc_r < len(term_dic_r[idx_term_r][1])):\n",
    "                    pushear_doc = term_dic_r[idx_term_r][1][idx_doc_r]\n",
    "                    idx_doc_r += 1\n",
    "                    new_term.append(pushear_doc)\n",
    "                new_term = (term_dic_l[idx_term_l][0], new_term)\n",
    "                idx_term_r += 1\n",
    "                idx_term_l += 1\n",
    "            new_block.append(new_term)\n",
    "            \n",
    "            unique_terms_current_block += 1\n",
    "            if (unique_terms_current_block == unique_terms_per_block):\n",
    "                escribir_bloque(dir_bloques, new_block, idx_insert_block)\n",
    "                unique_terms_current_block = 0\n",
    "                idx_insert_block += 1\n",
    "                new_block = []\n",
    "        if(len(term_dic_l) == idx_term_l):\n",
    "            if (it_l < mid - 1):\n",
    "                it_l += 1\n",
    "                term_dic_l = leer_bloque(dir_bloques, it_l)\n",
    "                idx_term_l = 0\n",
    "                idx_doc_l = 0\n",
    "                continue\n",
    "            else:\n",
    "                break\n",
    "        if(len(term_dic_r) == idx_term_r):\n",
    "            if (it_r < r):\n",
    "                it_r += 1\n",
    "                term_dic_r = leer_bloque(dir_bloques, it_r)\n",
    "                idx_term_r = 0\n",
    "                idx_doc_r = 0\n",
    "                continue\n",
    "            else:\n",
    "                break\n",
    "        if(it_l == mid | it_r == r + 1):\n",
    "            break\n",
    "    while(it_l < mid):\n",
    "        term_dic_l = leer_bloque(dir_bloques, it_l)\n",
    "        while(idx_term_l < len(term_dic_l)):\n",
    "            new_block.append(term_dic_l[idx_term_l])\n",
    "            unique_terms_current_block += 1\n",
    "            if (unique_terms_current_block == unique_terms_per_block):\n",
    "                escribir_bloque(dir_bloques, new_block, idx_insert_block)\n",
    "                unique_terms_current_block = 0\n",
    "                idx_insert_block += 1\n",
    "                new_block = []\n",
    "            idx_term_l += 1\n",
    "        idx_term_l = 0\n",
    "        it_l += 1\n",
    "    while(it_r < r + 1):\n",
    "        term_dic_r = leer_bloque(dir_bloques, it_r)\n",
    "        while(idx_term_r < len(term_dic_r)):\n",
    "            new_block.append(term_dic_r[idx_term_r])\n",
    "            unique_terms_current_block += 1\n",
    "            if (unique_terms_current_block == unique_terms_per_block):\n",
    "                escribir_bloque(dir_bloques, new_block, idx_insert_block)\n",
    "                unique_terms_current_block = 0\n",
    "                idx_insert_block += 1\n",
    "                new_block = []\n",
    "            idx_term_r += 1\n",
    "        idx_term_r = 0\n",
    "        it_r += 1\n",
    "\n",
    "    while(idx_insert_block < r + 1):\n",
    "        if len(new_block) > 0:\n",
    "            escribir_bloque(dir_bloques, new_block, idx_insert_block)\n",
    "        else:\n",
    "            escribir_bloque(dir_bloques, [], idx_insert_block)\n",
    "        new_block = []\n",
    "        idx_insert_block += 1\n",
    "    idx_insert_block = l\n",
    "    for idx_archivo in range(l, r + 1):\n",
    "        nuevo_nombre = os.path.join(dir_bloques, \"block_{}.pkl\".format(idx_archivo))\n",
    "        if os.path.exists(nuevo_nombre):\n",
    "            os.remove(nuevo_nombre)\n",
    "        os.rename(os.path.join(dir_bloques, \"block_{}_v2.pkl\".format(idx_archivo)), nuevo_nombre)\n",
    "def mergeSort(dir_bloques):\n",
    "    bloques_files_dir = os.listdir(os.path.join('./',dir_bloques))\n",
    "    # print(bloques_files_dir)\n",
    "    n = len(bloques_files_dir)\n",
    "    # n = int(math.exp2(math.floor(math.log2(n)) + 1))\n",
    "    mergeSortAux(dir_bloques, 0, n - 1)\n",
    "'''\n",
    "Recibe la direccion del folder con los diccionarios del spimi,\n",
    "modifica los archivos y los sobreescribe para crear el índice\n",
    "invertido con un índice global\n",
    "'''\n",
    "def InvertirListasDiccionarios(dir_bloques):\n",
    "    n_bloques = len(os.listdir(os.path.join('./',dir_bloques)))\n",
    "    for i in range(n_bloques):\n",
    "        bloque_path = os.path.join(dir_bloques,\"block_{}.pkl\".format(i))\n",
    "        with open(bloque_path, 'rb') as f:\n",
    "            bloque = pickle.load(f)\n",
    "        bloque_dict = {}\n",
    "        if len(bloque) != 0:\n",
    "            for term, poosting_list in bloque:\n",
    "                poosting_dict = {}\n",
    "                for doc, tf in poosting_list:\n",
    "                    poosting_dict[doc] = tf\n",
    "                bloque_dict[term] = poosting_dict\n",
    "        with open(bloque_path, 'wb') as f:\n",
    "            pickle.dump(bloque_dict, f)\n",
    "\n",
    "def getNumberWithAtributo(dataset_head, atributo):\n",
    "    if atributo in dataset_head.columns:\n",
    "        return dataset_head.columns.get_loc(atributo)  # Obtiene la posición del atributo\n",
    "    return -1  # Retorna -1 si no existe\n",
    "\n",
    "def CrearIndiceGlobal(dir_bloques, dir_global_index):\n",
    "    path_global_index = os.path.join('./', dir_global_index)\n",
    "    if not (os.path.exists(path_global_index) and os.path.isdir(path_global_index)):\n",
    "        os.mkdir(path_global_index)\n",
    "    path_global_index_file = os.path.join(path_global_index, \"btree_index\")\n",
    "    with dbm.open(path_global_index_file, \"c\") as db:\n",
    "        path_bloques = os.listdir(os.path.join('./',dir_bloques))\n",
    "        for path in path_bloques:\n",
    "            path_bloque = os.path.join(dir_bloques, path)\n",
    "            with open(path_bloque, 'rb') as f:\n",
    "                bloque = pickle.load(f)\n",
    "            for term, poosting in bloque.items():\n",
    "                db[term] = path\n",
    "def PreComputarNormas(dir_bloques, dir_normas, n_docs, n_blocks, disk_buffer_size):\n",
    "    \n",
    "    norms = {}\n",
    "    norm_path = os.path.join('./', dir_normas)\n",
    "    if not (os.path.exists(norm_path) and os.path.isdir(norm_path)):\n",
    "        os.mkdir(norm_path)\n",
    "    \n",
    "    for i in range(n_blocks):\n",
    "        bloque_i_path = os.path.join(dir_bloques, \"block_{}.pkl\".format(i))\n",
    "        with open(bloque_i_path, 'rb') as f:\n",
    "            bloque_i = pickle.load(f)\n",
    "        for term, poosting in bloque_i.items():\n",
    "            df = len(poosting)\n",
    "            idf = math.log10(n_docs / df)\n",
    "            for doc, tf in poosting.items():\n",
    "                itf = math.log10(1 + tf)\n",
    "                if doc not in norms.keys():\n",
    "                    norms[doc] = 0\n",
    "                norms[doc] += (itf*idf)**2\n",
    "    for doc, peso in norms.items():\n",
    "        norms[doc] = math.sqrt(peso)\n",
    "    buffer = {}\n",
    "    index_buffer = 0\n",
    "    for doc, peso in sorted(norms.items(), key=lambda x: x[0]):\n",
    "        buffer[doc] = peso\n",
    "        if sys.getsizeof(buffer) >= disk_buffer_size:\n",
    "            with open(os.path.join(dir_normas,\"norma_block_{}.pkl\".format(index_buffer)), 'wb') as f:\n",
    "                pickle.dump(buffer, f)\n",
    "                buffer = {}\n",
    "                index_buffer += 1\n",
    "    if buffer:\n",
    "        with open(os.path.join(dir_normas,\"norma_block_{}.pkl\".format(index_buffer)), 'wb') as f:\n",
    "            pickle.dump(buffer, f)\n",
    "            buffer = {}\n",
    "            index_buffer += 1\n",
    "    return index_buffer\n",
    "def getResultados(result, path, disk_limit):\n",
    "    res = []\n",
    "    for chunk in pd.read_csv(path, chunksize=disk_limit):\n",
    "        for doc, score in result:\n",
    "            if doc in chunk.index:\n",
    "                res.append((chunk.iloc[doc], score))\n",
    "    return pd.DataFrame(res, columns=['doc_id', 'score'])    \n",
    "def get_dfText(path, col, row, disk_limit):\n",
    "    for chunk in pd.read_csv(path, chunksize=disk_limit):\n",
    "        if row in chunk.index:\n",
    "            return chunk.iloc[row,col]\n",
    "    return \"\"\n",
    "\n",
    "def get_dfTex_Cols(path, row, columnas): #We don´t use it is only ofr simple an very fast testing\n",
    "    dataset =  pd.read_csv(path)\n",
    "    res = [dataset.iloc[row, i] for i in columnas]\n",
    "    return ' '.join(res)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "import sys\n",
    "import math\n",
    "import heapq\n",
    "\n",
    "class SPIMI:\n",
    "    def __init__(self, index_dataset=\"path\", position = 3, col_text = \"lyrics\", columnas = [\"track_name\",\"track_artist\",\"lyrics\", \"track_album_name\"], index_dir=\"index_blocks\", idf_dir=\"normas\", global_index_dir =\"global_index\"):\n",
    "        self.index_dir = index_dir  \n",
    "        self.idf_dir = idf_dir\n",
    "        self.global_index_dir = global_index_dir\n",
    "        self.path_ = index_dataset \n",
    "        self.block_counter = 0      \n",
    "        self.disk_limit = 4000  \n",
    "        self.position = position\n",
    "        self.col_text = col_text\n",
    "        self.norm_bloques = 0\n",
    "        self.n_docs = 0\n",
    "        #Define columns to crear SPIMI\n",
    "        self.columnas = columnas \n",
    "\n",
    "        if not os.path.exists(self.index_dir):\n",
    "            os.makedirs(self.index_dir)\n",
    "    def cargar_indice(self, dir_indice_invertido):\n",
    "        with open(dir_indice_invertido, 'rb') as f:\n",
    "            dict_info = pickle.load(f)\n",
    "\n",
    "        self.index_dir = dict_info['index_dir']\n",
    "        self.idf_dir = dict_info['idf_dir']\n",
    "        self.global_index_dir = dict_info['global_index_dir']\n",
    "        self.path_ = dict_info['path_']\n",
    "        self.block_counter = dict_info['block_counter']\n",
    "        self.disk_limit = dict_info['disk_limit']\n",
    "        self.position = dict_info['position']\n",
    "        self.col_text = dict_info['col_text']\n",
    "        self.norm_bloques = dict_info['norm_bloques']\n",
    "        self.n_docs = dict_info['n_docs']\n",
    "    def guardar_indice(self):\n",
    "        info_indice = {\n",
    "            \"index_dir\": self.index_dir,\n",
    "            \"idf_dir\": self.idf_dir,\n",
    "            \"global_index_dir\": self.global_index_dir,\n",
    "            \"path_\": self.path_,\n",
    "            \"block_counter\": self.block_counter,\n",
    "            \"disk_limit\": self.disk_limit,\n",
    "            \"position\": self.position,\n",
    "            \"col_text\": self.col_text,\n",
    "            \"norm_bloques\": self.norm_bloques,\n",
    "            \"n_docs\": self.n_docs\n",
    "        }\n",
    "        with open(\"indice_invertido.pkl\", 'wb') as f:\n",
    "            pickle.dump(info_indice, f)\n",
    "    def spimi_invert(self):\n",
    "        dictionary = {}\n",
    "        doc_ids = set()\n",
    "        stop = False\n",
    "\n",
    "        #Get columna indeces: \n",
    "        with open(self.path_, mode='r', encoding='utf-8') as file:\n",
    "            primera_linea = file.readline().strip()\n",
    "        #Get number columns\n",
    "        columnas = primera_linea.split(',')\n",
    "        columnas_numbers  = [i for i in range(len(columnas)) if columnas[i] in self.columnas]\n",
    "        for chunk in pd.read_csv(self.path_, chunksize=self.disk_limit):\n",
    "            for doc_id_, row in chunk.iterrows():\n",
    "                preFila = [row.iloc[i] for i in columnas_numbers]\n",
    "                texto = ' '.join(preFila)\n",
    "                words = preprocesamiento(texto)\n",
    "                for text in words:\n",
    "                    doc_id = doc_id_\n",
    "                    token = text\n",
    "                    doc_ids.add(doc_id)\n",
    "                    if token not in dictionary:\n",
    "                        dictionary[token] = {}  \n",
    "\n",
    "                    if doc_id not in dictionary[token]:\n",
    "                        dictionary[token][doc_id] = 1  \n",
    "                    else:\n",
    "                        dictionary[token][doc_id] += 1  \n",
    "\n",
    "                    dictionary_size = sys.getsizeof(dictionary)\n",
    "                    if dictionary_size >= self.disk_limit:\n",
    "                        self.write_block_to_disk(dictionary, level=0)\n",
    "                        dictionary.clear()\n",
    "        if dictionary:\n",
    "            self.write_block_to_disk(dictionary, level=0)\n",
    "        self.n_docs = len(doc_ids)\n",
    "        OrdenarPorBloques(dir_blocks=self.index_dir, n_blocks=self.block_counter)\n",
    "        mergeSort(self.index_dir)\n",
    "        InvertirListasDiccionarios(self.index_dir)\n",
    "        CrearIndiceGlobal(self.index_dir, self.global_index_dir)\n",
    "        self.norm_bloques = PreComputarNormas(self.index_dir, self.idf_dir, self.n_docs, self.block_counter, self.disk_limit)\n",
    "        self.guardar_indice()\n",
    "        \n",
    "    def load_index(self):\n",
    "        self.index = {}\n",
    "        files = sorted(os.listdir(self.index_dir))\n",
    "        for file in files:\n",
    "            if file.endswith(\".pkl\"):\n",
    "                block = self.load_block(os.path.join(self.index_dir, file))\n",
    "                for term, postings in block.items():\n",
    "                    if term not in self.index:\n",
    "                        self.index[term] = []\n",
    "                    for doc_id, tf in postings.items():\n",
    "                        \n",
    "                        self.index[term].append((doc_id, math.log10(1+ tf)))\n",
    " \n",
    "    def load_block(self, filepath):\n",
    "        \"\"\" Cargar un bloque del índice desde un archivo. \"\"\"\n",
    "        with open(filepath, \"rb\") as f:\n",
    "            return pickle.load(f)\n",
    "        \n",
    "\n",
    "\n",
    "    def write_block_to_disk(self, dictionary, level):\n",
    "        sorted_terms = dict(sorted(dictionary.items())) \n",
    "        # block_{it}\n",
    "        file_path = os.path.join(self.index_dir, f\"block_{self.block_counter}.pkl\")\n",
    "        with open(file_path, \"wb\") as f:\n",
    "            pickle.dump(sorted_terms, f)\n",
    "        \n",
    "        self.block_counter += 1\n",
    "\n",
    "    def load_block(self, filepath):\n",
    "        with open(filepath, \"rb\") as f:\n",
    "            return pickle.load(f)\n",
    "    def get_term(self, term, path_bloque):\n",
    "        complete_list = {}\n",
    "        with open(os.path.join(self.index_dir, path_bloque), 'rb') as f:\n",
    "            complete_list = pickle.load(f)\n",
    "        if term not in complete_list.keys():\n",
    "            print(\"Alerta \", term)\n",
    "        return complete_list[term]\n",
    "    \n",
    "    def retrieval(self, query, k):\n",
    "        N = self.n_docs  \n",
    "        scores = [0] * N  \n",
    "        tf_query = {}  \n",
    "        terms = preprocesamiento(query)  \n",
    "\n",
    "        for term in terms:\n",
    "            if term in tf_query:\n",
    "                tf_query[term] += 1\n",
    "            else:\n",
    "                tf_query[term] = 1\n",
    "\n",
    "        tfidf_query = {}\n",
    "        with dbm.open(os.path.join(self.global_index_dir, \"btree_index\"), \"c\") as db:\n",
    "            for term, tf in tf_query.items():\n",
    "                if term in db:\n",
    "                    poosting_list = self.get_term(term, db[term].decode(\"utf-8\"))\n",
    "                    df_term = len(poosting_list)\n",
    "                    tfidf_query[term] = math.log10(1 + tf) * math.log10(self.n_docs/df_term)\n",
    "        norm_query = math.sqrt(sum(w_tq**2 for w_tq in tfidf_query.values()))  # Normalización del query\n",
    "\n",
    "        # Aplicar similitud de coseno: Calculamos el puntaje para cada documento\n",
    "        with dbm.open(os.path.join(self.global_index_dir, \"btree_index\"), \"c\") as db:\n",
    "            for term, w_tq in tfidf_query.items():\n",
    "                if term in db:\n",
    "                    poosting_list = self.get_term(term, db[term].decode(\"utf-8\"))\n",
    "                    df_term = len(poosting_list)\n",
    "                    idf_term = math.log10(self.n_docs/df_term)\n",
    "                    for doc, tf_td in poosting_list.items():\n",
    "                        w_td = math.log10(1 + tf_td) * idf_term\n",
    "                        # print(\"self.index\", term, \"es\", self.index[term])\n",
    "                        scores[doc] += w_td * w_tq #Producto punto\n",
    "\n",
    "        # Normalizar las puntuaciones de los documentos\n",
    "        # idx_doc = 0\n",
    "        for i in range(self.norm_bloques):\n",
    "            with open(os.path.join(self.idf_dir, \"norma_block_{}.pkl\".format(i)), 'rb') as f:\n",
    "                normas = pickle.load(f)\n",
    "            for doc, peso in normas.items():\n",
    "                scores[doc] /= (peso * norm_query)\n",
    "            \n",
    "\n",
    "        # Ordenar las puntuaciones en orden descendente\n",
    "        result = sorted(enumerate(scores), key=lambda x: x[1], reverse=True)\n",
    "\n",
    "        # Devolver los k documentos más relevantes (top-k)\n",
    "        return result[:k]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --------Set Path----------\n",
    "import kagglehub\n",
    "path = kagglehub.dataset_download(\"imuhammad/audio-features-and-lyrics-of-spotify-songs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Path de songs:  C:\\Users\\Ce mar\\.cache\\kagglehub\\datasets\\imuhammad\\audio-features-and-lyrics-of-spotify-songs\\versions\\1\\spotify_songs.csv\n"
     ]
    }
   ],
   "source": [
    "# ----------Set dataset--------\n",
    "\n",
    "lista_ = os.listdir(path)\n",
    "songs = os.path.join(path, lista_[0])\n",
    "print(\"Path de songs: \",songs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Path de songs:  C:\\Users\\Ce mar\\.cache\\kagglehub\\datasets\\imuhammad\\audio-features-and-lyrics-of-spotify-songs\\versions\\1\\spotify_songs.csv\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>doc_id</th>\n",
       "      <th>score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>track_id                                      ...</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>track_id                                      ...</td>\n",
       "      <td>0.063394</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>track_id                                      ...</td>\n",
       "      <td>0.051552</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>track_id                                      ...</td>\n",
       "      <td>0.048244</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>track_id                                      ...</td>\n",
       "      <td>0.047911</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              doc_id     score\n",
       "0  track_id                                      ...  1.000000\n",
       "1  track_id                                      ...  0.063394\n",
       "2  track_id                                      ...  0.051552\n",
       "3  track_id                                      ...  0.048244\n",
       "4  track_id                                      ...  0.047911"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# --------Set Path----------\n",
    "import kagglehub\n",
    "path = kagglehub.dataset_download(\"imuhammad/audio-features-and-lyrics-of-spotify-songs\")\n",
    "\n",
    "# ----------Set dataset--------\n",
    "\n",
    "lista_ = os.listdir(path)\n",
    "songs = os.path.join(path, lista_[0])\n",
    "print(\"Path de songs: \",songs)\n",
    "# dataset = pd.read_csv(songs)\n",
    "# dataset = dataset.head(20)\n",
    "\n",
    "\n",
    "# ----------Set Atributos--------\n",
    "\n",
    "columna = \"lyrics\"\n",
    "\n",
    "# ---------Creation---------\n",
    "\n",
    "# s = SPIMI(songs)  #Put songs path for real\n",
    "s = SPIMI(\"spotify_20.csv\") #Put this for experimentation\n",
    "# s.spimi_invert()\n",
    "s.cargar_indice(dir_indice_invertido=\"indice_invertido.pkl\")\n",
    "# ---------End Creation---------\n",
    "\n",
    "\n",
    "#-----------Start of query------------\n",
    "\n",
    "query = get_dfText(songs, 3, 0, s.disk_limit)\n",
    "# query = get_dfTex_Cols(songs, 2, [1,2,3,6])\n",
    "top_k = 5\n",
    "\n",
    "result = s.retrieval(query, top_k)\n",
    "getResultados(result, s.path_, s.disk_limit) # La respuesta es un array pd de [fila del dataframe, score]\n",
    "#-----------End of query------------\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
