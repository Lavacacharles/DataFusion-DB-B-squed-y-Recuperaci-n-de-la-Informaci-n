{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Proyecto jiji"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: kagglehub in c:\\users\\ce mar\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (0.3.4)\n",
      "Requirement already satisfied: packaging in c:\\users\\ce mar\\appdata\\roaming\\python\\python310\\site-packages (from kagglehub) (23.2)\n",
      "Requirement already satisfied: requests in c:\\users\\ce mar\\appdata\\roaming\\python\\python310\\site-packages (from kagglehub) (2.28.1)\n",
      "Requirement already satisfied: tqdm in c:\\users\\ce mar\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from kagglehub) (4.66.1)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in c:\\users\\ce mar\\appdata\\roaming\\python\\python310\\site-packages (from requests->kagglehub) (2.1.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\ce mar\\appdata\\roaming\\python\\python310\\site-packages (from requests->kagglehub) (2022.12.7)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\ce mar\\appdata\\roaming\\python\\python310\\site-packages (from requests->kagglehub) (3.4)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\ce mar\\appdata\\roaming\\python\\python310\\site-packages (from requests->kagglehub) (1.26.13)\n",
      "Requirement already satisfied: colorama in c:\\users\\ce mar\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from tqdm->kagglehub) (0.4.6)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip available: 22.2.1 -> 24.3.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "!pip install kagglehub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Ce mar\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\scipy\\__init__.py:177: UserWarning: A NumPy version >=1.18.5 and <1.26.0 is required for this version of SciPy (detected version 1.26.2\n",
      "  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n",
      "[nltk_data] Downloading package punkt to C:\\Users\\Ce\n",
      "[nltk_data]     mar\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "import numpy as np\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "nltk.download('punkt')\n",
    "import regex as re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "stemmer = SnowballStemmer('english')\n",
    "import os\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Ce mar\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Path to dataset files: C:\\Users\\Ce mar\\.cache\\kagglehub\\datasets\\imuhammad\\audio-features-and-lyrics-of-spotify-songs\\versions\\1\n"
     ]
    }
   ],
   "source": [
    "import kagglehub\n",
    "path = kagglehub.dataset_download(\"imuhammad/audio-features-and-lyrics-of-spotify-songs\")\n",
    "print(\"Path to dataset files:\", path)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"stopwords-en.txt\", encoding=\"latin1\") as file:\n",
    "   stoplist = [line.rstrip().lower() for line in file]\n",
    "stoplist += ['?', '-', '.', ':', ',', '!', ';']\n",
    "\n",
    "def preprocesamiento(texto, stemming=True):\n",
    "  words = []\n",
    "  texto = str(texto)\n",
    "  texto = texto.lower()\n",
    "  texto = re.sub(r'[^a-zA-Z0-9_À-ÿ]', ' ', texto)\n",
    "  # tokenizar\n",
    "  words = nltk.word_tokenize(texto, language='spanish')\n",
    "  # filtrar stopwords\n",
    "  words = [word for word in words if word not in stoplist]\n",
    "  # reducir palabras (stemming)\n",
    "  if stemming:\n",
    "      words = [stemmer.stem(word) for word in words]\n",
    "  return words\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Ce mar\\.cache\\kagglehub\\datasets\\imuhammad\\audio-features-and-lyrics-of-spotify-songs\\versions\\1\\spotify_songs.csv\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>15</th>\n",
       "      <th>16</th>\n",
       "      <th>17</th>\n",
       "      <th>18</th>\n",
       "      <th>19</th>\n",
       "      <th>20</th>\n",
       "      <th>21</th>\n",
       "      <th>22</th>\n",
       "      <th>23</th>\n",
       "      <th>24</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>track_id</td>\n",
       "      <td>track_name</td>\n",
       "      <td>track_artist</td>\n",
       "      <td>lyrics</td>\n",
       "      <td>track_popularity</td>\n",
       "      <td>track_album_id</td>\n",
       "      <td>track_album_name</td>\n",
       "      <td>track_album_release_date</td>\n",
       "      <td>playlist_name</td>\n",
       "      <td>playlist_id</td>\n",
       "      <td>...</td>\n",
       "      <td>loudness</td>\n",
       "      <td>mode</td>\n",
       "      <td>speechiness</td>\n",
       "      <td>acousticness</td>\n",
       "      <td>instrumentalness</td>\n",
       "      <td>liveness</td>\n",
       "      <td>valence</td>\n",
       "      <td>tempo</td>\n",
       "      <td>duration_ms</td>\n",
       "      <td>language</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0017A6SJgTbfQVU2EtsPNo</td>\n",
       "      <td>Pangarap</td>\n",
       "      <td>Barbie's Cradle</td>\n",
       "      <td>Minsan pa Nang ako'y napalingon Hindi ko alam ...</td>\n",
       "      <td>41</td>\n",
       "      <td>1srJQ0njEQgd8w4XSqI4JQ</td>\n",
       "      <td>Trip</td>\n",
       "      <td>2001-01-01</td>\n",
       "      <td>Pinoy Classic Rock</td>\n",
       "      <td>37i9dQZF1DWYDQ8wBxd7xt</td>\n",
       "      <td>...</td>\n",
       "      <td>-10.068</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0236</td>\n",
       "      <td>0.279</td>\n",
       "      <td>0.0117</td>\n",
       "      <td>0.0887</td>\n",
       "      <td>0.5660000000000001</td>\n",
       "      <td>97.091</td>\n",
       "      <td>235440</td>\n",
       "      <td>tl</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>004s3t0ONYlzxII9PLgU6z</td>\n",
       "      <td>I Feel Alive</td>\n",
       "      <td>Steady Rollin</td>\n",
       "      <td>The trees, are singing in the wind The sky blu...</td>\n",
       "      <td>28</td>\n",
       "      <td>3z04Lb9Dsilqw68SHt6jLB</td>\n",
       "      <td>Love &amp; Loss</td>\n",
       "      <td>2017-11-21</td>\n",
       "      <td>Hard Rock Workout</td>\n",
       "      <td>3YouF0u7waJnolytf9JCXf</td>\n",
       "      <td>...</td>\n",
       "      <td>-4.739</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0442</td>\n",
       "      <td>0.0117</td>\n",
       "      <td>0.00994</td>\n",
       "      <td>0.34700000000000003</td>\n",
       "      <td>0.404</td>\n",
       "      <td>135.225</td>\n",
       "      <td>373512</td>\n",
       "      <td>en</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>00chLpzhgVjxs1zKC9UScL</td>\n",
       "      <td>Poison</td>\n",
       "      <td>Bell Biv DeVoe</td>\n",
       "      <td>NA Yeah, Spyderman and Freeze in full effect U...</td>\n",
       "      <td>0</td>\n",
       "      <td>6oZ6brjB8x3GoeSYdwJdPc</td>\n",
       "      <td>Gold</td>\n",
       "      <td>2005-01-01</td>\n",
       "      <td>Back in the day - R&amp;B, New Jack Swing, Swingbe...</td>\n",
       "      <td>3a9y4eeCJRmG9p4YKfqYIx</td>\n",
       "      <td>...</td>\n",
       "      <td>-7.504</td>\n",
       "      <td>0</td>\n",
       "      <td>0.21600000000000005</td>\n",
       "      <td>0.00432</td>\n",
       "      <td>0.007229999999999999</td>\n",
       "      <td>0.489</td>\n",
       "      <td>0.65</td>\n",
       "      <td>111.904</td>\n",
       "      <td>262467</td>\n",
       "      <td>en</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>00cqd6ZsSkLZqGMlQCR0Zo</td>\n",
       "      <td>Baby It's Cold Outside (feat. Christina Aguilera)</td>\n",
       "      <td>CeeLo Green</td>\n",
       "      <td>I really can't stay Baby it's cold outside I'v...</td>\n",
       "      <td>41</td>\n",
       "      <td>3ssspRe42CXkhPxdc12xcp</td>\n",
       "      <td>CeeLo's Magic Moment</td>\n",
       "      <td>2012-10-29</td>\n",
       "      <td>Christmas Soul</td>\n",
       "      <td>6FZYc2BvF7tColxO8PBShV</td>\n",
       "      <td>...</td>\n",
       "      <td>-5.819</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0341</td>\n",
       "      <td>0.6890000000000001</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0664</td>\n",
       "      <td>0.405</td>\n",
       "      <td>118.593</td>\n",
       "      <td>243067</td>\n",
       "      <td>en</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 25 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                       0                                                  1   \\\n",
       "0                track_id                                         track_name   \n",
       "1  0017A6SJgTbfQVU2EtsPNo                                           Pangarap   \n",
       "2  004s3t0ONYlzxII9PLgU6z                                       I Feel Alive   \n",
       "3  00chLpzhgVjxs1zKC9UScL                                             Poison   \n",
       "4  00cqd6ZsSkLZqGMlQCR0Zo  Baby It's Cold Outside (feat. Christina Aguilera)   \n",
       "\n",
       "                2                                                  3   \\\n",
       "0     track_artist                                             lyrics   \n",
       "1  Barbie's Cradle  Minsan pa Nang ako'y napalingon Hindi ko alam ...   \n",
       "2    Steady Rollin  The trees, are singing in the wind The sky blu...   \n",
       "3   Bell Biv DeVoe  NA Yeah, Spyderman and Freeze in full effect U...   \n",
       "4      CeeLo Green  I really can't stay Baby it's cold outside I'v...   \n",
       "\n",
       "                 4                       5                     6   \\\n",
       "0  track_popularity          track_album_id      track_album_name   \n",
       "1                41  1srJQ0njEQgd8w4XSqI4JQ                  Trip   \n",
       "2                28  3z04Lb9Dsilqw68SHt6jLB           Love & Loss   \n",
       "3                 0  6oZ6brjB8x3GoeSYdwJdPc                  Gold   \n",
       "4                41  3ssspRe42CXkhPxdc12xcp  CeeLo's Magic Moment   \n",
       "\n",
       "                         7   \\\n",
       "0  track_album_release_date   \n",
       "1                2001-01-01   \n",
       "2                2017-11-21   \n",
       "3                2005-01-01   \n",
       "4                2012-10-29   \n",
       "\n",
       "                                                  8                       9   \\\n",
       "0                                      playlist_name             playlist_id   \n",
       "1                                 Pinoy Classic Rock  37i9dQZF1DWYDQ8wBxd7xt   \n",
       "2                                  Hard Rock Workout  3YouF0u7waJnolytf9JCXf   \n",
       "3  Back in the day - R&B, New Jack Swing, Swingbe...  3a9y4eeCJRmG9p4YKfqYIx   \n",
       "4                                     Christmas Soul  6FZYc2BvF7tColxO8PBShV   \n",
       "\n",
       "   ...        15    16                   17                  18  \\\n",
       "0  ...  loudness  mode          speechiness        acousticness   \n",
       "1  ...   -10.068     1               0.0236               0.279   \n",
       "2  ...    -4.739     1               0.0442              0.0117   \n",
       "3  ...    -7.504     0  0.21600000000000005             0.00432   \n",
       "4  ...    -5.819     0               0.0341  0.6890000000000001   \n",
       "\n",
       "                     19                   20                  21       22  \\\n",
       "0      instrumentalness             liveness             valence    tempo   \n",
       "1                0.0117               0.0887  0.5660000000000001   97.091   \n",
       "2               0.00994  0.34700000000000003               0.404  135.225   \n",
       "3  0.007229999999999999                0.489                0.65  111.904   \n",
       "4                     0               0.0664               0.405  118.593   \n",
       "\n",
       "            23        24  \n",
       "0  duration_ms  language  \n",
       "1       235440        tl  \n",
       "2       373512        en  \n",
       "3       262467        en  \n",
       "4       243067        en  \n",
       "\n",
       "[5 rows x 25 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lista_ = os.listdir(path)\n",
    "songs = os.path.join(path, lista_[0])\n",
    "print(songs)\n",
    "dataset = pd.read_csv(songs, header=None)\n",
    "dataset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Index</th>\n",
       "      <th>ProcessedText</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>lyric</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>minsan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>nang</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>ako</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>napalingon</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3306</th>\n",
       "      <td>19</td>\n",
       "      <td>hear</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3307</th>\n",
       "      <td>19</td>\n",
       "      <td>secret</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3308</th>\n",
       "      <td>19</td>\n",
       "      <td>talk</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3309</th>\n",
       "      <td>19</td>\n",
       "      <td>talkin</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3310</th>\n",
       "      <td>19</td>\n",
       "      <td>talkin</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3311 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      Index ProcessedText\n",
       "0         0         lyric\n",
       "1         1        minsan\n",
       "2         1          nang\n",
       "3         1           ako\n",
       "4         1    napalingon\n",
       "...     ...           ...\n",
       "3306     19          hear\n",
       "3307     19        secret\n",
       "3308     19          talk\n",
       "3309     19        talkin\n",
       "3310     19        talkin\n",
       "\n",
       "[3311 rows x 2 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# dataset = dataset.head(l)\n",
    "dataset = dataset.head(20)\n",
    "#Procesar los documentos en pares\n",
    "position_text=3\n",
    "pairs = []\n",
    "\n",
    "for i, row in dataset.iterrows():\n",
    "    words = preprocesamiento(row.iloc[position_text])\n",
    "    for text in words:\n",
    "        pairs.append((i, text))\n",
    "\n",
    "pairs_df = pd.DataFrame(pairs, columns=['Index', 'ProcessedText'])\n",
    "pairs_df.head(len(pairs_df))\n",
    "\n",
    "\n",
    "# print(pairs_df[pairs_df[\"ProcessedText\"] == \"6\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nRecibe la direccion del folder con los diccionarios del spimi,\\nmodifica los archivos y los sobreescribe para crear el índice\\ninvertido con un índice global\\n'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pickle\n",
    "import math\n",
    "def Lista_To_Diccionario(dir_bloque):\n",
    "    bloque_dict = {}\n",
    "    try:\n",
    "        with open(dir_bloque, 'rb') as f:\n",
    "            bloque = pickle.load(f)\n",
    "        if len(bloque) != 0:\n",
    "            # print(bloque)\n",
    "            for term, posting in bloque:\n",
    "                term_dict = {}\n",
    "                for doc, tf in posting:\n",
    "                    term_dict[doc] = tf\n",
    "                bloque_dict[term] = term_dict\n",
    "    except EOFError:\n",
    "        print(f\"El archivo {dir_bloque} está vacío o incompleto.\")\n",
    "    return bloque_dict\n",
    "\n",
    "def OrdenarPorBloques(dir_blocks, n_blocks):\n",
    "    for i in range(n_blocks):\n",
    "        file_path = os.path.join(dir_blocks, 'block_{}.pkl'.format(i))\n",
    "        with open(file_path, 'rb') as f:\n",
    "            tuplas_ordenadas = pickle.load(f)\n",
    "        tuplas_ordenadas = sorted(list(tuplas_ordenadas.items()), key=lambda x: x[0])\n",
    "        tuplas_ordenadas = [par for par in tuplas_ordenadas if not par[0].isdigit()]\n",
    "        tuplas_ordenadas = [(term[0], list(term[1].items())) for term in tuplas_ordenadas]\n",
    "        with open(file_path, 'wb') as f:\n",
    "            pickle.dump(tuplas_ordenadas, f)\n",
    "\n",
    "def mergeSortAux(dir_bloques, l, r):\n",
    "    if l == r:\n",
    "        bloque = leer_bloque(dir_bloques, l)\n",
    "        unicos = set()    \n",
    "        for par in bloque:\n",
    "            unicos.add(par[0])\n",
    "        return list(unicos)\n",
    "    \n",
    "    if (l < r):\n",
    "        mid = int(math.ceil((r + l)/2.0))\n",
    "        unique_l = mergeSortAux(dir_bloques, l, mid - 1)\n",
    "        unique_r = mergeSortAux(dir_bloques, mid, r)\n",
    "        unicos = set()    \n",
    "        for term in unique_l:\n",
    "            unicos.add(term)\n",
    "        for term in unique_r:\n",
    "            unicos.add(term)\n",
    "        unicos = list(unicos)\n",
    "        merge_v2(dir_bloques, l, r, mid, len(unicos))\n",
    "        return list(unicos)\n",
    "        \n",
    "    return []\n",
    "\n",
    "def escribir_bloque(dir_bloques, block, idx_insert_block, buffer_limit = 2000):\n",
    "    with open(os.path.join(dir_bloques, \"block_{}_v2.pkl\".format(idx_insert_block)), 'wb') as f:\n",
    "        pickle.dump(block, f)    \n",
    "def leer_bloque(dir_bloques, it):\n",
    "    file_path = os.path.join(dir_bloques, f\"block_{it}.pkl\")\n",
    "    with open(file_path, \"rb\") as f:\n",
    "        buffer = pickle.load(f)\n",
    "    return buffer\n",
    "def merge_v2(dir_bloques, l, r, mid, num_terms):\n",
    "    idx_insert_block = l\n",
    "    new_block = []\n",
    "    mezclar_n_bloques = r - l + 1\n",
    "    unique_terms_per_block = int(math.ceil(num_terms/mezclar_n_bloques))\n",
    "    unique_terms_current_block = 0\n",
    "\n",
    "    it_l = l\n",
    "    it_r = mid\n",
    "    term_dic_l = leer_bloque(dir_bloques, it_l)\n",
    "    term_dic_r = leer_bloque(dir_bloques, it_r)\n",
    "    \n",
    "    idx_term_l = 0\n",
    "    idx_term_r = 0\n",
    "\n",
    "    idx_doc_l = 0\n",
    "    idx_doc_r = 0\n",
    "    new_block = []\n",
    "    while(it_l < mid and it_r < r + 1):\n",
    "        # print(f\"Toma 2 bloques {it_l} y {it_r} | idx_term_l: \", idx_term_l, \"| len(term_dic_l)\", len(term_dic_l), \"| idx_term_r: \", idx_term_r, \"| len(term_dic_r)\", len(term_dic_r))\n",
    "        while(idx_term_l < len(term_dic_l) and idx_term_r < len(term_dic_r)): # moverme entre palabras de dos bloques\n",
    "            # print(\"Current term_dic_l\", term_dic_l)\n",
    "            # print(\"Current term_dic_r\", term_dic_r)\n",
    "            # print(f\"Toma 2 terminos {term_dic_l[idx_term_l][0]} y {term_dic_r[idx_term_r][0]}\")\n",
    "            new_term = []\n",
    "            if(term_dic_l[idx_term_l][0] < term_dic_r[idx_term_r][0]):\n",
    "                new_term = term_dic_l[idx_term_l]\n",
    "                idx_term_l += 1\n",
    "            elif(term_dic_l[idx_term_l][0] > term_dic_r[idx_term_r][0]):\n",
    "                new_term = term_dic_r[idx_term_r]\n",
    "                idx_term_r += 1\n",
    "            else:\n",
    "                idx_doc_l = 0\n",
    "                idx_doc_r = 0\n",
    "                while(idx_doc_l < len(term_dic_l[idx_term_l][1]) and idx_doc_r < len(term_dic_r[idx_term_r][1])):\n",
    "                    # print(f\"Toma 2 terminos iguales con tf = {term_dic_l[idx_term_l][1]} y {term_dic_r[idx_term_r][1]}\")\n",
    "                    if term_dic_l[idx_term_l][1][idx_doc_l][0] > term_dic_r[idx_term_r][1][idx_doc_r][0]:\n",
    "                        pushear_doc = term_dic_r[idx_term_r][1][idx_doc_r]\n",
    "                        idx_doc_r += 1\n",
    "                    elif term_dic_l[idx_term_l][1][idx_doc_l][0] < term_dic_r[idx_term_r][1][idx_doc_r][0]:\n",
    "                        pushear_doc = term_dic_l[idx_term_l][1][idx_doc_l]\n",
    "                        idx_doc_l += 1\n",
    "                    else:\n",
    "                        pushear_doc = (term_dic_l[idx_term_l][1][idx_doc_l][0], term_dic_l[idx_term_l][1][idx_doc_l][1] + term_dic_r[idx_term_r][1][idx_doc_r][1])\n",
    "                        idx_doc_l += 1\n",
    "                        idx_doc_r += 1\n",
    "                    # print(\"pushear_doc: \", pushear_doc)\n",
    "                    new_term.append(pushear_doc)\n",
    "                while(idx_doc_l < len(term_dic_l[idx_term_l][1])):\n",
    "                    # print(f\"ya no hay documentos de derecha, rellena con izquierda {idx_doc_l}\")\n",
    "                    pushear_doc = term_dic_l[idx_term_l][1][idx_doc_l]\n",
    "                    idx_doc_l += 1\n",
    "                    new_term.append(pushear_doc)\n",
    "                while(idx_doc_r < len(term_dic_r[idx_term_r][1])):\n",
    "                    # print(f\"ya no hay documentos de izquierda, rellena con derecha {idx_doc_l}\")\n",
    "                    pushear_doc = term_dic_r[idx_term_r][1][idx_doc_r]\n",
    "                    idx_doc_r += 1\n",
    "                    new_term.append(pushear_doc)\n",
    "                new_term = (term_dic_l[idx_term_l][0], new_term)\n",
    "                idx_term_r += 1\n",
    "                idx_term_l += 1\n",
    "            new_block.append(new_term)\n",
    "            \n",
    "            unique_terms_current_block += 1\n",
    "            if (unique_terms_current_block == unique_terms_per_block):\n",
    "                escribir_bloque(dir_bloques, new_block, idx_insert_block)\n",
    "                unique_terms_current_block = 0\n",
    "                idx_insert_block += 1\n",
    "                new_block = []\n",
    "        if(len(term_dic_l) == idx_term_l):\n",
    "            if (it_l < mid - 1):\n",
    "                it_l += 1\n",
    "                term_dic_l = leer_bloque(dir_bloques, it_l)\n",
    "                idx_term_l = 0\n",
    "                idx_doc_l = 0\n",
    "                continue\n",
    "            else:\n",
    "                break\n",
    "        if(len(term_dic_r) == idx_term_r):\n",
    "            if (it_r < r):\n",
    "                it_r += 1\n",
    "                term_dic_r = leer_bloque(dir_bloques, it_r)\n",
    "                idx_term_r = 0\n",
    "                idx_doc_r = 0\n",
    "                continue\n",
    "            else:\n",
    "                break\n",
    "        if(it_l == mid | it_r == r + 1):\n",
    "            break\n",
    "    while(it_l < mid):\n",
    "        # print(f\"Se acabaron los bloques de derecha, llena solo izquierda {it_l}\")\n",
    "        term_dic_l = leer_bloque(dir_bloques, it_l)\n",
    "        while(idx_term_l < len(term_dic_l)):\n",
    "            new_block.append(term_dic_l[idx_term_l])\n",
    "            unique_terms_current_block += 1\n",
    "            if (unique_terms_current_block == unique_terms_per_block):\n",
    "                escribir_bloque(dir_bloques, new_block, idx_insert_block)\n",
    "                unique_terms_current_block = 0\n",
    "                idx_insert_block += 1\n",
    "                new_block = []\n",
    "            idx_term_l += 1\n",
    "        idx_term_l = 0\n",
    "        it_l += 1\n",
    "    while(it_r < r + 1):\n",
    "        # print(f\"Se acabaron los bloques de izquierda, llena solo derecha {it_r}\")\n",
    "        term_dic_r = leer_bloque(dir_bloques, it_r)\n",
    "        while(idx_term_r < len(term_dic_r)):\n",
    "            new_block.append(term_dic_r[idx_term_r])\n",
    "            unique_terms_current_block += 1\n",
    "            if (unique_terms_current_block == unique_terms_per_block):\n",
    "                escribir_bloque(dir_bloques, new_block, idx_insert_block)\n",
    "                unique_terms_current_block = 0\n",
    "                idx_insert_block += 1\n",
    "                new_block = []\n",
    "            idx_term_r += 1\n",
    "        idx_term_r = 0\n",
    "        it_r += 1\n",
    "\n",
    "    while(idx_insert_block < r + 1):\n",
    "        if len(new_block) > 0:\n",
    "            escribir_bloque(dir_bloques, new_block, idx_insert_block)\n",
    "        else:\n",
    "            escribir_bloque(dir_bloques, [], idx_insert_block)\n",
    "        new_block = []\n",
    "        idx_insert_block += 1\n",
    "    idx_insert_block = l\n",
    "    for idx_archivo in range(l, r + 1):\n",
    "        nuevo_nombre = os.path.join(dir_bloques, \"block_{}.pkl\".format(idx_archivo))\n",
    "        if os.path.exists(nuevo_nombre):\n",
    "            os.remove(nuevo_nombre)\n",
    "        os.rename(os.path.join(dir_bloques, \"block_{}_v2.pkl\".format(idx_archivo)), nuevo_nombre)\n",
    "def mergeSort(dir_bloques):\n",
    "    bloques_files_dir = os.listdir(os.path.join('./',dir_bloques))\n",
    "    n = len(bloques_files_dir)\n",
    "    mergeSortAux(dir_bloques, 0, n - 1)\n",
    "    \n",
    "def InvertirListasDiccionarios(dir_bloques):\n",
    "    n_bloques = len(os.listdir(os.path.join('./',dir_bloques)))\n",
    "    # print(\"revisa: \", os.path.join('./',dir_bloques), \" con: \", n_blocks,\" bloques\")\n",
    "    for i in range(n_bloques):\n",
    "        bloque_path = os.path.join(dir_bloques,\"block_{}.pkl\".format(i))\n",
    "        # print(\"revisa: \", bloque_path)\n",
    "        with open(bloque_path, 'rb') as f:\n",
    "            bloque = pickle.load(f)\n",
    "        bloque_dict = {}\n",
    "\n",
    "        if len(bloque) != 0:\n",
    "            for term, poosting_list in bloque:\n",
    "                poosting_dict = {}\n",
    "                for doc, tf in poosting_list:\n",
    "                    poosting_dict[doc] = tf\n",
    "                bloque_dict[term] = poosting_dict\n",
    "        with open(bloque_path, 'wb') as f:\n",
    "            pickle.dump(bloque_dict, f)\n",
    "'''\n",
    "Recibe la direccion del folder con los diccionarios del spimi,\n",
    "modifica los archivos y los sobreescribe para crear el índice\n",
    "invertido con un índice global\n",
    "'''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "La cantidad de niveles serán 5\n",
      "Cantidad de bloques generados 23\n",
      "file:  block_0.pkl\n",
      "file:  block_1.pkl\n",
      "file:  block_10.pkl\n",
      "file:  block_11.pkl\n",
      "file:  block_12.pkl\n",
      "file:  block_13.pkl\n",
      "file:  block_14.pkl\n",
      "file:  block_15.pkl\n",
      "file:  block_16.pkl\n",
      "file:  block_17.pkl\n",
      "file:  block_18.pkl\n",
      "file:  block_19.pkl\n",
      "file:  block_2.pkl\n",
      "file:  block_20.pkl\n",
      "file:  block_21.pkl\n",
      "file:  block_22.pkl\n",
      "file:  block_3.pkl\n",
      "file:  block_4.pkl\n",
      "file:  block_5.pkl\n",
      "file:  block_6.pkl\n",
      "file:  block_7.pkl\n",
      "file:  block_8.pkl\n",
      "file:  block_9.pkl\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pickle\n",
    "import sys\n",
    "import math\n",
    "import heapq\n",
    "\n",
    "class SPIMI:\n",
    "    def __init__(self, index_dir=\"index_blocks\"):\n",
    "        self.index_dir = index_dir  \n",
    "        self.block_counter = 0      \n",
    "        self.doc_count = 0  \n",
    "        self.doc_ids = set()         \n",
    "        self.idf = {}\n",
    "        self.length = {}\n",
    "        self.disk_limit = 4000  \n",
    "        self.totalLevels = 0 \n",
    "        self.currentL = 0\n",
    "        self.cuntermerged = 0 \n",
    "\n",
    "        if not os.path.exists(self.index_dir):\n",
    "            os.makedirs(self.index_dir)\n",
    "\n",
    "    def spimi_invert(self, token_stream):\n",
    "        dictionary = {}\n",
    "        for _, row in token_stream.iterrows():\n",
    "            doc_id = row['Index']\n",
    "            token = row['ProcessedText']\n",
    "            self.doc_ids.add(doc_id)\n",
    "            if token not in dictionary:\n",
    "                dictionary[token] = {}  \n",
    "            \n",
    "            if doc_id not in dictionary[token]:\n",
    "                dictionary[token][doc_id] = 1  \n",
    "            else:\n",
    "                dictionary[token][doc_id] += 1  \n",
    "            dictionary_size = sys.getsizeof(dictionary)\n",
    "            if dictionary_size >= self.disk_limit:\n",
    "                self.write_block_to_disk(dictionary, level=0)\n",
    "                dictionary.clear()\n",
    "                \n",
    "        if dictionary:\n",
    "            self.write_block_to_disk(dictionary, level=0)\n",
    "\n",
    "        self.totalLevels = math.ceil(math.log2(self.block_counter))\n",
    "        print(\"La cantidad de niveles serán\", self.totalLevels)\n",
    "        print(\"Cantidad de bloques generados\", self.block_counter)\n",
    "\n",
    "        ##\n",
    "        self.load_index()\n",
    "        self.calculate_idf()\n",
    "        \n",
    "    def write_block_to_disk(self, dictionary, level):\n",
    "        sorted_terms = dict(sorted(dictionary.items())) \n",
    "        # block_{it}\n",
    "        file_path = os.path.join(self.index_dir, f\"block_{self.block_counter}.pkl\")\n",
    "        with open(file_path, \"wb\") as f:\n",
    "            pickle.dump(sorted_terms, f)\n",
    "        \n",
    "        self.block_counter += 1\n",
    "\n",
    "    def load_block(self, filepath):\n",
    "        with open(filepath, \"rb\") as f:\n",
    "            return pickle.load(f)\n",
    "\n",
    "    def save_merged_block(self, merged_data, level):\n",
    "        file_path = os.path.join(self.index_dir, f\"block_{self.block_counter}.pkl\")\n",
    "        with open(file_path, \"wb\") as f:\n",
    "            pickle.dump(merged_data, f)\n",
    "        self.cuntermerged += 1\n",
    "\n",
    "    def retrieval(self, query, k):\n",
    "        self.load_index()  # Cargar el índice\n",
    "        print(\"El index es:\", self.index)\n",
    "        N = len(self.doc_ids)  # Número total de documentos\n",
    "        print(\"Existen \", N, \" documentos\")\n",
    "        scores = [0] * N  # Puntuaciones para cada documento\n",
    "        terms = preprocesamiento(query)  # Función de preprocesamiento para tokenizar\n",
    "        print(\"query: \", query)\n",
    "        print(\"terms: \", terms)\n",
    "        # Calcular el TF-IDF del query\n",
    "        tf_query = {}  # Almacenaremos el TF de los términos de la consulta\n",
    "        for term in terms:\n",
    "            if term in tf_query.keys():\n",
    "                tf_query[term] += 1\n",
    "            else:\n",
    "                tf_query[term] = 1\n",
    "        print(\"tf_query es: \", tf_query)\n",
    "\n",
    "        tfidf_query = {}\n",
    "        for term, tf in tf_query.items():\n",
    "            tfidf_query\n",
    "            idf_term = 0\n",
    "            if term in self.idf:\n",
    "                idf_term = self.idf[term]\n",
    "            tfidf_query[term] = math.log10(1 + tf) * idf_term\n",
    "        print(\"tfidf_query es: \", tfidf_query)\n",
    "        norm_query = math.sqrt(sum(w_tq**2 for w_tq in tfidf_query.values()))  # Normalización del query\n",
    "\n",
    "        # Aplicar similitud de coseno: Calculamos el puntaje para cada documento\n",
    "        for term, w_tq in tfidf_query.items():\n",
    "            if term in self.index:\n",
    "                for doc, tf_td in self.index[term]:\n",
    "                    w_td = math.log10(1 + tf_td) * self.idf[term]\n",
    "                    # print(\"se añade el peso del doc: \", doc)\n",
    "                    scores[doc - 1] += w_td * w_tq\n",
    "\n",
    "        # Normalizar las puntuaciones de los documentos\n",
    "        dir_normas = \"normas\"\n",
    "        dir_normas = os.listdir(os.path.join('./',dir_normas))\n",
    "        idx_doc = 0\n",
    "        for i in range(len(dir_normas)):\n",
    "            with open(os.path.join(\"normas\", \"norma_block_{}.pkl\".format(i)), 'rb') as f:\n",
    "                normas = pickle.load(f)\n",
    "            for norma_doc in normas:\n",
    "                scores[idx_doc] /= (norma_doc * norm_query)\n",
    "                idx_doc += 1\n",
    "            \n",
    "\n",
    "        # for d in range(N):\n",
    "        #     if self.length.get(d + 1, 0) != 0:\n",
    "        #         scores[d] /= (self.length[d + 1] * norm_query)  # Normalización documento y consulta\n",
    "\n",
    "        # Ordenar las puntuaciones en orden descendente\n",
    "        result = sorted(enumerate(scores), key=lambda x: x[1], reverse=True)\n",
    "        print(\"result es\", result)\n",
    "\n",
    "        # Devolver los k documentos más relevantes (top-k)\n",
    "        return result[:k]\n",
    "    \n",
    "    def calculate_idf(self):\n",
    "        N = len(self.doc_ids)\n",
    "        self.doc_count = N\n",
    "        for term, postings in self.index.items():\n",
    "            # Número de documentos que contienen el término\n",
    "            doc_freq = len(postings)\n",
    "            # Calcular IDF y almacenar\n",
    "            self.idf[term] = math.log10(N / (1 + doc_freq))\n",
    "        # Calcular la longitud de cada documento\n",
    "        for term, postings in self.index.items():\n",
    "            for doc_id, tf in postings:\n",
    "                if doc_id not in self.length:\n",
    "                    self.length[doc_id] = 0\n",
    "                # Sumar los TF-IDF al cuadrado para la longitud del documento\n",
    "                self.length[doc_id] += (tf * self.idf[term]) ** 2\n",
    "    \n",
    "        # Tomar la raíz cuadrada para completar la longitud de cada documento\n",
    "        for doc_id in self.length.keys():\n",
    "            self.length[doc_id] = math.sqrt(self.length[doc_id])\n",
    "\n",
    "    def load_index(self):\n",
    "        self.index = {}\n",
    "        files = sorted(os.listdir(self.index_dir))\n",
    "        for file in files:\n",
    "            if file.endswith(\".pkl\"):\n",
    "                print(\"file: \",file)\n",
    "                block = self.load_block(os.path.join(self.index_dir, file))\n",
    "                # print(block)\n",
    "                # print(\"block es: \", block)\n",
    "                for term, postings in block.items():\n",
    "                    if term not in self.index.keys():\n",
    "                        self.index[term] = []\n",
    "                    for doc_id, tf in postings.items():\n",
    "                        self.index[term].append((doc_id, tf))\n",
    " \n",
    "    def load_block(self, filepath):\n",
    "        \"\"\" Cargar un bloque del índice desde un archivo. \"\"\"\n",
    "        with open(filepath, \"rb\") as f:\n",
    "            return pickle.load(f)\n",
    "def PreComputarNormas(dir_blocks, n_docs):\n",
    "    n_blocks = len(os.listdir(os.path.join('./',dir_blocks)))\n",
    "    norms = {}\n",
    "    for i in range(n_blocks):\n",
    "        bloque_i_path = os.path.join(dir_blocks, \"block_{}.pkl\".format(i))\n",
    "        with open(bloque_i_path, 'rb') as f:\n",
    "            bloque_i = pickle.load(f)\n",
    "        for term, poosting in bloque_i.items():\n",
    "            for doc, tf in poosting.items():\n",
    "                df = len(poosting)\n",
    "                idf = math.log10(n_docs / (1 + df))\n",
    "                itf = math.log10(1 + tf)\n",
    "                if doc not in norms.keys():\n",
    "                    norms[doc] = 0\n",
    "                norms[doc] += (itf*idf)**2\n",
    "    for doc, peso in norms.items():\n",
    "        norms[doc] = math.sqrt(peso)\n",
    "    n_norm_blocks = int(math.ceil(math.log2(n_docs)))\n",
    "    norm_per_blocks = int(math.ceil(n_docs/n_norm_blocks))\n",
    "    norm_path = \"normas\"\n",
    "    norm_path = os.path.join('./',norm_path)\n",
    "    if not (os.path.exists(norm_path) and os.path.isdir(norm_path)):\n",
    "        os.mkdir(norm_path)\n",
    "    norm_block = []\n",
    "    i_block = 0\n",
    "    for i in range(0, n_docs ):\n",
    "        norm_block.append(norms[i])\n",
    "        if len(norm_block) == norm_per_blocks:\n",
    "            with open(os.path.join(norm_path,\"norma_block_{}.pkl\".format(i_block)), 'wb') as f:\n",
    "                pickle.dump(norm_block,f)\n",
    "            i_block += 1\n",
    "            norm_block = []\n",
    "    \n",
    "s = SPIMI()\n",
    "s.spimi_invert(pairs_df)\n",
    "n_docs = s.doc_count\n",
    "n_blocks = s.block_counter\n",
    "dir_blocks = 'index_blocks'\n",
    "OrdenarPorBloques(dir_blocks, n_blocks)\n",
    "mergeSort(dir_blocks)\n",
    "InvertirListasDiccionarios(\"index_blocks\")\n",
    "n_docs = len(dataset) - 1\n",
    "PreComputarNormas(\"index_blocks\", n_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.2943241326606739, 3.3963737491859742, 1.6833283097129683, 3.4711158300986544, 2.766177447946184, 3.0007900019072165, 2.436834682972394, 3.2842430789424792, 1.7609279886368192, 2.2228856908327277, 5.6954408176804, 4.683051024574584, 1.9841756943328097, 6.525679211955665, 4.004854736996582, 6.0668870944962485]\n"
     ]
    }
   ],
   "source": [
    "n_norm_blocks = len(os.listdir(os.path.join('./', 'normas')))\n",
    "norms = []\n",
    "for i in range(n_norm_blocks):\n",
    "    norm_path = os.path.join('normas', \"norma_block_{}.pkl\".format(i))\n",
    "    with open(norm_path, 'rb') as f:\n",
    "        norms.extend(pickle.load(f))\n",
    "print(norms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "file:  block_0.pkl\n",
      "file:  block_1.pkl\n",
      "file:  block_10.pkl\n",
      "file:  block_11.pkl\n",
      "file:  block_12.pkl\n",
      "file:  block_13.pkl\n",
      "file:  block_14.pkl\n",
      "file:  block_15.pkl\n",
      "file:  block_16.pkl\n",
      "file:  block_17.pkl\n",
      "file:  block_18.pkl\n",
      "file:  block_19.pkl\n",
      "file:  block_2.pkl\n",
      "file:  block_20.pkl\n",
      "file:  block_21.pkl\n",
      "file:  block_22.pkl\n",
      "file:  block_3.pkl\n",
      "file:  block_4.pkl\n",
      "file:  block_5.pkl\n",
      "file:  block_6.pkl\n",
      "file:  block_7.pkl\n",
      "file:  block_8.pkl\n",
      "file:  block_9.pkl\n",
      "El index es: {'aaa': [(3, 1)], 'abrazadito': [(13, 4)], 'accordar': [(10, 2)], 'acemo': [(11, 1)], 'actin': [(14, 1)], 'action': [(16, 1)], 'actitud': [(11, 1)], 'ador': [(7, 1)], 'ain': [(7, 7), (14, 5), (17, 2)], 'air': [(7, 1), (17, 1)], 'aisl': [(16, 1)], 'ake': [(1, 1)], 'akin': [(1, 4)], 'ako': [(1, 2)], 'ala': [(11, 3)], 'alam': [(1, 2)], 'aliv': [(2, 2)], 'alla': [(10, 1)], 'alright': [(15, 4)], 'altra': [(10, 1)], 'amanec': [(13, 4)], 'amanecimo': [(15, 1)], 'amarradito': [(13, 2)], 'america': [(10, 1)], 'amo': [(10, 1)], 'amplif': [(16, 1)], 'anch': [(10, 3)], 'ando': [(15, 2)], 'anello': [(10, 1)], 'ang': [(1, 7)], 'angel': [(2, 3), (15, 1)], 'answer': [(4, 1)], 'appeal': [(14, 1)], 'appear': [(7, 1)], 'arm': [(7, 1)], 'aserej': [(10, 1)], 'askin': [(18, 3)], 'así': [(13, 2)], 'atención': [(13, 2)], 'attract': [(16, 1)], 'aunt': [(4, 1)], 'awar': [(7, 1)], 'aww': [(14, 6)], 'ay': [(1, 5), (13, 4), (15, 4)], 'aye': [(5, 1)], 'azucar': [(17, 1)], 'babi': [(4, 8), (5, 7), (7, 2), (15, 6)], 'bacano': [(11, 1)], 'bacio': [(10, 2)], 'backstag': [(16, 1)], 'bad': [(4, 1), (5, 2), (7, 1)], 'baguett': [(7, 1)], 'bailar': [(13, 2), (15, 1)], 'baina': [(11, 1)], 'bajar': [(13, 2)], 'baje': [(11, 1)], 'bajo': [(13, 2)], 'balla': [(10, 2)], 'balliamo': [(10, 1)], 'bang': [(5, 7)], 'barch': [(10, 1)], 'basterebb': [(10, 1)], 'bawat': [(1, 1)], 'bañarno': [(13, 8)], 'beat': [(15, 1)], 'beauti': [(3, 1), (4, 2), (6, 1)], 'becki': [(15, 1)], 'bed': [(7, 1)], 'bein': [(14, 2)], 'believ': [(8, 1)], 'bell': [(3, 1)], 'bella': [(15, 10)], 'bello': [(10, 2)], 'belong': [(7, 1)], 'bench': [(2, 1)], 'bend': [(7, 1)], 'bene': [(10, 2)], 'berata': [(17, 1)], 'besito': [(15, 1)], 'bewar': [(3, 1)], 'bien': [(13, 6)], 'billi': [(10, 1)], 'bit': [(16, 2)], 'bitch': [(16, 1)], 'biv': [(3, 3)], 'black': [(14, 5), (15, 1)], 'blade': [(17, 1)], 'blast': [(14, 1)], 'blind': [(3, 1)], 'blizzard': [(4, 1)], 'blow': [(3, 1), (17, 2)], 'blue': [(2, 1)], 'bocado': [(13, 2)], 'bodi': [(5, 2), (15, 1), (16, 1)], 'book': [(12, 1)], 'boquita': [(13, 18)], 'bother': [(14, 1)], 'bound': [(4, 1)], 'bout': [(5, 4), (7, 1), (14, 1), (19, 1)], 'boy': [(3, 1), (5, 4), (7, 1), (15, 2)], 'brain': [(7, 1)], 'brave': [(6, 4)], 'keepin': [(3, 1)], 'kick': [(17, 1)], 'kid': [(6, 1), (14, 3)], 'kill': [(14, 3)], 'killin': [(18, 1)], 'king': [(9, 4)], 'kiss': [(2, 3), (3, 2)], 'knee': [(4, 1)], 'knock': [(6, 1)], 'know': [(2, 1)], 'ko': [(1, 3)], 'kong': [(1, 1)], 'kung': [(1, 1)], 'kénsel': [(15, 1)], 'labio': [(13, 2)], 'lace': [(7, 1)], 'ladi': [(16, 1)], 'laid': [(3, 1)], 'lamig': [(1, 4)], 'lang': [(1, 3)], 'langit': [(1, 1)], 'las': [(11, 2), (15, 5)], 'latina': [(15, 3)], 'laugh': [(2, 1)], 'lay': [(3, 1), (7, 1), (14, 1)], 'le': [(10, 4), (11, 8), (15, 1)], 'learn': [(6, 1), (14, 2)], 'leather': [(17, 3)], 'leav': [(7, 1)], 'left': [(19, 1)], 'lend': [(4, 1)], 'letal': [(13, 2)], 'lick': [(17, 1)], 'lie': [(4, 1), (7, 1), (17, 1), (19, 9)], 'life': [(4, 1), (6, 4), (8, 1), (14, 1), (18, 1)], 'lift': [(17, 10)], 'light': [(7, 1), (12, 1), (16, 1)], 'lil': [(17, 1)], 'lilingon': [(1, 1)], 'limeston': [(12, 2)], 'limit': [(8, 2)], 'lip': [(2, 1), (4, 2)], 'listen': [(4, 1), (7, 1), (16, 1)], 'litti': [(5, 21)], 'live': [(6, 3), (12, 1), (14, 1)], 'livin': [(14, 1)], 'llama': [(15, 4)], 'llamada': [(15, 2)], 'llamar': [(13, 2)], 'llegaba': [(15, 2)], 'llego': [(13, 8)], 'lleva': [(11, 4)], 'llevara': [(15, 2)], 'llevo': [(11, 1)], 'llevé': [(15, 4)], 'lo': [(11, 3), (13, 4), (15, 4)], 'loca': [(11, 5)], 'locaa': [(11, 1)], 'locat': [(16, 1)], 'loco': [(11, 1)], 'lone': [(2, 1)], 'lontana': [(10, 1)], 'lookin': [(3, 1), (14, 1)], 'los': [(11, 2), (15, 1)], 'loser': [(3, 1)], 'lot': [(16, 1)], 'loui': [(12, 1)], 'love': [(2, 1), (3, 4), (6, 1), (7, 10), (8, 15), (12, 1), (14, 2), (15, 1), (17, 2), (19, 8)], 'lover': [(8, 4), (9, 4)], 'low': [(3, 1)], 'loyal': [(7, 1)], 'lucio': [(10, 1)], 'lucki': [(4, 1)], 'lujuria': [(13, 2)], 'luna': [(15, 6)], 'lust': [(19, 1)], 'luz': [(11, 1)], 'lyric': [(0, 1)], 'líder': [(15, 3)], 'mai': [(10, 2)], 'maiden': [(4, 1)], 'makin': [(14, 2)], 'male': [(10, 1)], 'malibu': [(10, 1)], 'maluma': [(15, 6)], 'mama': [(14, 1), (17, 1)], 'mamacita': [(15, 4)], 'mami': [(11, 2), (15, 1)], 'manbo': [(11, 1)], 'mangarap': [(1, 1)], 'mano': [(10, 1)], 'mantien': [(13, 2)], 'manzana': [(13, 4)], 'maradona': [(10, 1)], 'marat': [(1, 1)], 'mare': [(10, 1)], 'marea': [(13, 8)], 'mari': [(15, 1)], 'maschi': [(10, 1)], 'matter': [(7, 1)], 'meant': [(8, 3)], 'meet': [(3, 1)], 'meglio': [(10, 2)], 'mellow': [(3, 1)], 'melt': [(7, 1)], 'menu': [(16, 1)], 'merchandis': [(16, 1)], 'met': [(7, 1)], 'metrica': [(10, 1)], 'mga': [(1, 5)], 'mi': [(10, 5), (11, 7), (13, 2), (15, 6)], 'mia': [(10, 2)], 'michael': [(3, 1)], 'michell': [(12, 1)], 'middl': [(14, 1)], 'midnight': [(2, 1)], 'miel': [(13, 2)], 'mike': [(3, 1)], 'miley': [(10, 1)], 'mill': [(10, 3)], 'mind': [(3, 4), (4, 2), (7, 1)], 'minsan': [(1, 1)], 'mio': [(10, 1)], 'mira': [(15, 1)], 'mis': [(13, 2)], 'misplac': [(14, 1)], 'mistak': [(6, 1)], 'mma': [(7, 1)], 'mmm': [(18, 2)], 'mobil': [(14, 1)], 'moda': [(10, 1)], 'modert': [(13, 8)], 'mold': [(7, 1)], 'moment': [(7, 1), (9, 2)], 'momento': [(10, 1)], 'mon': [(16, 3)], 'money': [(3, 1), (7, 1), (17, 6), (18, 2)], 'mong': [(1, 4)], 'monto': [(15, 1)], 'mood': [(17, 1)], 'morder': [(13, 4)], 'mordidita': [(13, 36)], 'morn': [(14, 1)], 'mother': [(4, 1), (14, 1)], 'mountain': [(12, 1)], 'mouth': [(14, 1)], 'movimiento': [(15, 1)], 'movin': [(3, 1)], 'muer': [(11, 2)], 'muli': [(1, 1)], 'mundo': [(15, 1)], 'musiica': [(11, 1)], 'má': [(15, 5)], 'más': [(15, 29)], 'nabitawan': [(1, 1)], 'nada': [(5, 1), (11, 4)], 'nadi': [(11, 1)], 'nah': [(18, 3)], 'nakapagbigay': [(1, 4)], 'name': [(12, 1)], 'nang': [(1, 1)], 'napalingon': [(1, 1)], 'natur': [(13, 2), (17, 3)], 'neglect': [(7, 1)], 'negro': [(14, 1)], 'nei': [(10, 1)], 'neighbor': [(4, 1)], 'nella': [(10, 1)], 'nere': [(10, 1)], 'netflix': [(15, 1)], 'ngiti': [(1, 4)], 'nice': [(4, 2)], 'nigga': [(14, 1)], 'night': [(3, 1), (7, 1), (9, 8), (12, 2), (16, 1)], 'ninguna': [(15, 1)], 'niña': [(13, 2)], 'noch': [(11, 2), (13, 8)], 'nostr': [(10, 1)], 'nott': [(10, 1)], 'notti': [(10, 2)], 'nulla': [(10, 1)], 'nunca': [(11, 4)], 'occhi': [(10, 3)], 'odd': [(6, 1)], 'ogni': [(10, 1)], 'olvid': [(15, 1)], 'ombr': [(10, 1)], 'ond': [(10, 1)], 'ooh': [(6, 1), (15, 1), (17, 4), (18, 16)], 'oowi': [(17, 1)], 'oper': [(14, 1)], 'ore': [(10, 3)], 'orillita': [(13, 8)], 'orizzont': [(10, 1)], 'orlean': [(16, 1)], 'outrag': [(16, 1)], 'outta': [(16, 1)], 'pace': [(4, 1)], 'pack': [(14, 1)], 'paid': [(3, 1), (14, 1)], 'pain': [(7, 1)], 'pal': [(11, 1)], 'pangarap': [(1, 6)], 'pansin': [(1, 2)], 'para': [(13, 6)], 'parcero': [(15, 1)], 'pare': [(11, 2)], 'parec': [(13, 2)], 'park': [(2, 1), (3, 1)], 'parol': [(10, 2)], 'part': [(10, 1)], 'parti': [(5, 5), (10, 3), (15, 1)], 'pass': [(16, 1)], 'passin': [(18, 3)], 'pastel': [(13, 2)], 'payback': [(14, 1)], 'peac': [(14, 1)], 'pege': [(11, 2)], 'pelé': [(10, 1)], 'penitentiari': [(14, 1)], 'pensar': [(13, 2)], 'penso': [(10, 1)], 'pentot': [(10, 1)], 'peopl': [(14, 2)], 'pequemo': [(13, 4)], 'perché': [(10, 3)], 'perdi': [(10, 3)], 'perdiamo': [(10, 1)], 'perfect': [(6, 1), (7, 1)], 'perform': [(16, 1)], 'persist': [(7, 1)], 'phone': [(14, 1)], 'pica': [(13, 12)], 'piec': [(16, 1)], 'piel': [(13, 2)], 'pijama': [(15, 1)], 'pill': [(16, 1)], 'pimp': [(14, 1)], 'più': [(10, 6)], 'plane': [(17, 1)], 'plant': [(7, 1)], 'play': [(3, 1), (7, 1), (9, 4), (12, 1), (14, 1)], 'playa': [(17, 1)], 'pleasur': [(16, 1)], 'plenti': [(4, 1)], 'pneumonia': [(4, 1)], 'poetica': [(10, 1)], 'poi': [(10, 5)], 'poison': [(3, 25)], 'polemica': [(10, 1)], 'polic': [(14, 1)], 'polit': [(17, 1)], 'pone': [(13, 6)], 'poniendo': [(15, 1)], 'pool': [(17, 1)], 'poor': [(14, 1)], 'por': [(11, 1), (15, 1)], 'porqu': [(11, 2), (15, 4)], 'portion': [(3, 1)], 'posicion': [(11, 2)], 'potrei': [(10, 1)], 'pour': [(4, 1)], 'poverti': [(14, 1)], 'power': [(3, 1)], 'pray': [(7, 1)], 'precious': [(8, 1)], 'prefiero': [(15, 1)], 'presid': [(14, 1)], 'presión': [(13, 2)], 'pretti': [(15, 1), (16, 1)], 'pride': [(4, 1)], 'pro': [(3, 1)], 'probada': [(15, 3)], 'probart': [(15, 3)], 'problema': [(15, 1)], 'promis': [(7, 2), (9, 4)], 'propuesta': [(15, 1)], 'protect': [(7, 1)], 'prove': [(7, 1)], 'provo': [(10, 7)], 'pull': [(14, 1), (16, 1)], 'pum': [(11, 6)], 'pupila': [(13, 2)], 'purs': [(14, 1)], 'puso': [(13, 4)], 'quando': [(10, 7)], 'quedó': [(15, 1)], 'queen': [(7, 1), (9, 4)], 'quell': [(10, 1)], 'quelli': [(10, 1)], 'quello': [(10, 7)], 'querían': [(15, 1)], 'questa': [(10, 1)], 'questo': [(10, 2)], 'qui': [(10, 6)], 'quick': [(17, 1)], 'quier': [(11, 7), (15, 12)], 'quiero': [(13, 2)], 'qué': [(15, 10)], 'race': [(14, 1)], 'racist': [(14, 1)], 'rais': [(14, 1)], 'ralph': [(3, 1)], 'rare': [(7, 1)], 'rat': [(14, 1)], 'ratata': [(11, 5)], 'raven': [(17, 1)], 'rawhid': [(17, 3)], 'razzamatazz': [(16, 1)], 'readi': [(3, 4), (14, 1), (16, 2)], 'real': [(7, 1), (13, 2), (14, 2), (17, 2)], 'realiz': [(19, 1)], 'reason': [(6, 1), (7, 1)], 'recompensaba': [(15, 2)], 'record': [(4, 1)], 'relationship': [(3, 1)], 'repitamo': [(15, 1)], 'requisito': [(15, 1)], 'resist': [(16, 1)], 'rest': [(3, 1)], 'resta': [(10, 2)], 'reykon': [(15, 7)], 'rico': [(13, 2), (15, 1)], 'ride': [(7, 1), (17, 3)], 'ridendo': [(10, 1)], 'rigbi': [(8, 1)], 'rimanendo': [(10, 2)], 'riot': [(16, 1)], 'risvegliano': [(10, 1)], 'ritorni': [(10, 2)], 'ritorno': [(10, 1)], 'roar': [(4, 1)], 'rock': [(16, 2)], 'rockin': [(17, 1)], 'roll': [(16, 1)], 'romeo': [(15, 1)], 'ron': [(3, 1)], 'rough': [(14, 1)], 'royal': [(7, 1)], 'rule': [(7, 1)], 'rumour': [(6, 1)], 'run': [(3, 1), (12, 3)], 'runnin': [(3, 1)], 'rush': [(14, 1)], 'sabe': [(11, 1), (15, 2)], 'sack': [(17, 1)], 'sad': [(7, 1)], 'sale': [(10, 6)], 'salir': [(15, 5)], 'salvaj': [(13, 2)], 'sana': [(1, 1)], 'sandali': [(1, 2)], 'sangu': [(10, 1)], 'satisfi': [(7, 2)], 'sayin': [(3, 2)], 'sayo': [(1, 1)], 'scare': [(6, 1), (7, 1)], 'scene': [(16, 1)], 'schemin': [(3, 2)], 'scorrer': [(10, 1)], 'screamin': [(3, 1)], 'screen': [(17, 3)], 'scritt': [(10, 2)], 'scurri': [(4, 1)], 'scusi': [(10, 2)], 'sea': [(11, 2), (12, 1)], 'secondo': [(10, 2)], 'secret': [(14, 1), (19, 12)], 'seed': [(7, 1)], 'seguo': [(10, 1)], 'sei': [(10, 5)], 'sell': [(16, 1)], 'sellin': [(14, 1)], 'sellout': [(16, 1)], 'semana': [(13, 2), (15, 1)], 'sembra': [(10, 1)], 'sempr': [(10, 1)], 'sens': [(3, 1), (4, 1)], 'sensazion': [(10, 1)], 'sentido': [(11, 4)], 'sentimento': [(10, 2)], 'sentir': [(13, 2)], 'sepa': [(15, 1)], 'sere': [(10, 1)], 'seren': [(10, 1)], 'servono': [(10, 1)], 'set': [(12, 1), (17, 1)], 'sexi': [(17, 1)], 'sha': [(13, 2)], 'shake': [(3, 1)], 'shape': [(18, 1)], 'share': [(7, 1), (14, 1)], 'shift': [(17, 4)], 'shine': [(7, 1)], 'ship': [(14, 1)], 'shit': [(7, 1), (16, 1)], 'shore': [(4, 1)], 'shori': [(11, 1)], 'shot': [(14, 1)], 'shoulder': [(6, 3)], 'sicuro': [(10, 1)], 'sient': [(15, 1)], 'siento': [(11, 4)], 'siero': [(10, 1)], 'sight': [(16, 1)], 'sign': [(2, 1)], 'simpli': [(4, 1), (7, 1)], 'sin': [(13, 2), (15, 3)], 'sing': [(2, 1), (7, 1), (16, 1)], 'singl': [(6, 2)], 'sir': [(4, 1)], 'sister': [(4, 1), (7, 1), (14, 1)], 'sit': [(7, 1), (12, 1)], 'situat': [(3, 1)], 'skill': [(14, 1)], 'skull': [(12, 1)], 'sky': [(2, 1), (8, 1)], 'sleazi': [(14, 1)], 'sleep': [(19, 6)], 'slick': [(3, 2)], 'slow': [(3, 1)], 'smack': [(14, 1)], 'smile': [(2, 3), (3, 2)], 'smokin': [(14, 1)], 'snatch': [(14, 1)], 'sofa': [(7, 1)], 'soft': [(2, 2)], 'sogni': [(10, 1)], 'soil': [(7, 1)], 'sola': [(11, 5)], 'soldier': [(6, 7), (7, 1)], 'sole': [(10, 3)], 'solo': [(10, 3)], 'son': [(11, 2), (16, 1)], 'song': [(7, 1)], 'sono': [(10, 1)], 'sonó': [(13, 2)], 'sorrow': [(4, 1)], 'soul': [(7, 1)], 'sound': [(14, 1), (16, 1)], 'soy': [(11, 1), (15, 3)], 'spark': [(12, 1)], 'special': [(7, 1)], 'spell': [(4, 1)], 'spend': [(7, 1), (8, 1)], 'spiagg': [(10, 1)], 'spirit': [(7, 1)], 'spite': [(8, 1)], 'spog': [(10, 1)], 'spyderman': [(3, 1)], 'stack': [(18, 1)], 'star': [(9, 2), (17, 4)], 'starlight': [(4, 1)], 'start': [(3, 2), (4, 1), (8, 2), (14, 2)], 'stato': [(10, 1)], 'stay': [(4, 3), (7, 1), (9, 4), (14, 2)], 'stayin': [(14, 1)], 'steal': [(3, 1)], 'step': [(14, 1), (18, 4)], 'stiamo': [(10, 1)], 'stickel': [(16, 1)], 'sticki': [(17, 1)], 'stimul': [(7, 1)], 'stomach': [(14, 1)], 'stood': [(3, 1)], 'storm': [(4, 1)], 'straight': [(7, 1)], 'strang': [(3, 1)], 'stranger': [(14, 1)], 'strap': [(14, 1)], 'street': [(14, 1)], 'strengthen': [(7, 1)], 'strong': [(7, 1)], 'stumbl': [(6, 1)], 'stupid': [(5, 6)], 'style': [(16, 1), (18, 3)], 'sudar': [(13, 2)], 'sudor': [(13, 2)], 'suga': [(17, 32)], 'sugar': [(17, 5)], 'sul': [(10, 1)], 'sun': [(2, 1)], 'superfli': [(17, 1)], 'suppos': [(7, 1), (14, 1)], 'sur': [(11, 1)], 'surviv': [(14, 1)], 'suspici': [(4, 1)], 'swe': [(16, 1)], 'swear': [(6, 1)], 'sweet': [(17, 4)], 'swell': [(4, 1)], 'sé': [(15, 1)], 'sólo': [(15, 6)], 'ta': [(11, 1), (14, 7)], 'take': [(14, 2)], 'takin': [(3, 1)], 'talk': [(4, 1), (19, 6)], 'talkin': [(19, 18)], 'tanong': [(1, 1)], 'tanqu': [(11, 1)], 'tat': [(14, 4)], 'te': [(10, 3), (11, 1), (13, 10), (15, 12)], 'tear': [(6, 1)], 'telegram': [(10, 1)], 'tellin': [(7, 1)], 'tempo': [(10, 1)], 'tender': [(8, 8)], 'tensión': [(13, 4)], 'tenso': [(15, 1)], 'tequila': [(13, 2)], 'break': [(3, 1), (4, 1), (8, 3)], 'breakfast': [(16, 1)], 'breakin': [(3, 1)], 'breath': [(6, 1), (7, 1)], 'brian': [(16, 1)], 'broken': [(8, 1)], 'brother': [(4, 1), (14, 7)], 'brown': [(3, 1)], 'brum': [(11, 3)], 'buck': [(14, 1)], 'buena': [(11, 1)], 'bugi': [(10, 1)], 'bumulong': [(1, 1)], 'busi': [(5, 1)], 'bust': [(14, 1)], 'butt': [(3, 2)], 'búscame': [(13, 2)], 'cab': [(4, 1)], 'caiga': [(15, 2)], 'calcio': [(10, 1)], 'cama': [(15, 2)], 'cambi': [(11, 2)], 'camera': [(18, 3)], 'campana': [(13, 2)], 'canzoni': [(10, 2)], 'car': [(7, 1)], 'care': [(2, 1), (14, 1)], 'caress': [(7, 1)], 'carri': [(6, 3)], 'carro': [(11, 1)], 'cart': [(16, 1)], 'catch': [(19, 2)], 'caught': [(4, 1)], 'caution': [(3, 1)], 'ceil': [(17, 4)], 'cervello': [(10, 1)], 'chabli': [(12, 1)], 'chang': [(14, 7)], 'chat': [(18, 1)], 'che': [(10, 10), (15, 1)], 'cheat': [(7, 1)], 'checkin': [(3, 1)], 'chest': [(3, 1), (16, 1)], 'chez': [(15, 5)], 'chicago': [(16, 1)], 'chick': [(17, 1)], 'child': [(7, 1)], 'chill': [(14, 1)], 'chissà': [(10, 2)], 'chit': [(18, 1)], 'chitarra': [(10, 2)], 'thang': [(17, 1)], 'think': [(12, 6)], 'thrill': [(4, 1), (16, 1)], 'throne': [(12, 1)], 'throw': [(18, 1)], 'ti': [(10, 11), (15, 2)], 'tien': [(11, 1), (13, 2), (15, 1)], 'tiffani': [(16, 1)], 'tight': [(17, 1)], 'tiguer': [(11, 2)], 'time': [(3, 1), (6, 1), (7, 3), (8, 2), (14, 4), (16, 1), (17, 1), (18, 2), (19, 4)], 'tingin': [(1, 1)], 'tinig': [(1, 4)], 'tire': [(12, 1), (14, 1)], 'toda': [(11, 2), (13, 8), (15, 1)], 'todito': [(13, 8)], 'todo': [(15, 3)], 'told': [(7, 1), (19, 2)], 'tom': [(15, 4)], 'tomorrow': [(4, 1)], 'tone': [(17, 1)], 'tongu': [(18, 2)], 'tonight': [(14, 1)], 'tool': [(14, 1)], 'totoo': [(1, 1)], 'touch': [(4, 1), (7, 1), (14, 1)], 'tour': [(16, 1)], 'toy': [(7, 1)], 'tra': [(10, 2)], 'track': [(18, 1)], 'traje': [(13, 2)], 'trama': [(15, 1)], 'treat': [(7, 1), (14, 1), (17, 1)], 'tree': [(2, 2)], 'tremenda': [(15, 4)], 'tri': [(7, 1)], 'tribù': [(10, 1)], 'trigger': [(14, 1)], 'tropic': [(4, 1)], 'troppo': [(10, 1)], 'trovarl': [(10, 1)], 'trovato': [(10, 1)], 'true': [(7, 2), (16, 1)], 'trust': [(3, 2), (14, 1)], 'tryna': [(7, 1)], 'tu': [(10, 2), (11, 5), (13, 22), (15, 5)], 'tum': [(11, 3)], 'tus': [(13, 4), (15, 1)], 'tutta': [(10, 1)], 'tutto': [(10, 1)], 'tutugon': [(1, 1)], 'tú': [(15, 12)], 'uh': [(3, 1), (5, 1), (15, 14), (18, 1)], 'ulit': [(1, 1)], 'ultim': [(17, 4)], 'una': [(10, 5), (11, 10), (13, 36), (15, 4)], 'understand': [(7, 1)], 'unit': [(9, 2)], 'uoh': [(11, 3)], 'vain': [(12, 1)], 'vaina': [(11, 9)], 'vale': [(10, 2)], 'vali': [(10, 1)], 'vamo': [(13, 10)], 'vampir': [(12, 1)], 'vampiro': [(13, 2)], 'vena': [(10, 1)], 'vendita': [(10, 1)], 'ver': [(13, 2)], 'verità': [(10, 1)], 'vestido': [(13, 2)], 'vibe': [(18, 1)], 'vicina': [(10, 1)], 'vicious': [(4, 1)], 'vida': [(11, 5)], 'viendo': [(15, 1)], 'viern': [(15, 1)], 'vinil': [(10, 1)], 'viril': [(10, 1)], 'voic': [(7, 1)], 'void': [(7, 1)], 'volevo': [(10, 6)], 'volt': [(10, 1)], 'volum': [(5, 2)], 'vos': [(15, 1)], 'voy': [(11, 2)], 'vuelto': [(15, 1)], 'wait': [(7, 6)], 'wake': [(5, 1), (14, 1)], 'wall': [(3, 1)], 'wan': [(6, 5), (7, 2), (16, 1), (18, 3)], 'war': [(14, 4)], 'warm': [(4, 1)], 'warn': [(3, 1)], 'wassup': [(3, 1)], 'wast': [(14, 1)], 'watch': [(14, 1), (17, 3)], 'wave': [(4, 1)], 'weak': [(7, 1)], 'wealth': [(7, 1)], 'wee': [(17, 4)], 'welfar': [(14, 1)], 'white': [(14, 1)], 'wild': [(5, 2)], 'wind': [(2, 1)], 'window': [(4, 1)], 'winner': [(3, 1)], 'wipe': [(19, 1)], 'wit': [(5, 1)], 'woah': [(16, 1), (18, 3)], 'woman': [(7, 2)], 'woo': [(6, 1)], 'workin': [(14, 1)], 'worri': [(4, 1), (5, 2), (7, 1), (14, 1), (17, 1), (19, 1)], 'wors': [(14, 1)], 'worth': [(6, 3), (14, 1)], 'worthwhil': [(7, 1)], 'wou': [(11, 3)], 'wow': [(15, 3)], 'wrap': [(7, 1)], 'write': [(12, 1)], 'wrong': [(3, 2), (7, 1), (17, 1)], 'wuh': [(15, 1)], 'wuoh': [(15, 3)], 'yah': [(11, 1)], 'yata': [(1, 1)], 'yeah': [(3, 3), (5, 2), (6, 7), (11, 1), (14, 6), (16, 1), (18, 32)], 'yo': [(3, 3), (11, 8), (13, 2), (15, 8), (17, 1)], 'york': [(16, 1)], 'zona': [(15, 2)], 'zone': [(17, 1)], 'è': [(10, 8)], 'chiusi': [(10, 2)], 'choco': [(11, 1)], 'chose': [(7, 1)], 'chulo': [(17, 1)], 'cielo': [(15, 2)], 'cigarett': [(4, 1)], 'cintura': [(15, 5)], 'circl': [(18, 1)], 'clockin': [(3, 1)], 'close': [(2, 1), (14, 1), (19, 2)], 'closer': [(4, 1), (7, 1), (9, 2)], 'cloud': [(9, 4)], 'coat': [(4, 1)], 'coca': [(10, 1)], 'coil': [(7, 1)], 'cola': [(10, 1)], 'cold': [(4, 6)], 'colder': [(9, 2)], 'comin': [(14, 1)], 'como': [(11, 5), (15, 2)], 'conceal': [(14, 1)], 'confond': [(10, 1)], 'confort': [(15, 2)], 'contest': [(15, 1)], 'contigo': [(11, 2), (15, 1)], 'conto': [(10, 3)], 'contro': [(10, 1)], 'convers': [(17, 1)], 'cool': [(14, 1), (17, 1)], 'cop': [(14, 2)], 'corner': [(7, 1)], 'cos': [(10, 5)], 'cosa': [(10, 1)], 'costum': [(16, 1)], 'count': [(9, 2)], 'courag': [(7, 1)], 'crack': [(14, 4)], 'crampin': [(18, 3)], 'crazi': [(16, 1)], 'credi': [(10, 1)], 'cree': [(15, 1)], 'crew': [(3, 2)], 'crime': [(14, 1)], 'crimin': [(5, 1)], 'cross': [(12, 1)], 'cruella': [(16, 1)], 'cruis': [(18, 1)], 'crujient': [(13, 2)], 'crumbl': [(6, 1)], 'cuando': [(11, 10), (13, 2)], 'cubana': [(17, 1)], 'cuerpo': [(13, 2)], 'cumpl': [(15, 1)], 'cura': [(13, 2), (15, 4)], 'cure': [(3, 1)], 'cut': [(3, 1)], 'da': [(10, 6)], 'dai': [(10, 2)], 'dal': [(10, 1)], 'dalla': [(10, 1)], 'dame': [(15, 1)], 'damn': [(14, 1), (18, 2)], 'danc': [(2, 1), (9, 4), (12, 1), (16, 1)], 'darat': [(1, 1)], 'dark': [(14, 1)], 'darkest': [(6, 3)], 'darl': [(6, 1)], 'dart': [(11, 2)], 'day': [(6, 7), (12, 4)], 'dead': [(3, 4), (14, 1)], 'deal': [(14, 1)], 'dealin': [(7, 1)], 'deceiv': [(7, 1)], 'decis': [(8, 1)], 'dedica': [(10, 1)], 'dedicar': [(10, 6)], 'deg': [(10, 1)], 'deja': [(13, 2)], 'dejast': [(15, 4)], 'del': [(10, 1), (11, 1)], 'delici': [(4, 2)], 'dell': [(10, 3)], 'della': [(10, 1)], 'dement': [(13, 4)], 'demon': [(3, 1)], 'depth': [(6, 1)], 'descontrola': [(11, 5)], 'desea': [(11, 2)], 'desir': [(19, 1)], 'despacito': [(13, 2)], 'despair': [(6, 1), (7, 1)], 'destila': [(13, 2)], 'devic': [(16, 1)], 'devil': [(14, 1)], 'devill': [(16, 1)], 'devo': [(3, 2)], 'di': [(10, 3)], 'diablo': [(11, 1)], 'diamond': [(7, 1)], 'dice': [(15, 3)], 'diciamo': [(10, 1)], 'die': [(4, 1), (12, 1)], 'dient': [(13, 2)], 'diferent': [(15, 1)], 'dig': [(12, 3), (16, 1), (17, 1)], 'dijist': [(15, 2)], 'dile': [(15, 2)], 'dimmi': [(10, 2)], 'dio': [(13, 8)], 'dirti': [(10, 1), (15, 1)], 'disapprov': [(6, 1)], 'disfruta': [(11, 2)], 'disgrac': [(14, 1)], 'dist': [(15, 2)], 'distanc': [(6, 1)], 'distant': [(14, 1)], 'dito': [(1, 1)], 'diversi': [(10, 1)], 'divid': [(19, 1)], 'divina': [(15, 1)], 'divorc': [(16, 1)], 'dolc': [(17, 1)], 'dond': [(11, 2)], 'donn': [(10, 1)], 'door': [(4, 1)], 'doowop': [(17, 1)], 'dope': [(14, 2)], 'doubt': [(7, 1)], 'dove': [(17, 1)], 'drama': [(5, 2)], 'dreamin': [(3, 1)], 'dress': [(18, 2)], 'drink': [(4, 2), (18, 3)], 'drippin': [(5, 1)], 'drive': [(3, 1), (7, 1)], 'drivin': [(3, 2)], 'drop': [(4, 2)], 'drug': [(14, 1)], 'drum': [(5, 7)], 'dude': [(17, 1)], 'dumb': [(5, 39)], 'dura': [(10, 1), (15, 12)], 'duro': [(11, 2), (15, 6)], 'dé': [(13, 8)], 'déjà': [(10, 1)], 'día': [(13, 8)], 'díselo': [(15, 1)], 'eas': [(7, 1)], 'easi': [(14, 1)], 'east': [(14, 1)], 'eat': [(14, 1)], 'eilish': [(10, 1)], 'el': [(11, 1), (13, 16), (15, 8)], 'eleanor': [(8, 1)], 'elektra': [(16, 1)], 'ella': [(11, 8), (15, 1)], 'em': [(3, 1), (14, 5), (19, 1)], 'emi': [(16, 1)], 'empezando': [(15, 1)], 'empti': [(7, 1)], 'en': [(11, 6), (13, 12), (15, 7)], 'enamorada': [(15, 1)], 'energi': [(17, 1)], 'enseñart': [(13, 2)], 'entendido': [(15, 1)], 'entero': [(15, 1)], 'entertain': [(16, 13)], 'entrambi': [(10, 1)], 'envida': [(15, 1)], 'era': [(10, 1)], 'eras': [(14, 1)], 'erba': [(10, 1)], 'ere': [(13, 2), (15, 3)], 'esci': [(10, 1)], 'escono': [(10, 1)], 'eso': [(11, 4)], 'esperaban': [(15, 1)], 'esperando': [(15, 2)], 'estamo': [(11, 5)], 'esto': [(15, 1)], 'estoy': [(13, 2)], 'estrella': [(15, 4)], 'está': [(13, 12), (15, 1)], 'eterno': [(10, 1)], 'eto': [(11, 4)], 'even': [(4, 1)], 'evil': [(14, 1)], 'expir': [(12, 1)], 'explico': [(15, 1)], 'ey': [(15, 4)], 'eye': [(3, 1), (4, 1), (7, 1), (19, 4)], 'fa': [(10, 1), (17, 2)], 'facil': [(10, 1)], 'fade': [(7, 1)], 'fai': [(10, 1)], 'fall': [(3, 1), (6, 1), (18, 1)], 'fare': [(10, 1)], 'farlo': [(10, 1)], 'fat': [(17, 1)], 'father': [(4, 1)], 'fatta': [(10, 1)], 'fault': [(8, 1)], 'fear': [(7, 1)], 'feel': [(2, 2), (5, 1), (6, 1), (7, 4), (8, 1), (17, 8)], 'feelin': [(15, 3), (18, 1)], 'fella': [(3, 2)], 'fellow': [(3, 1)], 'femminil': [(10, 1)], 'fenc': [(2, 1)], 'ferrari': [(15, 1)], 'fiesta': [(13, 8)], 'fight': [(6, 3), (14, 1)], 'fill': [(14, 1)], 'fin': [(13, 2)], 'fina': [(15, 6)], 'finché': [(10, 2)], 'finest': [(18, 2)], 'fino': [(10, 3)], 'fireplac': [(4, 1)], 'flama': [(15, 4)], 'flame': [(12, 1)], 'flashin': [(18, 3)], 'fli': [(3, 2), (17, 28)], 'fling': [(17, 3)], 'floor': [(4, 1)], 'flow': [(13, 2)], 'follow': [(5, 1)], 'fool': [(14, 1), (17, 1)], 'forc': [(16, 1)], 'forget': [(3, 1)], 'frame': [(7, 1)], 'freez': [(3, 1), (4, 1)], 'frenar': [(13, 2)], 'freno': [(13, 2)], 'friend': [(12, 1)], 'frighten': [(6, 1)], 'fro': [(3, 1)], 'fuck': [(18, 7)], 'fue': [(13, 4), (15, 3)], 'fuego': [(11, 2), (13, 2)], 'fumando': [(15, 1)], 'funni': [(16, 1)], 'futuroo': [(11, 1)], 'gabbana': [(17, 1)], 'game': [(7, 1), (14, 1)], 'gawin': [(1, 1)], 'gent': [(10, 1), (13, 2)], 'gentlemen': [(16, 1)], 'gettin': [(3, 1)], 'gift': [(17, 4)], 'gioiello': [(10, 1)], 'girl': [(3, 7), (7, 4), (8, 1), (18, 1)], 'giuro': [(10, 1)], 'givin': [(14, 1)], 'gli': [(10, 2)], 'glisten': [(7, 1)], 'gloria': [(11, 4)], 'gon': [(4, 1), (6, 1), (7, 1), (9, 8), (12, 5), (16, 2), (18, 1)], 'goowi': [(17, 1)], 'gosh': [(4, 1)], 'grand': [(4, 1)], 'grave': [(12, 2)], 'gray': [(7, 1)], 'grip': [(5, 1)], 'grita': [(11, 1)], 'groovi': [(17, 1)], 'ground': [(16, 1)], 'groupi': [(16, 1)], 'grow': [(6, 1), (7, 2), (12, 1)], 'grown': [(6, 2), (17, 1)], 'guarda': [(10, 1)], 'gun': [(14, 1)], 'gurl': [(17, 3)], 'gusta': [(11, 2)], 'gustan': [(11, 2)], 'guy': [(19, 2)], 'ha': [(16, 4)], 'habitación': [(15, 2)], 'hacemo': [(11, 1)], 'haga': [(15, 1)], 'hago': [(11, 2)], 'hair': [(4, 1)], 'hand': [(2, 1), (4, 2)], 'hanggang': [(1, 1)], 'hangin': [(1, 1)], 'hard': [(3, 2), (6, 1)], 'harm': [(7, 1)], 'haré': [(15, 1)], 'hasta': [(13, 4), (15, 2)], 'hat': [(4, 1)], 'hate': [(14, 1)], 'hay': [(15, 3)], 'head': [(3, 2)], 'heal': [(14, 1)], 'hear': [(7, 1), (19, 12)], 'heart': [(3, 2), (7, 1), (8, 3), (9, 2), (12, 1), (19, 1)], 'heartfelt': [(7, 1)], 'heaven': [(14, 1)], 'heidi': [(10, 1)], 'hero': [(14, 1)], 'hey': [(2, 2), (4, 1), (14, 1), (16, 5)], 'hice': [(15, 2)], 'hicimo': [(15, 1)], 'hide': [(19, 1)], 'hideaway': [(9, 10)], 'high': [(3, 1)], 'highdrow': [(17, 1)], 'hindi': [(1, 2)], 'hit': [(17, 1)], 'ho': [(3, 1), (10, 2), (18, 6)], 'hoe': [(3, 1)], 'hold': [(4, 2), (6, 1), (7, 2), (8, 2), (9, 8), (14, 1), (18, 2)], 'homi': [(18, 6)], 'hoo': [(3, 1)], 'hope': [(4, 1)], 'hous': [(3, 1)], 'huey': [(14, 2)], 'huh': [(3, 2)], 'hungri': [(14, 1)], 'hunnybun': [(17, 3)], 'hunt': [(12, 1)], 'hurri': [(4, 2)], 'hurt': [(4, 1), (7, 1), (8, 1), (14, 1)], 'huski': [(10, 1)], 'ice': [(4, 1)], 'ickey': [(17, 1)], 'ihhh': [(13, 4)], 'ika': [(1, 1)], 'ikaw': [(1, 1)], 'impli': [(4, 1)], 'importa': [(10, 1)], 'impress': [(7, 1)], 'incostanti': [(10, 1)], 'indecent': [(15, 1)], 'inocent': [(15, 2)], 'iphon': [(18, 3)], 'isang': [(1, 2)], 'ita': [(13, 36)], 'italian': [(17, 1)], 'ito': [(1, 1)], 'iyong': [(1, 4)], 'ja': [(13, 8)], 'jack': [(14, 1)], 'jaja': [(15, 1)], 'jajaja': [(15, 1)], 'japanes': [(16, 1)], 'jazz': [(16, 1)], 'jealous': [(6, 1), (14, 1)], 'jet': [(7, 1)], 'johnni': [(3, 1)], 'jugueton': [(11, 2)], 'jump': [(2, 1)], 'juro': [(13, 2)], 'ka': [(1, 5)], 'kailan': [(1, 1)], 'kay': [(1, 4)], 'kaya': [(1, 2)]}\n",
      "Existen  20  documentos\n",
      "query:  Everybody here wants you My love, my love And I know that you want 'em too My love, my love I ask you what your heart desires My love, my love You tell me I'm the only one My love, my love It's a lie, a lie I catch you every time In your lies, your lies Every time you close your eyes I hear the secrets that you keep When you're talkin' in your sleep I hear the secrets that you keep, keep, keep When you're talk, talkin', talkin' I hear the secrets that you keep When you're talkin' in your sleep I hear the secrets that you keep, keep, keep When you're talk, talkin', talkin' You told me not to worry 'bout Those guys, those guys You told me that you left it all behind Behind It's a lie, a lie I catch you every time In your lies, your lies Every time you close your eyes I hear the secrets that you keep When you're talkin' in your sleep I hear the secrets that you keep, keep, keep When you're talk, talkin', talkin' I hear the secrets that you keep When you're talkin' in your sleep I hear the secrets that you keep, keep, keep When you're talk, talkin', talkin' Wipe the... Lies from your eyes I see that you're not mine I can see the lust in your eyes You can't hide it You can't be the one I realize, we're divided I hear the secrets that you keep When you're talkin' in your sleep I hear the secrets that you keep, keep, keep When you're talk, talkin', talkin' I hear the secrets that you keep When you're talkin' in your sleep I hear the secrets that you keep, keep, keep When you're talk, talkin', talkin'\n",
      "terms:  ['love', 'love', 'em', 'love', 'love', 'heart', 'desir', 'love', 'love', 'love', 'love', 'lie', 'lie', 'catch', 'time', 'lie', 'lie', 'time', 'close', 'eye', 'hear', 'secret', 'talkin', 'sleep', 'hear', 'secret', 'talk', 'talkin', 'talkin', 'hear', 'secret', 'talkin', 'sleep', 'hear', 'secret', 'talk', 'talkin', 'talkin', 'told', 'worri', 'bout', 'guy', 'guy', 'told', 'left', 'lie', 'lie', 'catch', 'time', 'lie', 'lie', 'time', 'close', 'eye', 'hear', 'secret', 'talkin', 'sleep', 'hear', 'secret', 'talk', 'talkin', 'talkin', 'hear', 'secret', 'talkin', 'sleep', 'hear', 'secret', 'talk', 'talkin', 'talkin', 'wipe', 'lie', 'eye', 'lust', 'eye', 'hide', 'realiz', 'divid', 'hear', 'secret', 'talkin', 'sleep', 'hear', 'secret', 'talk', 'talkin', 'talkin', 'hear', 'secret', 'talkin', 'sleep', 'hear', 'secret', 'talk', 'talkin', 'talkin']\n",
      "tf_query es:  {'love': 8, 'em': 1, 'heart': 1, 'desir': 1, 'lie': 9, 'catch': 2, 'time': 4, 'close': 2, 'eye': 4, 'hear': 12, 'secret': 12, 'talkin': 18, 'sleep': 6, 'talk': 6, 'told': 2, 'worri': 1, 'bout': 1, 'guy': 2, 'left': 1, 'wipe': 1, 'lust': 1, 'hide': 1, 'realiz': 1, 'divid': 1}\n",
      "tfidf_query es:  {'love': 0.09247565400260538, 'em': 0.1812381165789131, 'heart': 0.137249194632307, 'desir': 0.3010299956639812, 'lie': 0.6020599913279624, 'catch': 0.47712125471966244, 'time': 0.155065621481285, 'close': 0.3334934454802092, 'eye': 0.42082187474904936, 'hear': 0.9177876648824196, 'secret': 0.9177876648824196, 'talkin': 1.2787536009528289, 'sleep': 0.8450980400142568, 'talk': 0.6962836621226578, 'told': 0.3931043722538507, 'worri': 0.137249194632307, 'bout': 0.1812381165789131, 'guy': 0.47712125471966244, 'left': 0.3010299956639812, 'wipe': 0.3010299956639812, 'lust': 0.3010299956639812, 'hide': 0.3010299956639812, 'realiz': 0.3010299956639812, 'divid': 0.3010299956639812}\n",
      "result es [(18, 6.549786089675727), (16, 0.14258520088334747), (6, 0.09798482020620321), (3, 0.04242887561736865), (2, 0.0361165529178456), (13, 0.027706104557471113), (17, 0.01641350278713123), (4, 0.014990838475729571), (1, 0.008383212270222203), (7, 0.007718932195353721), (8, 0.0066249628366521615), (11, 0.0017968208972935242), (5, 0.0016997272617340676), (15, 0.0006669646413204618), (14, 0.00026321189338788023), (0, 0.0), (9, 0.0), (10, 0.0), (12, 0.0), (19, 0)]\n",
      "Documento 18  con similitud:  6.549786089675727\n",
      "Documento 16  con similitud:  0.14258520088334747\n",
      "Documento 6  con similitud:  0.09798482020620321\n",
      "Documento 3  con similitud:  0.04242887561736865\n",
      "Documento 2  con similitud:  0.0361165529178456\n",
      "_____\n"
     ]
    }
   ],
   "source": [
    "def mostrarDocumentos(result):\n",
    "    # print(result)\n",
    "    for doc, score in result:\n",
    "        print(\"Documento\", doc, \" con similitud: \", score)\n",
    "    print(\"_____\")   \n",
    "    \n",
    "query = dataset.iloc[19,3]\n",
    "top_k = 5\n",
    "result = s.retrieval(query, top_k)\n",
    "mostrarDocumentos(result)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
