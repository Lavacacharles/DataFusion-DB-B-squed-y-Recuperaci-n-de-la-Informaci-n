{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Proyecto jiji"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: kagglehub in c:\\users\\ce mar\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (0.3.4)\n",
      "Requirement already satisfied: requests in c:\\users\\ce mar\\appdata\\roaming\\python\\python310\\site-packages (from kagglehub) (2.28.1)\n",
      "Requirement already satisfied: tqdm in c:\\users\\ce mar\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from kagglehub) (4.66.1)\n",
      "Requirement already satisfied: packaging in c:\\users\\ce mar\\appdata\\roaming\\python\\python310\\site-packages (from kagglehub) (23.2)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\ce mar\\appdata\\roaming\\python\\python310\\site-packages (from requests->kagglehub) (1.26.13)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in c:\\users\\ce mar\\appdata\\roaming\\python\\python310\\site-packages (from requests->kagglehub) (2.1.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\ce mar\\appdata\\roaming\\python\\python310\\site-packages (from requests->kagglehub) (3.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\ce mar\\appdata\\roaming\\python\\python310\\site-packages (from requests->kagglehub) (2022.12.7)\n",
      "Requirement already satisfied: colorama in c:\\users\\ce mar\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from tqdm->kagglehub) (0.4.6)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip available: 22.2.1 -> 24.3.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "!pip install kagglehub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to C:\\Users\\Ce\n",
      "[nltk_data]     mar\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "import numpy as np\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "nltk.download('punkt')\n",
    "import regex as re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "stemmer = SnowballStemmer('english')\n",
    "import os\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"stopwords-en.txt\", encoding=\"latin1\") as file:\n",
    "   stoplist = [line.rstrip().lower() for line in file]\n",
    "stoplist += ['?', '-', '.', ':', ',', '!', ';']\n",
    "\n",
    "def preprocesamiento(texto, stemming=True):\n",
    "  words = []\n",
    "  texto = str(texto)\n",
    "  texto = texto.lower()\n",
    "  texto = re.sub(r'[^a-zA-Z0-9_À-ÿ]', ' ', texto)\n",
    "  # tokenizar\n",
    "  words = nltk.word_tokenize(texto, language='spanish')\n",
    "  # filtrar stopwords\n",
    "  words = [word for word in words if word not in stoplist]\n",
    "  # reducir palabras (stemming)\n",
    "  if stemming:\n",
    "      words = [stemmer.stem(word) for word in words]\n",
    "  return words\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def OrdenarPorBloques(dir_blocks, n_blocks):\n",
    "    for i in range(n_blocks):\n",
    "        file_path = os.path.join(dir_blocks, 'block_{}.pkl'.format(i))\n",
    "        with open(file_path, 'rb') as f:\n",
    "            tuplas_ordenadas = pickle.load(f)\n",
    "        tuplas_ordenadas = sorted(list(tuplas_ordenadas.items()), key=lambda x: x[0])\n",
    "        # tuplas_ordenadas = [par for par in tuplas_ordenadas if not par[0].isdigit()]\n",
    "        tuplas_ordenadas = [(term[0], list(term[1].items())) for term in tuplas_ordenadas]\n",
    "        with open(file_path, 'wb') as f:\n",
    "            pickle.dump(tuplas_ordenadas, f)\n",
    "\n",
    "def mergeSortAux(dir_bloques, l, r):\n",
    "    if l == r:\n",
    "        bloque = leer_bloque(dir_bloques, l)\n",
    "        unicos = set()    \n",
    "        for par in bloque:\n",
    "            unicos.add(par[0])\n",
    "        return list(unicos)\n",
    "    \n",
    "    if (l < r):\n",
    "        mid = int(math.ceil((r + l)/2.0))\n",
    "        unique_l = mergeSortAux(dir_bloques, l, mid - 1)\n",
    "        unique_r = mergeSortAux(dir_bloques, mid, r)\n",
    "        unicos = set()    \n",
    "        for term in unique_l:\n",
    "            unicos.add(term)\n",
    "        for term in unique_r:\n",
    "            unicos.add(term)\n",
    "        unicos = list(unicos)\n",
    "        merge_v2(dir_bloques, l, r, mid, len(unicos))\n",
    "        return list(unicos)\n",
    "        \n",
    "    return []\n",
    "\n",
    "def escribir_bloque(dir_bloques, block, idx_insert_block, buffer_limit = 2000):\n",
    "    with open(os.path.join(dir_bloques, \"block_{}_v2.pkl\".format(idx_insert_block)), 'wb') as f:\n",
    "        pickle.dump(block, f)    \n",
    "def leer_bloque(dir_bloques, it):\n",
    "    file_path = os.path.join(dir_bloques, f\"block_{it}.pkl\")\n",
    "    with open(file_path, \"rb\") as f:\n",
    "        buffer = pickle.load(f)\n",
    "    return buffer\n",
    "def merge_v2(dir_bloques, l, r, mid, num_terms):\n",
    "    idx_insert_block = l\n",
    "    new_block = []\n",
    "    mezclar_n_bloques = r - l + 1\n",
    "    unique_terms_per_block = int(math.ceil(num_terms/mezclar_n_bloques))\n",
    "    unique_terms_current_block = 0\n",
    "\n",
    "    it_l = l\n",
    "    it_r = mid\n",
    "    term_dic_l = leer_bloque(dir_bloques, it_l)\n",
    "    term_dic_r = leer_bloque(dir_bloques, it_r)\n",
    "    \n",
    "    idx_term_l = 0\n",
    "    idx_term_r = 0\n",
    "\n",
    "    idx_doc_l = 0\n",
    "    idx_doc_r = 0\n",
    "    new_block = []\n",
    "    while(it_l < mid and it_r < r + 1):\n",
    "        while(idx_term_l < len(term_dic_l) and idx_term_r < len(term_dic_r)): # moverme entre palabras de dos bloques\n",
    "            new_term = []\n",
    "            if(term_dic_l[idx_term_l][0] < term_dic_r[idx_term_r][0]):\n",
    "                new_term = term_dic_l[idx_term_l]\n",
    "                idx_term_l += 1\n",
    "            elif(term_dic_l[idx_term_l][0] > term_dic_r[idx_term_r][0]):\n",
    "                new_term = term_dic_r[idx_term_r]\n",
    "                idx_term_r += 1\n",
    "            else:\n",
    "                idx_doc_l = 0\n",
    "                idx_doc_r = 0\n",
    "                while(idx_doc_l < len(term_dic_l[idx_term_l][1]) and idx_doc_r < len(term_dic_r[idx_term_r][1])):\n",
    "                    if term_dic_l[idx_term_l][1][idx_doc_l][0] > term_dic_r[idx_term_r][1][idx_doc_r][0]:\n",
    "                        pushear_doc = term_dic_r[idx_term_r][1][idx_doc_r]\n",
    "                        idx_doc_r += 1\n",
    "                    elif term_dic_l[idx_term_l][1][idx_doc_l][0] < term_dic_r[idx_term_r][1][idx_doc_r][0]:\n",
    "                        pushear_doc = term_dic_l[idx_term_l][1][idx_doc_l]\n",
    "                        idx_doc_l += 1\n",
    "                    else:\n",
    "                        pushear_doc = (term_dic_l[idx_term_l][1][idx_doc_l][0], term_dic_l[idx_term_l][1][idx_doc_l][1] + term_dic_r[idx_term_r][1][idx_doc_r][1])\n",
    "                        idx_doc_l += 1\n",
    "                        idx_doc_r += 1\n",
    "                    new_term.append(pushear_doc)\n",
    "                while(idx_doc_l < len(term_dic_l[idx_term_l][1])):\n",
    "                    pushear_doc = term_dic_l[idx_term_l][1][idx_doc_l]\n",
    "                    idx_doc_l += 1\n",
    "                    new_term.append(pushear_doc)\n",
    "                while(idx_doc_r < len(term_dic_r[idx_term_r][1])):\n",
    "                    pushear_doc = term_dic_r[idx_term_r][1][idx_doc_r]\n",
    "                    idx_doc_r += 1\n",
    "                    new_term.append(pushear_doc)\n",
    "                new_term = (term_dic_l[idx_term_l][0], new_term)\n",
    "                idx_term_r += 1\n",
    "                idx_term_l += 1\n",
    "            new_block.append(new_term)\n",
    "            \n",
    "            unique_terms_current_block += 1\n",
    "            if (unique_terms_current_block == unique_terms_per_block):\n",
    "                escribir_bloque(dir_bloques, new_block, idx_insert_block)\n",
    "                unique_terms_current_block = 0\n",
    "                idx_insert_block += 1\n",
    "                new_block = []\n",
    "        if(len(term_dic_l) == idx_term_l):\n",
    "            if (it_l < mid - 1):\n",
    "                it_l += 1\n",
    "                term_dic_l = leer_bloque(dir_bloques, it_l)\n",
    "                idx_term_l = 0\n",
    "                idx_doc_l = 0\n",
    "                continue\n",
    "            else:\n",
    "                break\n",
    "        if(len(term_dic_r) == idx_term_r):\n",
    "            if (it_r < r):\n",
    "                it_r += 1\n",
    "                term_dic_r = leer_bloque(dir_bloques, it_r)\n",
    "                idx_term_r = 0\n",
    "                idx_doc_r = 0\n",
    "                continue\n",
    "            else:\n",
    "                break\n",
    "        if(it_l == mid | it_r == r + 1):\n",
    "            break\n",
    "    while(it_l < mid):\n",
    "        term_dic_l = leer_bloque(dir_bloques, it_l)\n",
    "        while(idx_term_l < len(term_dic_l)):\n",
    "            new_block.append(term_dic_l[idx_term_l])\n",
    "            unique_terms_current_block += 1\n",
    "            if (unique_terms_current_block == unique_terms_per_block):\n",
    "                escribir_bloque(dir_bloques, new_block, idx_insert_block)\n",
    "                unique_terms_current_block = 0\n",
    "                idx_insert_block += 1\n",
    "                new_block = []\n",
    "            idx_term_l += 1\n",
    "        idx_term_l = 0\n",
    "        it_l += 1\n",
    "    while(it_r < r + 1):\n",
    "        term_dic_r = leer_bloque(dir_bloques, it_r)\n",
    "        while(idx_term_r < len(term_dic_r)):\n",
    "            new_block.append(term_dic_r[idx_term_r])\n",
    "            unique_terms_current_block += 1\n",
    "            if (unique_terms_current_block == unique_terms_per_block):\n",
    "                escribir_bloque(dir_bloques, new_block, idx_insert_block)\n",
    "                unique_terms_current_block = 0\n",
    "                idx_insert_block += 1\n",
    "                new_block = []\n",
    "            idx_term_r += 1\n",
    "        idx_term_r = 0\n",
    "        it_r += 1\n",
    "\n",
    "    while(idx_insert_block < r + 1):\n",
    "        if len(new_block) > 0:\n",
    "            escribir_bloque(dir_bloques, new_block, idx_insert_block)\n",
    "        else:\n",
    "            escribir_bloque(dir_bloques, [], idx_insert_block)\n",
    "        new_block = []\n",
    "        idx_insert_block += 1\n",
    "    idx_insert_block = l\n",
    "    for idx_archivo in range(l, r + 1):\n",
    "        nuevo_nombre = os.path.join(dir_bloques, \"block_{}.pkl\".format(idx_archivo))\n",
    "        if os.path.exists(nuevo_nombre):\n",
    "            os.remove(nuevo_nombre)\n",
    "        os.rename(os.path.join(dir_bloques, \"block_{}_v2.pkl\".format(idx_archivo)), nuevo_nombre)\n",
    "def mergeSort(dir_bloques):\n",
    "    bloques_files_dir = os.listdir(os.path.join('./',dir_bloques))\n",
    "    n = len(bloques_files_dir)\n",
    "    mergeSortAux(dir_bloques, 0, n - 1)\n",
    "'''\n",
    "Recibe la direccion del folder con los diccionarios del spimi,\n",
    "modifica los archivos y los sobreescribe para crear el índice\n",
    "invertido con un índice global\n",
    "'''\n",
    "def InvertirListasDiccionarios(dir_bloques):\n",
    "    n_bloques = len(os.listdir(os.path.join('./',dir_bloques)))\n",
    "    for i in range(n_bloques):\n",
    "        bloque_path = os.path.join(dir_bloques,\"block_{}.pkl\".format(i))\n",
    "        with open(bloque_path, 'rb') as f:\n",
    "            bloque = pickle.load(f)\n",
    "        bloque_dict = {}\n",
    "        if len(bloque) != 0:\n",
    "            for term, poosting_list in bloque:\n",
    "                poosting_dict = {}\n",
    "                for doc, tf in poosting_list:\n",
    "                    poosting_dict[doc] = tf\n",
    "                bloque_dict[term] = poosting_dict\n",
    "        with open(bloque_path, 'wb') as f:\n",
    "            pickle.dump(bloque_dict, f)\n",
    "\n",
    "def getNumberWithAtributo(dataset_head, atributo):\n",
    "    if atributo in dataset_head.columns:\n",
    "        return dataset_head.columns.get_loc(atributo)  # Obtiene la posición del atributo\n",
    "    return -1  # Retorna -1 si no existe\n",
    " \n",
    "\n",
    "def getResultados(result, path, disk_limit):\n",
    "    res = []\n",
    "    for chunk in pd.read_csv(path, chunksize=disk_limit):\n",
    "        for doc, score in result:\n",
    "            if doc in chunk.index:\n",
    "                res.append((chunk.iloc[doc], score))\n",
    "    return pd.DataFrame(res, columns=['doc_id', 'score'])  \n",
    "def getResultadosDF(result, path, disk_limit):\n",
    "    res = []\n",
    "    for chunk in pd.read_csv(path, chunksize=disk_limit):\n",
    "        for doc, score in result:\n",
    "            row = chunk.iloc[doc].copy()\n",
    "            row['score'] = score  \n",
    "            res.append(row)\n",
    "    return pd.DataFrame(res)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "import sys\n",
    "import math\n",
    "import dbm\n",
    "import time\n",
    "\n",
    "class SPIMI:\n",
    "    def __init__(self, index_dataset_path=\"path\",index_dir=\"index_blocks\" , position = 3, columnas = [\"track_name\",\"track_artist\",\"lyrics\", \"track_album_name\"]):\n",
    "        self.index_dir = index_dir  \n",
    "        self.dataset_path = index_dataset_path  \n",
    "        self.block_counter = 0      \n",
    "        self.doc_ids = set()         \n",
    "        self.idf = {}\n",
    "        self.length = {}\n",
    "        self.disk_limit = 4000  \n",
    "        self.position = position\n",
    "        \n",
    "        #--Columnas:\n",
    "        self.columnas = columnas \n",
    "\n",
    "\n",
    "        # ----Direciones de memoria para los files--\n",
    "        self.tf_file_dir = index_dir+\"_tf\"\n",
    "        self.idf_file_dir = index_dir+\"_idf\"\n",
    "        self.length_file_dir = index_dir+\"_lenght\"\n",
    "        # ----Direciones de memoria para los files--\n",
    "\n",
    "        if not os.path.exists(self.index_dir):\n",
    "            os.makedirs(self.index_dir)\n",
    "    def make_tf_idf_length(self): #Despuesd e convertir todo a diccionarios \n",
    "        N = len(self.doc_ids)  \n",
    "        files = sorted(os.listdir(self.index_dir))\n",
    "\n",
    "        #Creación de los files: \n",
    "        if not os.path.exists(self.tf_file_dir):\n",
    "            with dbm.open(self.tf_file_dir, \"c\") as tf_file:\n",
    "                print(\"Archivo de índice creado:\", self.index_dir)\n",
    "\n",
    "        if not os.path.exists(self.idf_file_dir):\n",
    "            with dbm.open(self.idf_file_dir, \"c\") as idf_file:\n",
    "                print(\"Archivo de IDF creado:\", self.idf_file_dir)\n",
    "\n",
    "        if not os.path.exists(self.length_file_dir):\n",
    "            with dbm.open(self.length_file_dir, \"c\") as length_file:\n",
    "                print(\"Archivo de longitud creado:\", self.length_file_dir)\n",
    "\n",
    "        #Iteramos: \n",
    "        tf_dict = {}\n",
    "        idf_dict = {}\n",
    "        length_dict = {}\n",
    "        with dbm.open(self.length_file_dir, \"c\") as length_db: \n",
    "            with dbm.open(self.idf_file_dir, \"c\") as idf_db: \n",
    "                with dbm.open(self.tf_file_dir, \"c\") as tf_db: \n",
    "                    for file in files:\n",
    "                        if file.endswith(\".pkl\"):\n",
    "                            block = self.load_block(os.path.join(self.index_dir, file)) #Abro el bloqque\n",
    "                            for term, postings in block.items():\n",
    "                                doc_freq = len(postings) # Calculate N docs\n",
    "                                idf_dict[term] = math.log10(len(self.doc_ids) / (1 + doc_freq))\n",
    "\n",
    "                                if term not in tf_dict:\n",
    "                                    tf_dict[term] = []\n",
    "                                for doc_id, tf in postings.items(): # Guardo el tf\n",
    "                                    tf_log = math.log10(1 + tf)\n",
    "                                    tf_dict[term].append((doc_id, tf_log))\n",
    "\n",
    "                                    # Calcular contribución al cuadrado de la norma del documento\n",
    "                                    if doc_id not in length_dict:\n",
    "                                        length_dict[doc_id] = 0\n",
    "                                    length_dict[doc_id] += (tf_log * idf_dict[term]) ** 2\n",
    "\n",
    "                                 #Guardar en tf_file\n",
    "                                tf_db[term.encode()] = pickle.dumps(tf_dict[term])\n",
    "                                tf_dict.clear() \n",
    "\n",
    "                                 #Guardar en idf_file\n",
    "                                idf_db[term.encode()] = pickle.dumps(idf_dict[term])\n",
    "                                idf_db.clear()\n",
    "\n",
    "                    for doc_id, squared_sum in length_dict.items():\n",
    "                        length_dict[doc_id] = math.sqrt(squared_sum)\n",
    "                        \n",
    "                    for doc_id, norm in length_dict.items():\n",
    "                        length_db[str(doc_id).encode()] = pickle.dumps(norm)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    def spimi_invert(self):\n",
    "        dictionary = {}\n",
    "        doc_ids = set()\n",
    "        time_spimiInvert_start = time.time()\n",
    "\n",
    "        #Get columna indeces: \n",
    "        with open(self.dataset_path, mode='r', encoding='utf-8') as file:\n",
    "            primera_linea = file.readline().strip()\n",
    "        #Get number columns\n",
    "        columnas = primera_linea.split(',')\n",
    "        columnas_numbers  = [i for i in range(len(columnas)) if columnas[i] in self.columnas]\n",
    "\n",
    "        for chunk in pd.read_csv(self.dataset_path,chunksize= self.disk_limit):\n",
    "            for doc_id_, row in chunk.iterrows():\n",
    "                preFila = [str(row.iloc[i]) for i in columnas_numbers]\n",
    "                texto = ' '.join(item for item in preFila)\n",
    "                words = preprocesamiento(texto)\n",
    "                for text in words:\n",
    "                    doc_id = doc_id_\n",
    "                    doc_ids.add(doc_id)\n",
    "                    token = text\n",
    "                    self.doc_ids.add(doc_id)\n",
    "                    if token not in dictionary:\n",
    "                        dictionary[token] = {}  \n",
    "\n",
    "                    if doc_id not in dictionary[token]:\n",
    "                        dictionary[token][doc_id] = 1  \n",
    "                    else:\n",
    "                        dictionary[token][doc_id] += 1  \n",
    "\n",
    "                    dictionary_size = sys.getsizeof(dictionary)\n",
    "                    if dictionary_size >= self.disk_limit:\n",
    "                        self.write_block_to_disk(dictionary, level=0)\n",
    "                        dictionary.clear()\n",
    "\n",
    "        if dictionary:\n",
    "            self.write_block_to_disk(dictionary, level=0)  \n",
    "        self.n_docs = len(doc_ids)\n",
    "        time_spimiInvert = time.time()- time_spimiInvert_start\n",
    "        \n",
    "        \n",
    "        time_bloques_start = time.time()\n",
    "        OrdenarPorBloques(dir_blocks=self.index_dir, n_blocks=self.block_counter)\n",
    "        time_bloques = time.time() - time_bloques_start\n",
    "\n",
    "        time_merge_start = time.time()\n",
    "        mergeSort(self.index_dir)\n",
    "        time_merge = time.time() -time_merge_start  \n",
    "\n",
    "        time_invertir_start = time.time()\n",
    "        InvertirListasDiccionarios(self.index_dir)\n",
    "        time_invertir = time.time() -time_invertir_start  \n",
    "        \n",
    "        time_tf_idf_length_start = time.time()\n",
    "        self.make_tf_idf_length()\n",
    "        time_tf_idf_length = time.time() - time_tf_idf_length_start\n",
    "\n",
    "\n",
    "        print(\"\\nResumen de tiempos:\")\n",
    "        print(f\"Tiempo para SpimiInvert: {time_spimiInvert:.2f} segundos\")\n",
    "        print(f\"Tiempo para OrdenarPorBloques: {time_bloques:.2f} segundos\")\n",
    "        print(f\"Tiempo para mergeSort: {time_merge:.2f} segundos\")\n",
    "        print(f\"Tiempo para InvertirListasDiccionarios: {time_invertir:.2f} segundos\")\n",
    "        print(f\"Tiempo para Make_tf_idf_length: {time_tf_idf_length:.2f} segundos\")\n",
    "        print(f\"Tiempo total de creación del índice: {time_bloques + time_merge + time_invertir+  time_tf_idf_length :.2f} segundos\")\n",
    "        \n",
    "    def write_block_to_disk(self, dictionary, level):\n",
    "        sorted_terms = dict(sorted(dictionary.items())) \n",
    "        file_path = os.path.join(self.index_dir, f\"block_{self.block_counter}.pkl\")\n",
    "        with open(file_path, \"wb\") as f:\n",
    "            pickle.dump(sorted_terms, f)\n",
    "        \n",
    "        self.block_counter += 1\n",
    "\n",
    "    def load_block(self, filepath):\n",
    "        with open(filepath, \"rb\") as f:\n",
    "            return pickle.load(f)\n",
    "\n",
    "    \n",
    "    def retrieval(self, query, k):\n",
    "        N = len(self.doc_ids)  \n",
    "        scores = [0] * N  \n",
    "        tf_query = {}  \n",
    "        terms = preprocesamiento(query)  \n",
    "\n",
    "        # Calcular el TF-IDF del query\n",
    "        for term in terms:\n",
    "            if term in tf_query:\n",
    "                tf_query[term] += 1\n",
    "            else:\n",
    "                tf_query[term] = 1\n",
    "\n",
    "        tfidf_query = {}\n",
    "        with dbm.open(self.idf_file_dir, \"r\") as idf_db:\n",
    "            for term, tf in tf_query.items():\n",
    "                idf_encoded = idf_db.get(term.encode())\n",
    "                if idf_encoded:\n",
    "                    idf = pickle.loads(idf_encoded)\n",
    "                    tfidf_query[term] = math.log10(1 + tf) * idf\n",
    "        norm_query = math.sqrt(sum(w_tq**2 for w_tq in tfidf_query.values()))\n",
    "\n",
    "        # similitud de coseno\n",
    "        with dbm.open(self.tf_file_dir, \"r\") as tf_db:\n",
    "            with dbm.open(self.length_file_dir, \"r\") as length_db:\n",
    "                for term, w_tq in tfidf_query.items():\n",
    "                    tf_encoded = tf_db.get(term.encode())\n",
    "                    if tf_encoded:\n",
    "                        postings = pickle.loads(tf_encoded)  # Lista de (doc_id, tf)\n",
    "                        for doc_id, tf_log in postings:\n",
    "                            # Multiplicamos el tf por el idf y luego por la consulta tf-idf\n",
    "                            idf_encoded = idf_db.get(term.encode())  # Obtener IDF del término\n",
    "                            if idf_encoded:\n",
    "                                idf0 = pickle.loads(idf_encoded)\n",
    "                                tfidf_td = tf_log * idf  # TF * IDF del término en el documento\n",
    "                                scores[doc_id] += tfidf_td * w_tq  # Producto punto\n",
    "        # Normalizar las puntuaciones de los documentos\n",
    "\n",
    "                for doc_id in range(N):\n",
    "                    norm_encoded = length_db.get(str(doc_id).encode())\n",
    "\n",
    "                    if norm_encoded:\n",
    "                        doc_norm = pickle.loads(norm_encoded)\n",
    "                        \n",
    "                        if doc_norm != 0:\n",
    "                            print(\"norm_query es: \", norm_query)\n",
    "                            scores[doc_id] /= (doc_norm * norm_query)\n",
    "\n",
    "        # Ordenar las puntuaciones en orden descendente\n",
    "        result = sorted(enumerate(scores), key=lambda x: x[1], reverse=True)\n",
    "\n",
    "        # Devolver los k documentos más relevantes (top-k)\n",
    "        return result[:k]\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Archivo de índice creado: index_blocks\n",
      "Archivo de IDF creado: index_blocks_idf\n",
      "Archivo de longitud creado: index_blocks_lenght\n",
      "\n",
      "Resumen de tiempos:\n",
      "Tiempo para SpimiInvert: 0.43 segundos\n",
      "Tiempo para OrdenarPorBloques: 0.14 segundos\n",
      "Tiempo para mergeSort: 1.38 segundos\n",
      "Tiempo para InvertirListasDiccionarios: 0.17 segundos\n",
      "Tiempo para Make_tf_idf_length: 52.23 segundos\n",
      "Tiempo total de creación del índice: 53.92 segundos\n",
      "norm_query es:  0.0\n"
     ]
    },
    {
     "ename": "ZeroDivisionError",
     "evalue": "float division by zero",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mZeroDivisionError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[19], line 17\u001b[0m\n\u001b[0;32m     14\u001b[0m \u001b[38;5;66;03m# query = get_dfTex_Cols(songs, 2, [1,2,3,6])\u001b[39;00m\n\u001b[0;32m     15\u001b[0m top_k \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m5\u001b[39m\n\u001b[1;32m---> 17\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[43ms\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mretrieval\u001b[49m\u001b[43m(\u001b[49m\u001b[43mquery\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtop_k\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     18\u001b[0m respuesta \u001b[38;5;241m=\u001b[39m getResultados(result, s\u001b[38;5;241m.\u001b[39mdataset_path, s\u001b[38;5;241m.\u001b[39mdisk_limit) \u001b[38;5;66;03m# La respuesta es un array pd de [fila del dataframe, score]\u001b[39;00m\n\u001b[0;32m     19\u001b[0m \u001b[38;5;66;03m#-----------End of query------------\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[18], line 215\u001b[0m, in \u001b[0;36mSPIMI.retrieval\u001b[1;34m(self, query, k)\u001b[0m\n\u001b[0;32m    213\u001b[0m                 \u001b[38;5;28;01mif\u001b[39;00m doc_norm \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m    214\u001b[0m                     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnorm_query es: \u001b[39m\u001b[38;5;124m\"\u001b[39m, norm_query)\n\u001b[1;32m--> 215\u001b[0m                     scores[doc_id] \u001b[38;5;241m/\u001b[39m\u001b[38;5;241m=\u001b[39m (doc_norm \u001b[38;5;241m*\u001b[39m norm_query)\n\u001b[0;32m    217\u001b[0m \u001b[38;5;66;03m# Ordenar las puntuaciones en orden descendente\u001b[39;00m\n\u001b[0;32m    218\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28msorted\u001b[39m(\u001b[38;5;28menumerate\u001b[39m(scores), key\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mlambda\u001b[39;00m x: x[\u001b[38;5;241m1\u001b[39m], reverse\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "\u001b[1;31mZeroDivisionError\u001b[0m: float division by zero"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# ----------Set Atributo--------\n",
    "# columna = getNumberWithAtributo(dataset, \"lyrics\")\n",
    "path = \"spotify_20.csv\"\n",
    "# ---------Creation---------\n",
    "s = SPIMI( path)  \n",
    "s.spimi_invert() \n",
    "\n",
    "# ---------End Creation---------\n",
    "\n",
    "\n",
    "#\n",
    "# query = get_dfText(songs, 3, 0, s.disk_limit)\n",
    "query = \"this is my breath\"\n",
    "# query = get_dfTex_Cols(songs, 2, [1,2,3,6])\n",
    "top_k = 5\n",
    "\n",
    "result = s.retrieval(query, top_k)\n",
    "respuesta = getResultados(result, s.dataset_path, s.disk_limit) # La respuesta es un array pd de [fila del dataframe, score]\n",
    "#-----------End of query------------\n",
    "\n",
    "\n",
    "respuestadf = getResultadosDF(result, s.dataset_path, s.disk_limit)\n",
    "\n",
    "print(respuestadf[\"track_name\"], respuestadf[\"score\"])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19}"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "def verificar_termino(tf_file_dir, term):\n",
    "    with dbm.open(tf_file_dir, \"r\") as tf_db:  \n",
    "        try:\n",
    "            term_key = term.encode()\n",
    "            postings_list = pickle.loads(tf_db[term_key])\n",
    "            print(f\"Postings list para '{term}': {postings_list}\")\n",
    "        except KeyError:\n",
    "            print(f\"El término '{term}' no se encuentra en el archivo DBM.\")\n",
    "\n",
    "# verificar_termino(s.tf_file_dir, \"6\") "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
