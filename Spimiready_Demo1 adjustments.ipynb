{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Proyecto jiji"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: kagglehub in c:\\users\\ce mar\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (0.3.4)\n",
      "Requirement already satisfied: requests in c:\\users\\ce mar\\appdata\\roaming\\python\\python310\\site-packages (from kagglehub) (2.28.1)\n",
      "Requirement already satisfied: tqdm in c:\\users\\ce mar\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from kagglehub) (4.66.1)\n",
      "Requirement already satisfied: packaging in c:\\users\\ce mar\\appdata\\roaming\\python\\python310\\site-packages (from kagglehub) (23.2)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in c:\\users\\ce mar\\appdata\\roaming\\python\\python310\\site-packages (from requests->kagglehub) (2.1.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\ce mar\\appdata\\roaming\\python\\python310\\site-packages (from requests->kagglehub) (2022.12.7)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\ce mar\\appdata\\roaming\\python\\python310\\site-packages (from requests->kagglehub) (1.26.13)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\ce mar\\appdata\\roaming\\python\\python310\\site-packages (from requests->kagglehub) (3.4)\n",
      "Requirement already satisfied: colorama in c:\\users\\ce mar\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from tqdm->kagglehub) (0.4.6)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip available: 22.2.1 -> 24.3.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "!pip install kagglehub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Ce mar\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\scipy\\__init__.py:177: UserWarning: A NumPy version >=1.18.5 and <1.26.0 is required for this version of SciPy (detected version 1.26.2\n",
      "  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n",
      "[nltk_data] Downloading package punkt to C:\\Users\\Ce\n",
      "[nltk_data]     mar\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "import numpy as np\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "nltk.download('punkt')\n",
    "import regex as re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "stemmer = SnowballStemmer('english')\n",
    "import os\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"stopwords-en.txt\", encoding=\"latin1\") as file:\n",
    "   stoplist = [line.rstrip().lower() for line in file]\n",
    "stoplist += ['?', '-', '.', ':', ',', '!', ';']\n",
    "\n",
    "def preprocesamiento(texto, stemming=True):\n",
    "  words = []\n",
    "  texto = str(texto)\n",
    "  texto = texto.lower()\n",
    "  texto = re.sub(r'[^a-zA-Z0-9_À-ÿ]', ' ', texto)\n",
    "  # tokenizar\n",
    "  words = nltk.word_tokenize(texto, language='spanish')\n",
    "  # filtrar stopwords\n",
    "  words = [word for word in words if word not in stoplist]\n",
    "  # reducir palabras (stemming)\n",
    "  if stemming:\n",
    "      words = [stemmer.stem(word) for word in words]\n",
    "  return words\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "import sys\n",
    "import math\n",
    "import dbm\n",
    "\n",
    "class SPIMI:\n",
    "    def __init__(self, index_dir=\"index_blocks\", index_dataset_path=\"path\" , position = 3):\n",
    "        self.index_dir = index_dir  \n",
    "        self.dataset_path = index_dataset_path  \n",
    "        self.block_counter = 0      \n",
    "        self.doc_ids = set()         \n",
    "        self.idf = {}\n",
    "        self.length = {}\n",
    "        self.disk_limit = 4000  \n",
    "        self.position = position\n",
    "\n",
    "        # ----Direciones de memoria para los files--\n",
    "        self.tf_file_dir = index_dir+\"_tf\"\n",
    "        self.idf_file_dir = index_dir+\"_idf\"\n",
    "        self.length_file_dir = index_dir+\"_lenght\"\n",
    "        # ----Direciones de memoria para los files--\n",
    "\n",
    "        if not os.path.exists(self.index_dir):\n",
    "            os.makedirs(self.index_dir)\n",
    "\n",
    "    def spimi_invert(self, limit = 20):\n",
    "        dictionary = {}\n",
    "        for chunk in pd.read_csv(self.dataset_path, chunksize=1000):\n",
    "            for doc_id_, row in chunk.iterrows():\n",
    "                words = preprocesamiento(row.iloc[self.position])\n",
    "                if doc_id_ >= limit:\n",
    "                    break\n",
    "                for text in words:\n",
    "                    doc_id = doc_id_\n",
    "                    token = text\n",
    "                    self.doc_ids.add(doc_id)\n",
    "                    if token not in dictionary:\n",
    "                        dictionary[token] = {}  \n",
    "\n",
    "                    if doc_id not in dictionary[token]:\n",
    "                        dictionary[token][doc_id] = 1  \n",
    "                    else:\n",
    "                        dictionary[token][doc_id] += 1  \n",
    "\n",
    "                    dictionary_size = sys.getsizeof(dictionary)\n",
    "                    if dictionary_size >= self.disk_limit:\n",
    "                        self.write_block_to_disk(dictionary, level=0)\n",
    "                        dictionary.clear()\n",
    "\n",
    "                if dictionary:\n",
    "                    self.write_block_to_disk(dictionary, level=0)  \n",
    "                     \n",
    "            if doc_id_ >= limit:\n",
    "                break\n",
    "\n",
    "                         \n",
    "        self.load_index()\n",
    "        self.calculate_idf()\n",
    "        \n",
    "    def write_block_to_disk(self, dictionary, level):\n",
    "        sorted_terms = dict(sorted(dictionary.items())) \n",
    "        file_path = os.path.join(self.index_dir, f\"block_{self.block_counter}.pkl\")\n",
    "        with open(file_path, \"wb\") as f:\n",
    "            pickle.dump(sorted_terms, f)\n",
    "        \n",
    "        self.block_counter += 1\n",
    "\n",
    "    def load_block(self, filepath):\n",
    "        with open(filepath, \"rb\") as f:\n",
    "            return pickle.load(f)\n",
    "\n",
    "    \n",
    "    def retrieval(self, query, k):\n",
    "        self.load_index() \n",
    "        N = len(self.doc_ids)  \n",
    "        scores = [0] * N  \n",
    "        tf_query = {}  \n",
    "        terms = preprocesamiento(query)  \n",
    "\n",
    "        # Calcular el TF-IDF del query\n",
    "        for term in terms:\n",
    "            if term in tf_query:\n",
    "                tf_query[term] += 1\n",
    "            else:\n",
    "                tf_query[term] = 1\n",
    "\n",
    "        tfidf_query = {}\n",
    "        for term, tf in tf_query.items():\n",
    "            if term in self.idf:\n",
    "                tfidf_query[term] = math.log10(1 + tf) * self.idf[term]\n",
    "        norm_query = math.sqrt(sum(w_tq**2 for w_tq in tfidf_query.values()))  \n",
    "\n",
    "        # similitud de coseno\n",
    "        for term, w_tq in tfidf_query.items():\n",
    "            if term in self.index:\n",
    "                for doc, tf_td in self.index[term]:\n",
    "                    w_td =tf_td* self.idf[term]\n",
    "                    # print(\"self.index\", term, \"es\", self.index[term])\n",
    "                    scores[doc] += w_td * w_tq #Producto punto\n",
    "\n",
    "        # Normalizar las puntuaciones de los documentos\n",
    "        for d in range(N):\n",
    "            if self.length.get(d, 0) != 0:\n",
    "                scores[d] /= (self.length.get(d, 1) * norm_query)  # Normalización documento y consulta\n",
    "\n",
    "        # Ordenar las puntuaciones en orden descendente\n",
    "        result = sorted(enumerate(scores), key=lambda x: x[1], reverse=True)\n",
    "\n",
    "        # Devolver los k documentos más relevantes (top-k)\n",
    "        return result[:k]\n",
    "    \n",
    "    def calculate_idf(self):\n",
    "        for term, postings in self.index.items():\n",
    "            # Número de documentos que contienen el término\n",
    "            doc_freq = len(postings)\n",
    "            # Calcular IDF y almacenar\n",
    "            self.idf[term] = math.log10(len(self.doc_ids) / (1 + doc_freq))\n",
    "        # Calcular la longitud de cada documento\n",
    "        for term, postings in self.index.items():\n",
    "            for doc_id, tf in postings:\n",
    "                if doc_id not in self.length:\n",
    "                    self.length[doc_id] = 0\n",
    "                # Sumar los TF-IDF al cuadrado para la longitud del documento\n",
    "                self.length[doc_id] += (tf * self.idf[term]) ** 2\n",
    "    \n",
    "        # Tomar la raíz cuadrada para completar la longitud de cada documento\n",
    "        for doc_id in self.length:\n",
    "            self.length[doc_id] = math.sqrt(self.length[doc_id])\n",
    "\n",
    "    def load_index(self):\n",
    "        self.index = {}\n",
    "        files = sorted(os.listdir(self.index_dir))\n",
    "        for file in files:\n",
    "            if file.endswith(\".pkl\"):\n",
    "                block = self.load_block(os.path.join(self.index_dir, file))\n",
    "                # print(\"block is: \", block)\n",
    "                for term, postings in block.items():\n",
    "                    if term not in self.index:\n",
    "                        self.index[term] = []\n",
    "                    for doc_id, tf in postings.items():\n",
    "                        \n",
    "                        self.index[term].append((doc_id, math.log10(1+ tf)))\n",
    " \n",
    "    \n",
    "\n",
    "    def make_tf_idf_length(self): #Despuesd e convertir todo a diccionarios \n",
    "        N = len(self.doc_ids)  \n",
    "        files = sorted(os.listdir(self.index_dir))\n",
    "\n",
    "        #Creación de los files: \n",
    "        if not os.path.exists(self.tf_file_dir):\n",
    "            with dbm.open(self.tf_file_dir, \"c\") as tf_file:\n",
    "                print(\"Archivo de índice creado:\", self.index_dir)\n",
    "\n",
    "        if not os.path.exists(self.idf_file_dir):\n",
    "            with dbm.open(self.idf_file_dir, \"c\") as idf_file:\n",
    "                print(\"Archivo de IDF creado:\", self.idf_file_dir)\n",
    "\n",
    "        if not os.path.exists(self.length_file_dir):\n",
    "            with dbm.open(self.length_file_dir, \"c\") as length_file:\n",
    "                print(\"Archivo de longitud creado:\", self.length_file_dir)\n",
    "\n",
    "        #Iteramos: \n",
    "        tf_dict = {}\n",
    "        idf_dict = {}\n",
    "        length_dict = {}\n",
    "        with dbm.open(self.length_file_dir, \"c\") as length_db: \n",
    "            with dbm.open(self.idf_file_dir, \"c\") as idf_db: \n",
    "                with dbm.open(self.tf_file_dir, \"c\") as tf_db: \n",
    "                    for file in files:\n",
    "                        if file.endswith(\".pkl\"):\n",
    "                            block = self.load_block(os.path.join(self.index_dir, file)) #Abro el bloqque\n",
    "                            for term, postings in block.items():\n",
    "                                doc_freq = len(postings) # Calculate N docs\n",
    "                                idf_dict[term] = math.log10(len(self.doc_ids) / (1 + doc_freq))\n",
    "\n",
    "                                if term not in tf_dict:\n",
    "                                    tf_dict[term] = []\n",
    "                                for doc_id, tf in postings.items(): # Guardo el tf\n",
    "                                    tf_dict[term].append((doc_id, math.log10(1+ tf)))\n",
    "\n",
    "\n",
    "                                 #Guardar en tf_file\n",
    "                                tf_db[term.encode()] = pickle.dumps(tf_dict[term])\n",
    "                                tf_dict.clear() \n",
    "\n",
    "                                 #Guardar en idf_file\n",
    "                                idf_db[term.encode()] = pickle.dumps(idf_db[term])\n",
    "                                idf_db.clear()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def OrdenarPorBloques(dir_blocks, n_blocks):\n",
    "    for i in range(n_blocks):\n",
    "        file_path = os.path.join(dir_blocks, 'block_{}.pkl'.format(i))\n",
    "        with open(file_path, 'rb') as f:\n",
    "            tuplas_ordenadas = pickle.load(f)\n",
    "        tuplas_ordenadas = sorted(list(tuplas_ordenadas.items()), key=lambda x: x[0])\n",
    "        # tuplas_ordenadas = [par for par in tuplas_ordenadas if not par[0].isdigit()]\n",
    "        tuplas_ordenadas = [(term[0], list(term[1].items())) for term in tuplas_ordenadas]\n",
    "        with open(file_path, 'wb') as f:\n",
    "            pickle.dump(tuplas_ordenadas, f)\n",
    "\n",
    "def mergeSortAux(dir_bloques, l, r):\n",
    "    if l == r:\n",
    "        bloque = leer_bloque(dir_bloques, l)\n",
    "        unicos = set()    \n",
    "        for par in bloque:\n",
    "            unicos.add(par[0])\n",
    "        return list(unicos)\n",
    "    \n",
    "    if (l < r):\n",
    "        mid = int(math.ceil((r + l)/2.0))\n",
    "        unique_l = mergeSortAux(dir_bloques, l, mid - 1)\n",
    "        unique_r = mergeSortAux(dir_bloques, mid, r)\n",
    "        unicos = set()    \n",
    "        for term in unique_l:\n",
    "            unicos.add(term)\n",
    "        for term in unique_r:\n",
    "            unicos.add(term)\n",
    "        unicos = list(unicos)\n",
    "        merge_v2(dir_bloques, l, r, mid, len(unicos))\n",
    "        return list(unicos)\n",
    "        \n",
    "    return []\n",
    "\n",
    "def escribir_bloque(dir_bloques, block, idx_insert_block, buffer_limit = 2000):\n",
    "    with open(os.path.join(dir_bloques, \"block_{}_v2.pkl\".format(idx_insert_block)), 'wb') as f:\n",
    "        pickle.dump(block, f)    \n",
    "def leer_bloque(dir_bloques, it):\n",
    "    file_path = os.path.join(dir_bloques, f\"block_{it}.pkl\")\n",
    "    with open(file_path, \"rb\") as f:\n",
    "        buffer = pickle.load(f)\n",
    "    return buffer\n",
    "def merge_v2(dir_bloques, l, r, mid, num_terms):\n",
    "    idx_insert_block = l\n",
    "    new_block = []\n",
    "    mezclar_n_bloques = r - l + 1\n",
    "    unique_terms_per_block = int(math.ceil(num_terms/mezclar_n_bloques))\n",
    "    unique_terms_current_block = 0\n",
    "\n",
    "    it_l = l\n",
    "    it_r = mid\n",
    "    term_dic_l = leer_bloque(dir_bloques, it_l)\n",
    "    term_dic_r = leer_bloque(dir_bloques, it_r)\n",
    "    \n",
    "    idx_term_l = 0\n",
    "    idx_term_r = 0\n",
    "\n",
    "    idx_doc_l = 0\n",
    "    idx_doc_r = 0\n",
    "    new_block = []\n",
    "    while(it_l < mid and it_r < r + 1):\n",
    "        while(idx_term_l < len(term_dic_l) and idx_term_r < len(term_dic_r)): # moverme entre palabras de dos bloques\n",
    "            new_term = []\n",
    "            if(term_dic_l[idx_term_l][0] < term_dic_r[idx_term_r][0]):\n",
    "                new_term = term_dic_l[idx_term_l]\n",
    "                idx_term_l += 1\n",
    "            elif(term_dic_l[idx_term_l][0] > term_dic_r[idx_term_r][0]):\n",
    "                new_term = term_dic_r[idx_term_r]\n",
    "                idx_term_r += 1\n",
    "            else:\n",
    "                idx_doc_l = 0\n",
    "                idx_doc_r = 0\n",
    "                while(idx_doc_l < len(term_dic_l[idx_term_l][1]) and idx_doc_r < len(term_dic_r[idx_term_r][1])):\n",
    "                    if term_dic_l[idx_term_l][1][idx_doc_l][0] > term_dic_r[idx_term_r][1][idx_doc_r][0]:\n",
    "                        pushear_doc = term_dic_r[idx_term_r][1][idx_doc_r]\n",
    "                        idx_doc_r += 1\n",
    "                    elif term_dic_l[idx_term_l][1][idx_doc_l][0] < term_dic_r[idx_term_r][1][idx_doc_r][0]:\n",
    "                        pushear_doc = term_dic_l[idx_term_l][1][idx_doc_l]\n",
    "                        idx_doc_l += 1\n",
    "                    else:\n",
    "                        pushear_doc = (term_dic_l[idx_term_l][1][idx_doc_l][0], term_dic_l[idx_term_l][1][idx_doc_l][1] + term_dic_r[idx_term_r][1][idx_doc_r][1])\n",
    "                        idx_doc_l += 1\n",
    "                        idx_doc_r += 1\n",
    "                    new_term.append(pushear_doc)\n",
    "                while(idx_doc_l < len(term_dic_l[idx_term_l][1])):\n",
    "                    pushear_doc = term_dic_l[idx_term_l][1][idx_doc_l]\n",
    "                    idx_doc_l += 1\n",
    "                    new_term.append(pushear_doc)\n",
    "                while(idx_doc_r < len(term_dic_r[idx_term_r][1])):\n",
    "                    pushear_doc = term_dic_r[idx_term_r][1][idx_doc_r]\n",
    "                    idx_doc_r += 1\n",
    "                    new_term.append(pushear_doc)\n",
    "                new_term = (term_dic_l[idx_term_l][0], new_term)\n",
    "                idx_term_r += 1\n",
    "                idx_term_l += 1\n",
    "            new_block.append(new_term)\n",
    "            \n",
    "            unique_terms_current_block += 1\n",
    "            if (unique_terms_current_block == unique_terms_per_block):\n",
    "                escribir_bloque(dir_bloques, new_block, idx_insert_block)\n",
    "                unique_terms_current_block = 0\n",
    "                idx_insert_block += 1\n",
    "                new_block = []\n",
    "        if(len(term_dic_l) == idx_term_l):\n",
    "            if (it_l < mid - 1):\n",
    "                it_l += 1\n",
    "                term_dic_l = leer_bloque(dir_bloques, it_l)\n",
    "                idx_term_l = 0\n",
    "                idx_doc_l = 0\n",
    "                continue\n",
    "            else:\n",
    "                break\n",
    "        if(len(term_dic_r) == idx_term_r):\n",
    "            if (it_r < r):\n",
    "                it_r += 1\n",
    "                term_dic_r = leer_bloque(dir_bloques, it_r)\n",
    "                idx_term_r = 0\n",
    "                idx_doc_r = 0\n",
    "                continue\n",
    "            else:\n",
    "                break\n",
    "        if(it_l == mid | it_r == r + 1):\n",
    "            break\n",
    "    while(it_l < mid):\n",
    "        term_dic_l = leer_bloque(dir_bloques, it_l)\n",
    "        while(idx_term_l < len(term_dic_l)):\n",
    "            new_block.append(term_dic_l[idx_term_l])\n",
    "            unique_terms_current_block += 1\n",
    "            if (unique_terms_current_block == unique_terms_per_block):\n",
    "                escribir_bloque(dir_bloques, new_block, idx_insert_block)\n",
    "                unique_terms_current_block = 0\n",
    "                idx_insert_block += 1\n",
    "                new_block = []\n",
    "            idx_term_l += 1\n",
    "        idx_term_l = 0\n",
    "        it_l += 1\n",
    "    while(it_r < r + 1):\n",
    "        term_dic_r = leer_bloque(dir_bloques, it_r)\n",
    "        while(idx_term_r < len(term_dic_r)):\n",
    "            new_block.append(term_dic_r[idx_term_r])\n",
    "            unique_terms_current_block += 1\n",
    "            if (unique_terms_current_block == unique_terms_per_block):\n",
    "                escribir_bloque(dir_bloques, new_block, idx_insert_block)\n",
    "                unique_terms_current_block = 0\n",
    "                idx_insert_block += 1\n",
    "                new_block = []\n",
    "            idx_term_r += 1\n",
    "        idx_term_r = 0\n",
    "        it_r += 1\n",
    "\n",
    "    while(idx_insert_block < r + 1):\n",
    "        if len(new_block) > 0:\n",
    "            escribir_bloque(dir_bloques, new_block, idx_insert_block)\n",
    "        else:\n",
    "            escribir_bloque(dir_bloques, [], idx_insert_block)\n",
    "        new_block = []\n",
    "        idx_insert_block += 1\n",
    "    idx_insert_block = l\n",
    "    for idx_archivo in range(l, r + 1):\n",
    "        nuevo_nombre = os.path.join(dir_bloques, \"block_{}.pkl\".format(idx_archivo))\n",
    "        if os.path.exists(nuevo_nombre):\n",
    "            os.remove(nuevo_nombre)\n",
    "        os.rename(os.path.join(dir_bloques, \"block_{}_v2.pkl\".format(idx_archivo)), nuevo_nombre)\n",
    "def mergeSort(dir_bloques):\n",
    "    bloques_files_dir = os.listdir(os.path.join('./',dir_bloques))\n",
    "    n = len(bloques_files_dir)\n",
    "    mergeSortAux(dir_bloques, 0, n - 1)\n",
    "'''\n",
    "Recibe la direccion del folder con los diccionarios del spimi,\n",
    "modifica los archivos y los sobreescribe para crear el índice\n",
    "invertido con un índice global\n",
    "'''\n",
    "def InvertirListasDiccionarios(dir_bloques):\n",
    "    n_bloques = len(os.listdir(os.path.join('./',dir_bloques)))\n",
    "    for i in range(n_bloques):\n",
    "        bloque_path = os.path.join(dir_bloques,\"block_{}.pkl\".format(i))\n",
    "        with open(bloque_path, 'rb') as f:\n",
    "            bloque = pickle.load(f)\n",
    "        bloque_dict = {}\n",
    "        if len(bloque) != 0:\n",
    "            for term, poosting_list in bloque:\n",
    "                poosting_dict = {}\n",
    "                for doc, tf in poosting_list:\n",
    "                    poosting_dict[doc] = tf\n",
    "                bloque_dict[term] = poosting_dict\n",
    "        with open(bloque_path, 'wb') as f:\n",
    "            pickle.dump(bloque_dict, f)\n",
    "\n",
    "def getNumberWithAtributo(dataset_head, atributo):\n",
    "    if atributo in dataset_head.columns:\n",
    "        return dataset_head.columns.get_loc(atributo)  # Obtiene la posición del atributo\n",
    "    return -1  # Retorna -1 si no existe\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Ce mar\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Path de songs:  C:\\Users\\Ce mar\\.cache\\kagglehub\\datasets\\imuhammad\\audio-features-and-lyrics-of-spotify-songs\\versions\\1\\spotify_songs.csv\n",
      "Archivo de índice creado: index_blocks\n",
      "Archivo de IDF creado: index_blocks_idf\n",
      "Archivo de longitud creado: index_blocks_lenght\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "b'6'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[9], line 25\u001b[0m\n\u001b[0;32m     22\u001b[0m InvertirListasDiccionarios(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mindex_blocks\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     24\u001b[0m \u001b[38;5;66;03m# ---------End Creation---------\u001b[39;00m\n\u001b[1;32m---> 25\u001b[0m \u001b[43ms\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmake_tf_idf_length\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     27\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mverificar_termino\u001b[39m(tf_file_dir, term):\n\u001b[0;32m     28\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m dbm\u001b[38;5;241m.\u001b[39mopen(tf_file_dir, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m tf_db:  \n",
      "Cell \u001b[1;32mIn[7], line 189\u001b[0m, in \u001b[0;36mSPIMI.make_tf_idf_length\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    186\u001b[0m tf_dict\u001b[38;5;241m.\u001b[39mclear() \n\u001b[0;32m    188\u001b[0m  \u001b[38;5;66;03m#Guardar en idf_file\u001b[39;00m\n\u001b[1;32m--> 189\u001b[0m idf_db[term\u001b[38;5;241m.\u001b[39mencode()] \u001b[38;5;241m=\u001b[39m pickle\u001b[38;5;241m.\u001b[39mdumps(\u001b[43midf_db\u001b[49m\u001b[43m[\u001b[49m\u001b[43mterm\u001b[49m\u001b[43m]\u001b[49m)\n\u001b[0;32m    190\u001b[0m idf_db\u001b[38;5;241m.\u001b[39mclear()\n",
      "File \u001b[1;32mc:\\Users\\Ce mar\\AppData\\Local\\Programs\\Python\\Python310\\lib\\dbm\\dumb.py:147\u001b[0m, in \u001b[0;36m_Database.__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m    145\u001b[0m     key \u001b[38;5;241m=\u001b[39m key\u001b[38;5;241m.\u001b[39mencode(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m    146\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_verify_open()\n\u001b[1;32m--> 147\u001b[0m pos, siz \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_index\u001b[49m\u001b[43m[\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m]\u001b[49m     \u001b[38;5;66;03m# may raise KeyError\u001b[39;00m\n\u001b[0;32m    148\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m _io\u001b[38;5;241m.\u001b[39mopen(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_datfile, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrb\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[0;32m    149\u001b[0m     f\u001b[38;5;241m.\u001b[39mseek(pos)\n",
      "\u001b[1;31mKeyError\u001b[0m: b'6'"
     ]
    }
   ],
   "source": [
    "# --------Set Path----------\n",
    "import kagglehub\n",
    "dowload = kagglehub.dataset_download(\"imuhammad/audio-features-and-lyrics-of-spotify-songs\")\n",
    "\n",
    "# ----------Set dataset--------\n",
    "\n",
    "lista_ = os.listdir(dowload)\n",
    "path = os.path.join(dowload, lista_[0])\n",
    "\n",
    "print(\"Path de songs: \",path)\n",
    "dataset = pd.read_csv(path)\n",
    "dataset = dataset.head(20)\n",
    "\n",
    "# ----------Set Atributo--------\n",
    "columna = getNumberWithAtributo(dataset, \"lyrics\")\n",
    "\n",
    "# ---------Creation---------\n",
    "s = SPIMI(\"index_blocks\", path, columna)  \n",
    "s.spimi_invert()\n",
    "OrdenarPorBloques(s.index_dir, s.block_counter )\n",
    "mergeSort(s.index_dir)\n",
    "InvertirListasDiccionarios(\"index_blocks\")\n",
    "\n",
    "# ---------End Creation---------\n",
    "s.make_tf_idf_length()\n",
    "\n",
    "def verificar_termino(tf_file_dir, term):\n",
    "    with dbm.open(tf_file_dir, \"r\") as tf_db:  \n",
    "        try:\n",
    "            term_key = term.encode()\n",
    "            postings_list = pickle.loads(tf_db[term_key])\n",
    "            print(f\"Postings list para '{term}': {postings_list}\")\n",
    "        except KeyError:\n",
    "            print(f\"El término '{term}' no se encuentra en el archivo DBM.\")\n",
    "\n",
    "verificar_termino(s.tf_file_dir, \"6\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19}"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s.doc_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'dataset' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 8\u001b[0m\n\u001b[0;32m      5\u001b[0m         res\u001b[38;5;241m.\u001b[39mappend((dataset\u001b[38;5;241m.\u001b[39miloc[doc], score))\n\u001b[0;32m      6\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m pd\u001b[38;5;241m.\u001b[39mDataFrame(res) \n\u001b[1;32m----> 8\u001b[0m query \u001b[38;5;241m=\u001b[39m \u001b[43mdataset\u001b[49m\u001b[38;5;241m.\u001b[39miloc[\u001b[38;5;241m4\u001b[39m, \u001b[38;5;241m3\u001b[39m]\n\u001b[0;32m      9\u001b[0m top_k \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m5\u001b[39m\n\u001b[0;32m     11\u001b[0m \u001b[38;5;66;03m#-----------Start of query------------\u001b[39;00m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'dataset' is not defined"
     ]
    }
   ],
   "source": [
    "def getResultados(result, path):\n",
    "    res = []\n",
    "    dataset = pd.read_csv(path)\n",
    "    for doc, score in result:\n",
    "        res.append((dataset.iloc[doc], score))\n",
    "    return pd.DataFrame(res) \n",
    "\n",
    "query = dataset.iloc[4, 3]\n",
    "top_k = 5\n",
    "\n",
    "#-----------Start of query------------\n",
    "result = s.retrieval(query, top_k)\n",
    "getResultados(result, s.dataset_path) # La respuesta es un array pd de [fila del dataframe, score]\n",
    "#-----------End of query------------\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
