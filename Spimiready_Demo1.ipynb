{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Proyecto jiji"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: kagglehub in c:\\users\\ce mar\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (0.3.4)\n",
      "Requirement already satisfied: requests in c:\\users\\ce mar\\appdata\\roaming\\python\\python310\\site-packages (from kagglehub) (2.28.1)\n",
      "Requirement already satisfied: tqdm in c:\\users\\ce mar\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from kagglehub) (4.66.1)\n",
      "Requirement already satisfied: packaging in c:\\users\\ce mar\\appdata\\roaming\\python\\python310\\site-packages (from kagglehub) (23.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\ce mar\\appdata\\roaming\\python\\python310\\site-packages (from requests->kagglehub) (2022.12.7)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\ce mar\\appdata\\roaming\\python\\python310\\site-packages (from requests->kagglehub) (3.4)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\ce mar\\appdata\\roaming\\python\\python310\\site-packages (from requests->kagglehub) (1.26.13)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in c:\\users\\ce mar\\appdata\\roaming\\python\\python310\\site-packages (from requests->kagglehub) (2.1.1)\n",
      "Requirement already satisfied: colorama in c:\\users\\ce mar\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from tqdm->kagglehub) (0.4.6)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip available: 22.2.1 -> 24.3.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "!pip install kagglehub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to C:\\Users\\Ce\n",
      "[nltk_data]     mar\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "import numpy as np\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "nltk.download('punkt')\n",
    "import regex as re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "stemmer = SnowballStemmer('english')\n",
    "import os\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"stopwords-en.txt\", encoding=\"latin1\") as file:\n",
    "   stoplist = [line.rstrip().lower() for line in file]\n",
    "stoplist += ['?', '-', '.', ':', ',', '!', ';']\n",
    "\n",
    "def preprocesamiento(texto, stemming=True):\n",
    "  words = []\n",
    "  texto = str(texto)\n",
    "  texto = texto.lower()\n",
    "  texto = re.sub(r'[^a-zA-Z0-9_À-ÿ]', ' ', texto)\n",
    "  # tokenizar\n",
    "  words = nltk.word_tokenize(texto, language='spanish')\n",
    "  # filtrar stopwords\n",
    "  words = [word for word in words if word not in stoplist]\n",
    "  # reducir palabras (stemming)\n",
    "  if stemming:\n",
    "      words = [stemmer.stem(word) for word in words]\n",
    "  return words\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "import sys\n",
    "import math\n",
    "import heapq\n",
    "\n",
    "class SPIMI:\n",
    "    def __init__(self, index_dir=\"index_blocks\", index_dataset=\"path\" , position = 3):\n",
    "        self.index_dir = index_dir  \n",
    "        self.path_ = index_dataset  \n",
    "        self.block_counter = 0      \n",
    "        self.doc_ids = set()         \n",
    "        self.idf = {}\n",
    "        self.length = {}\n",
    "        self.disk_limit = 4000  \n",
    "        self.position = position\n",
    "\n",
    "        if not os.path.exists(self.index_dir):\n",
    "            os.makedirs(self.index_dir)\n",
    "\n",
    "    def spimi_invert(self, token_stream):\n",
    "        dictionary = {}\n",
    "\n",
    "        for doc_id_, row in dataset.iterrows():\n",
    "            words = preprocesamiento(row.iloc[self.position])\n",
    "            for text in words:\n",
    "                doc_id = doc_id_\n",
    "                token = text\n",
    "                self.doc_ids.add(doc_id)\n",
    "                if token not in dictionary:\n",
    "                    dictionary[token] = {}  \n",
    "\n",
    "                if doc_id not in dictionary[token]:\n",
    "                    dictionary[token][doc_id] = 1  \n",
    "                else:\n",
    "                    dictionary[token][doc_id] += 1  \n",
    "\n",
    "                dictionary_size = sys.getsizeof(dictionary)\n",
    "                if dictionary_size >= self.disk_limit:\n",
    "                    self.write_block_to_disk(dictionary, level=0)\n",
    "                    dictionary.clear()\n",
    "                \n",
    "            if dictionary:\n",
    "                self.write_block_to_disk(dictionary, level=0)\n",
    "        self.load_index()\n",
    "        self.calculate_idf()\n",
    "        \n",
    "    def write_block_to_disk(self, dictionary, level):\n",
    "        sorted_terms = dict(sorted(dictionary.items())) \n",
    "        # block_{it}\n",
    "        file_path = os.path.join(self.index_dir, f\"block_{self.block_counter}.pkl\")\n",
    "        with open(file_path, \"wb\") as f:\n",
    "            pickle.dump(sorted_terms, f)\n",
    "        \n",
    "        self.block_counter += 1\n",
    "\n",
    "    def load_block(self, filepath):\n",
    "        with open(filepath, \"rb\") as f:\n",
    "            return pickle.load(f)\n",
    "\n",
    "    \n",
    "    def retrieval(self, query, k):\n",
    "        self.load_index() \n",
    "        N = len(self.doc_ids)  \n",
    "        scores = [0] * N  \n",
    "        tf_query = {}  \n",
    "        terms = preprocesamiento(query)  \n",
    "\n",
    "        # Calcular el TF-IDF del query\n",
    "        for term in terms:\n",
    "            if term in tf_query:\n",
    "                tf_query[term] += 1\n",
    "            else:\n",
    "                tf_query[term] = 1\n",
    "\n",
    "        tfidf_query = {}\n",
    "        for term, tf in tf_query.items():\n",
    "            if term in self.idf:\n",
    "                tfidf_query[term] = math.log10(1 + tf) * self.idf[term]\n",
    "        norm_query = math.sqrt(sum(w_tq**2 for w_tq in tfidf_query.values()))  # Normalización del query\n",
    "\n",
    "        # Aplicar similitud de coseno: Calculamos el puntaje para cada documento\n",
    "        for term, w_tq in tfidf_query.items():\n",
    "            if term in self.index:\n",
    "                for doc, tf_td in self.index[term]:\n",
    "                    w_td =tf_td* self.idf[term]\n",
    "                    # print(\"self.index\", term, \"es\", self.index[term])\n",
    "\n",
    "                    scores[doc] += w_td * w_tq #Producto punto\n",
    "\n",
    "        # Normalizar las puntuaciones de los documentos\n",
    "        for d in range(N):\n",
    "            if self.length.get(d, 0) != 0:\n",
    "                scores[d] /= (self.length.get(d, 1) * norm_query)  # Normalización documento y consulta\n",
    "\n",
    "        # Ordenar las puntuaciones en orden descendente\n",
    "        result = sorted(enumerate(scores), key=lambda x: x[1], reverse=True)\n",
    "\n",
    "        # Devolver los k documentos más relevantes (top-k)\n",
    "        return result[:k]\n",
    "    \n",
    "    def calculate_idf(self):\n",
    "        for term, postings in self.index.items():\n",
    "            # Número de documentos que contienen el término\n",
    "            doc_freq = len(postings)\n",
    "            # Calcular IDF y almacenar\n",
    "            self.idf[term] = math.log10(len(self.doc_ids) / (1 + doc_freq))\n",
    "        # Calcular la longitud de cada documento\n",
    "        for term, postings in self.index.items():\n",
    "            for doc_id, tf in postings:\n",
    "                if doc_id not in self.length:\n",
    "                    self.length[doc_id] = 0\n",
    "                # Sumar los TF-IDF al cuadrado para la longitud del documento\n",
    "                self.length[doc_id] += (tf * self.idf[term]) ** 2\n",
    "    \n",
    "        # Tomar la raíz cuadrada para completar la longitud de cada documento\n",
    "        for doc_id in self.length:\n",
    "            self.length[doc_id] = math.sqrt(self.length[doc_id])\n",
    "\n",
    "    def load_index(self):\n",
    "        self.index = {}\n",
    "        files = sorted(os.listdir(self.index_dir))\n",
    "        for file in files:\n",
    "            if file.endswith(\".pkl\"):\n",
    "                block = self.load_block(os.path.join(self.index_dir, file))\n",
    "                for term, postings in block.items():\n",
    "                    if term not in self.index:\n",
    "                        self.index[term] = []\n",
    "                    for doc_id, tf in postings.items():\n",
    "                        \n",
    "                        self.index[term].append((doc_id, math.log10(1+ tf)))\n",
    " \n",
    "    def load_block(self, filepath):\n",
    "        \"\"\" Cargar un bloque del índice desde un archivo. \"\"\"\n",
    "        with open(filepath, \"rb\") as f:\n",
    "            return pickle.load(f)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def OrdenarPorBloques(dir_blocks, n_blocks):\n",
    "    for i in range(n_blocks):\n",
    "        file_path = os.path.join(dir_blocks, 'block_{}.pkl'.format(i))\n",
    "        with open(file_path, 'rb') as f:\n",
    "            tuplas_ordenadas = pickle.load(f)\n",
    "        tuplas_ordenadas = sorted(list(tuplas_ordenadas.items()), key=lambda x: x[0])\n",
    "        # tuplas_ordenadas = [par for par in tuplas_ordenadas if not par[0].isdigit()]\n",
    "        tuplas_ordenadas = [(term[0], list(term[1].items())) for term in tuplas_ordenadas]\n",
    "        with open(file_path, 'wb') as f:\n",
    "            pickle.dump(tuplas_ordenadas, f)\n",
    "\n",
    "def mergeSortAux(dir_bloques, l, r):\n",
    "    if l == r:\n",
    "        bloque = leer_bloque(dir_bloques, l)\n",
    "        unicos = set()    \n",
    "        for par in bloque:\n",
    "            unicos.add(par[0])\n",
    "        return list(unicos)\n",
    "    \n",
    "    if (l < r):\n",
    "        mid = int(math.ceil((r + l)/2.0))\n",
    "        unique_l = mergeSortAux(dir_bloques, l, mid - 1)\n",
    "        unique_r = mergeSortAux(dir_bloques, mid, r)\n",
    "        unicos = set()    \n",
    "        for term in unique_l:\n",
    "            unicos.add(term)\n",
    "        for term in unique_r:\n",
    "            unicos.add(term)\n",
    "        unicos = list(unicos)\n",
    "        merge_v2(dir_bloques, l, r, mid, len(unicos))\n",
    "        return list(unicos)\n",
    "        \n",
    "    return []\n",
    "\n",
    "def escribir_bloque(dir_bloques, block, idx_insert_block, buffer_limit = 2000):\n",
    "    with open(os.path.join(dir_bloques, \"block_{}_v2.pkl\".format(idx_insert_block)), 'wb') as f:\n",
    "        pickle.dump(block, f)    \n",
    "def leer_bloque(dir_bloques, it):\n",
    "    file_path = os.path.join(dir_bloques, f\"block_{it}.pkl\")\n",
    "    with open(file_path, \"rb\") as f:\n",
    "        buffer = pickle.load(f)\n",
    "    return buffer\n",
    "def merge_v2(dir_bloques, l, r, mid, num_terms):\n",
    "    idx_insert_block = l\n",
    "    new_block = []\n",
    "    mezclar_n_bloques = r - l + 1\n",
    "    unique_terms_per_block = int(math.ceil(num_terms/mezclar_n_bloques))\n",
    "    unique_terms_current_block = 0\n",
    "\n",
    "    it_l = l\n",
    "    it_r = mid\n",
    "    term_dic_l = leer_bloque(dir_bloques, it_l)\n",
    "    term_dic_r = leer_bloque(dir_bloques, it_r)\n",
    "    \n",
    "    idx_term_l = 0\n",
    "    idx_term_r = 0\n",
    "\n",
    "    idx_doc_l = 0\n",
    "    idx_doc_r = 0\n",
    "    new_block = []\n",
    "    while(it_l < mid and it_r < r + 1):\n",
    "        # print(f\"Toma 2 bloques {it_l} y {it_r} | idx_term_l: \", idx_term_l, \"| len(term_dic_l)\", len(term_dic_l), \"| idx_term_r: \", idx_term_r, \"| len(term_dic_r)\", len(term_dic_r))\n",
    "        while(idx_term_l < len(term_dic_l) and idx_term_r < len(term_dic_r)): # moverme entre palabras de dos bloques\n",
    "            # print(\"Current term_dic_l\", term_dic_l)\n",
    "            # print(\"Current term_dic_r\", term_dic_r)\n",
    "            # print(f\"Toma 2 terminos {term_dic_l[idx_term_l][0]} y {term_dic_r[idx_term_r][0]}\")\n",
    "            new_term = []\n",
    "            if(term_dic_l[idx_term_l][0] < term_dic_r[idx_term_r][0]):\n",
    "                new_term = term_dic_l[idx_term_l]\n",
    "                idx_term_l += 1\n",
    "            elif(term_dic_l[idx_term_l][0] > term_dic_r[idx_term_r][0]):\n",
    "                new_term = term_dic_r[idx_term_r]\n",
    "                idx_term_r += 1\n",
    "            else:\n",
    "                idx_doc_l = 0\n",
    "                idx_doc_r = 0\n",
    "                while(idx_doc_l < len(term_dic_l[idx_term_l][1]) and idx_doc_r < len(term_dic_r[idx_term_r][1])):\n",
    "                    # print(f\"Toma 2 terminos iguales con tf = {term_dic_l[idx_term_l][1]} y {term_dic_r[idx_term_r][1]}\")\n",
    "                    if term_dic_l[idx_term_l][1][idx_doc_l][0] > term_dic_r[idx_term_r][1][idx_doc_r][0]:\n",
    "                        pushear_doc = term_dic_r[idx_term_r][1][idx_doc_r]\n",
    "                        idx_doc_r += 1\n",
    "                    elif term_dic_l[idx_term_l][1][idx_doc_l][0] < term_dic_r[idx_term_r][1][idx_doc_r][0]:\n",
    "                        pushear_doc = term_dic_l[idx_term_l][1][idx_doc_l]\n",
    "                        idx_doc_l += 1\n",
    "                    else:\n",
    "                        pushear_doc = (term_dic_l[idx_term_l][1][idx_doc_l][0], term_dic_l[idx_term_l][1][idx_doc_l][1] + term_dic_r[idx_term_r][1][idx_doc_r][1])\n",
    "                        idx_doc_l += 1\n",
    "                        idx_doc_r += 1\n",
    "                    new_term.append(pushear_doc)\n",
    "                while(idx_doc_l < len(term_dic_l[idx_term_l][1])):\n",
    "                    pushear_doc = term_dic_l[idx_term_l][1][idx_doc_l]\n",
    "                    idx_doc_l += 1\n",
    "                    new_term.append(pushear_doc)\n",
    "                while(idx_doc_r < len(term_dic_r[idx_term_r][1])):\n",
    "                    pushear_doc = term_dic_r[idx_term_r][1][idx_doc_r]\n",
    "                    idx_doc_r += 1\n",
    "                    new_term.append(pushear_doc)\n",
    "                new_term = (term_dic_l[idx_term_l][0], new_term)\n",
    "                idx_term_r += 1\n",
    "                idx_term_l += 1\n",
    "            new_block.append(new_term)\n",
    "            \n",
    "            unique_terms_current_block += 1\n",
    "            if (unique_terms_current_block == unique_terms_per_block):\n",
    "                escribir_bloque(dir_bloques, new_block, idx_insert_block)\n",
    "                unique_terms_current_block = 0\n",
    "                idx_insert_block += 1\n",
    "                new_block = []\n",
    "        if(len(term_dic_l) == idx_term_l):\n",
    "            if (it_l < mid - 1):\n",
    "                it_l += 1\n",
    "                term_dic_l = leer_bloque(dir_bloques, it_l)\n",
    "                idx_term_l = 0\n",
    "                idx_doc_l = 0\n",
    "                continue\n",
    "            else:\n",
    "                break\n",
    "        if(len(term_dic_r) == idx_term_r):\n",
    "            if (it_r < r):\n",
    "                it_r += 1\n",
    "                term_dic_r = leer_bloque(dir_bloques, it_r)\n",
    "                idx_term_r = 0\n",
    "                idx_doc_r = 0\n",
    "                continue\n",
    "            else:\n",
    "                break\n",
    "        if(it_l == mid | it_r == r + 1):\n",
    "            break\n",
    "    while(it_l < mid):\n",
    "        term_dic_l = leer_bloque(dir_bloques, it_l)\n",
    "        while(idx_term_l < len(term_dic_l)):\n",
    "            new_block.append(term_dic_l[idx_term_l])\n",
    "            unique_terms_current_block += 1\n",
    "            if (unique_terms_current_block == unique_terms_per_block):\n",
    "                escribir_bloque(dir_bloques, new_block, idx_insert_block)\n",
    "                unique_terms_current_block = 0\n",
    "                idx_insert_block += 1\n",
    "                new_block = []\n",
    "            idx_term_l += 1\n",
    "        idx_term_l = 0\n",
    "        it_l += 1\n",
    "    while(it_r < r + 1):\n",
    "        term_dic_r = leer_bloque(dir_bloques, it_r)\n",
    "        while(idx_term_r < len(term_dic_r)):\n",
    "            new_block.append(term_dic_r[idx_term_r])\n",
    "            unique_terms_current_block += 1\n",
    "            if (unique_terms_current_block == unique_terms_per_block):\n",
    "                escribir_bloque(dir_bloques, new_block, idx_insert_block)\n",
    "                unique_terms_current_block = 0\n",
    "                idx_insert_block += 1\n",
    "                new_block = []\n",
    "            idx_term_r += 1\n",
    "        idx_term_r = 0\n",
    "        it_r += 1\n",
    "\n",
    "    while(idx_insert_block < r + 1):\n",
    "        if len(new_block) > 0:\n",
    "            escribir_bloque(dir_bloques, new_block, idx_insert_block)\n",
    "        else:\n",
    "            escribir_bloque(dir_bloques, [], idx_insert_block)\n",
    "        new_block = []\n",
    "        idx_insert_block += 1\n",
    "    idx_insert_block = l\n",
    "    for idx_archivo in range(l, r + 1):\n",
    "        nuevo_nombre = os.path.join(dir_bloques, \"block_{}.pkl\".format(idx_archivo))\n",
    "        if os.path.exists(nuevo_nombre):\n",
    "            os.remove(nuevo_nombre)\n",
    "        os.rename(os.path.join(dir_bloques, \"block_{}_v2.pkl\".format(idx_archivo)), nuevo_nombre)\n",
    "def mergeSort(dir_bloques):\n",
    "    bloques_files_dir = os.listdir(os.path.join('./',dir_bloques))\n",
    "    # print(bloques_files_dir)\n",
    "    n = len(bloques_files_dir)\n",
    "    # n = int(math.exp2(math.floor(math.log2(n)) + 1))\n",
    "    mergeSortAux(dir_bloques, 0, n - 1)\n",
    "'''\n",
    "Recibe la direccion del folder con los diccionarios del spimi,\n",
    "modifica los archivos y los sobreescribe para crear el índice\n",
    "invertido con un índice global\n",
    "'''\n",
    "def InvertirListasDiccionarios(dir_bloques):\n",
    "    n_bloques = len(os.listdir(os.path.join('./',dir_bloques)))\n",
    "    for i in range(n_bloques):\n",
    "        bloque_path = os.path.join(dir_bloques,\"block_{}.pkl\".format(i))\n",
    "        with open(bloque_path, 'rb') as f:\n",
    "            bloque = pickle.load(f)\n",
    "        bloque_dict = {}\n",
    "        if len(bloque) != 0:\n",
    "            for term, poosting_list in bloque:\n",
    "                poosting_dict = {}\n",
    "                for doc, tf in poosting_list:\n",
    "                    poosting_dict[doc] = tf\n",
    "                bloque_dict[term] = poosting_dict\n",
    "        with open(bloque_path, 'wb') as f:\n",
    "            pickle.dump(bloque_dict, f)\n",
    "\n",
    "def getNumberWithAtributo(dataset_head, atributo):\n",
    "    if atributo in dataset_head.columns:\n",
    "        return dataset_head.columns.get_loc(atributo)  # Obtiene la posición del atributo\n",
    "    return -1  # Retorna -1 si no existe\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Ce mar\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Path de songs:  C:\\Users\\Ce mar\\.cache\\kagglehub\\datasets\\imuhammad\\audio-features-and-lyrics-of-spotify-songs\\versions\\1\\spotify_songs.csv\n"
     ]
    }
   ],
   "source": [
    "# --------Set Path----------\n",
    "import kagglehub\n",
    "path = kagglehub.dataset_download(\"imuhammad/audio-features-and-lyrics-of-spotify-songs\")\n",
    "\n",
    "# ----------Set dataset--------\n",
    "\n",
    "lista_ = os.listdir(path)\n",
    "songs = os.path.join(path, lista_[0])\n",
    "print(\"Path de songs: \",songs)\n",
    "dataset = pd.read_csv(songs)\n",
    "dataset = dataset.head(20)\n",
    "\n",
    "# ----------Set Atributo--------\n",
    "columna = getNumberWithAtributo(dataset, \"lyrics\")\n",
    "\n",
    "# ---------Creation---------\n",
    "s = SPIMI(\"index_blocks\", path, columna)  \n",
    "s.spimi_invert(dataset)\n",
    "OrdenarPorBloques(s.index_dir, s.block_counter )\n",
    "mergeSort(s.index_dir)\n",
    "InvertirListasDiccionarios(\"index_blocks\")\n",
    "\n",
    "# ---------End Creation---------\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>track_id                                      ...</td>\n",
       "      <td>1.012346</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>track_id                                      ...</td>\n",
       "      <td>0.081857</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>track_id                                      ...</td>\n",
       "      <td>0.056653</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>track_id                                      ...</td>\n",
       "      <td>0.028093</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>track_id                                      ...</td>\n",
       "      <td>0.011535</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   0         1\n",
       "0  track_id                                      ...  1.012346\n",
       "1  track_id                                      ...  0.081857\n",
       "2  track_id                                      ...  0.056653\n",
       "3  track_id                                      ...  0.028093\n",
       "4  track_id                                      ...  0.011535"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def getResultados(result, path):\n",
    "    res = []\n",
    "    lista_ = os.listdir(path)\n",
    "    dataset_path_ = os.path.join(path, lista_[0])\n",
    "    dataset = pd.read_csv(dataset_path_)\n",
    "    for doc, score in result:\n",
    "        res.append((dataset.iloc[doc], score))\n",
    "    return pd.DataFrame(res)    \n",
    "\n",
    "query = dataset.iloc[19, 3]\n",
    "top_k = 5\n",
    "\n",
    "#-----------Start of query------------\n",
    "result = s.retrieval(query, top_k)\n",
    "getResultados(result, s.path_) # La respuesta es un array pd de [fila del dataframe, score]\n",
    "#-----------End of query------------\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
