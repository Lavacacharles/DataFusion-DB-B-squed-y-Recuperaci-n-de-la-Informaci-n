{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Proyecto jiji"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: kagglehub in c:\\users\\ce mar\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (0.3.4)\n",
      "Requirement already satisfied: tqdm in c:\\users\\ce mar\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from kagglehub) (4.66.1)\n",
      "Requirement already satisfied: requests in c:\\users\\ce mar\\appdata\\roaming\\python\\python310\\site-packages (from kagglehub) (2.28.1)\n",
      "Requirement already satisfied: packaging in c:\\users\\ce mar\\appdata\\roaming\\python\\python310\\site-packages (from kagglehub) (23.2)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in c:\\users\\ce mar\\appdata\\roaming\\python\\python310\\site-packages (from requests->kagglehub) (2.1.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\ce mar\\appdata\\roaming\\python\\python310\\site-packages (from requests->kagglehub) (2022.12.7)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\ce mar\\appdata\\roaming\\python\\python310\\site-packages (from requests->kagglehub) (3.4)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\ce mar\\appdata\\roaming\\python\\python310\\site-packages (from requests->kagglehub) (1.26.13)\n",
      "Requirement already satisfied: colorama in c:\\users\\ce mar\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from tqdm->kagglehub) (0.4.6)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip available: 22.2.1 -> 24.3.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "!pip install kagglehub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Ce mar\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\scipy\\__init__.py:177: UserWarning: A NumPy version >=1.18.5 and <1.26.0 is required for this version of SciPy (detected version 1.26.2\n",
      "  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n",
      "[nltk_data] Downloading package punkt to C:\\Users\\Ce\n",
      "[nltk_data]     mar\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "import numpy as np\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "nltk.download('punkt')\n",
    "import regex as re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "stemmer = SnowballStemmer('english')\n",
    "import os\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Ce mar\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Path to dataset files: C:\\Users\\Ce mar\\.cache\\kagglehub\\datasets\\imuhammad\\audio-features-and-lyrics-of-spotify-songs\\versions\\1\n"
     ]
    }
   ],
   "source": [
    "import kagglehub\n",
    "path = kagglehub.dataset_download(\"imuhammad/audio-features-and-lyrics-of-spotify-songs\")\n",
    "print(\"Path to dataset files:\", path)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"stopwords-en.txt\", encoding=\"latin1\") as file:\n",
    "    stoplist = [line.rstrip().lower() for line in file]\n",
    "\n",
    "def preprocesamiento(texto, stemming=True):\n",
    "  words = []\n",
    "  texto = texto.lower()\n",
    "  texto = re.sub(r'[^a-zA-Z0-9_À-ÿ]', ' ', texto)\n",
    "  # tokenizar\n",
    "  words = nltk.word_tokenize(texto, language='spanish')\n",
    "  # filtrar stopwords\n",
    "  words = [word for word in words if word not in stoplist]\n",
    "  # reducir palabras (stemming)\n",
    "  if stemming:\n",
    "      words = [stemmer.stem(word) for word in words]\n",
    "  return words\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Ce mar\\.cache\\kagglehub\\datasets\\imuhammad\\audio-features-and-lyrics-of-spotify-songs\\versions\\1\\spotify_songs.csv\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>15</th>\n",
       "      <th>16</th>\n",
       "      <th>17</th>\n",
       "      <th>18</th>\n",
       "      <th>19</th>\n",
       "      <th>20</th>\n",
       "      <th>21</th>\n",
       "      <th>22</th>\n",
       "      <th>23</th>\n",
       "      <th>24</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>track_id</td>\n",
       "      <td>track_name</td>\n",
       "      <td>track_artist</td>\n",
       "      <td>lyrics</td>\n",
       "      <td>track_popularity</td>\n",
       "      <td>track_album_id</td>\n",
       "      <td>track_album_name</td>\n",
       "      <td>track_album_release_date</td>\n",
       "      <td>playlist_name</td>\n",
       "      <td>playlist_id</td>\n",
       "      <td>...</td>\n",
       "      <td>loudness</td>\n",
       "      <td>mode</td>\n",
       "      <td>speechiness</td>\n",
       "      <td>acousticness</td>\n",
       "      <td>instrumentalness</td>\n",
       "      <td>liveness</td>\n",
       "      <td>valence</td>\n",
       "      <td>tempo</td>\n",
       "      <td>duration_ms</td>\n",
       "      <td>language</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0017A6SJgTbfQVU2EtsPNo</td>\n",
       "      <td>Pangarap</td>\n",
       "      <td>Barbie's Cradle</td>\n",
       "      <td>Minsan pa Nang ako'y napalingon Hindi ko alam ...</td>\n",
       "      <td>41</td>\n",
       "      <td>1srJQ0njEQgd8w4XSqI4JQ</td>\n",
       "      <td>Trip</td>\n",
       "      <td>2001-01-01</td>\n",
       "      <td>Pinoy Classic Rock</td>\n",
       "      <td>37i9dQZF1DWYDQ8wBxd7xt</td>\n",
       "      <td>...</td>\n",
       "      <td>-10.068</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0236</td>\n",
       "      <td>0.279</td>\n",
       "      <td>0.0117</td>\n",
       "      <td>0.0887</td>\n",
       "      <td>0.5660000000000001</td>\n",
       "      <td>97.091</td>\n",
       "      <td>235440</td>\n",
       "      <td>tl</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>004s3t0ONYlzxII9PLgU6z</td>\n",
       "      <td>I Feel Alive</td>\n",
       "      <td>Steady Rollin</td>\n",
       "      <td>The trees, are singing in the wind The sky blu...</td>\n",
       "      <td>28</td>\n",
       "      <td>3z04Lb9Dsilqw68SHt6jLB</td>\n",
       "      <td>Love &amp; Loss</td>\n",
       "      <td>2017-11-21</td>\n",
       "      <td>Hard Rock Workout</td>\n",
       "      <td>3YouF0u7waJnolytf9JCXf</td>\n",
       "      <td>...</td>\n",
       "      <td>-4.739</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0442</td>\n",
       "      <td>0.0117</td>\n",
       "      <td>0.00994</td>\n",
       "      <td>0.34700000000000003</td>\n",
       "      <td>0.404</td>\n",
       "      <td>135.225</td>\n",
       "      <td>373512</td>\n",
       "      <td>en</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>00chLpzhgVjxs1zKC9UScL</td>\n",
       "      <td>Poison</td>\n",
       "      <td>Bell Biv DeVoe</td>\n",
       "      <td>NA Yeah, Spyderman and Freeze in full effect U...</td>\n",
       "      <td>0</td>\n",
       "      <td>6oZ6brjB8x3GoeSYdwJdPc</td>\n",
       "      <td>Gold</td>\n",
       "      <td>2005-01-01</td>\n",
       "      <td>Back in the day - R&amp;B, New Jack Swing, Swingbe...</td>\n",
       "      <td>3a9y4eeCJRmG9p4YKfqYIx</td>\n",
       "      <td>...</td>\n",
       "      <td>-7.504</td>\n",
       "      <td>0</td>\n",
       "      <td>0.21600000000000005</td>\n",
       "      <td>0.00432</td>\n",
       "      <td>0.007229999999999999</td>\n",
       "      <td>0.489</td>\n",
       "      <td>0.65</td>\n",
       "      <td>111.904</td>\n",
       "      <td>262467</td>\n",
       "      <td>en</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>00cqd6ZsSkLZqGMlQCR0Zo</td>\n",
       "      <td>Baby It's Cold Outside (feat. Christina Aguilera)</td>\n",
       "      <td>CeeLo Green</td>\n",
       "      <td>I really can't stay Baby it's cold outside I'v...</td>\n",
       "      <td>41</td>\n",
       "      <td>3ssspRe42CXkhPxdc12xcp</td>\n",
       "      <td>CeeLo's Magic Moment</td>\n",
       "      <td>2012-10-29</td>\n",
       "      <td>Christmas Soul</td>\n",
       "      <td>6FZYc2BvF7tColxO8PBShV</td>\n",
       "      <td>...</td>\n",
       "      <td>-5.819</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0341</td>\n",
       "      <td>0.6890000000000001</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0664</td>\n",
       "      <td>0.405</td>\n",
       "      <td>118.593</td>\n",
       "      <td>243067</td>\n",
       "      <td>en</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 25 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                       0                                                  1   \\\n",
       "0                track_id                                         track_name   \n",
       "1  0017A6SJgTbfQVU2EtsPNo                                           Pangarap   \n",
       "2  004s3t0ONYlzxII9PLgU6z                                       I Feel Alive   \n",
       "3  00chLpzhgVjxs1zKC9UScL                                             Poison   \n",
       "4  00cqd6ZsSkLZqGMlQCR0Zo  Baby It's Cold Outside (feat. Christina Aguilera)   \n",
       "\n",
       "                2                                                  3   \\\n",
       "0     track_artist                                             lyrics   \n",
       "1  Barbie's Cradle  Minsan pa Nang ako'y napalingon Hindi ko alam ...   \n",
       "2    Steady Rollin  The trees, are singing in the wind The sky blu...   \n",
       "3   Bell Biv DeVoe  NA Yeah, Spyderman and Freeze in full effect U...   \n",
       "4      CeeLo Green  I really can't stay Baby it's cold outside I'v...   \n",
       "\n",
       "                 4                       5                     6   \\\n",
       "0  track_popularity          track_album_id      track_album_name   \n",
       "1                41  1srJQ0njEQgd8w4XSqI4JQ                  Trip   \n",
       "2                28  3z04Lb9Dsilqw68SHt6jLB           Love & Loss   \n",
       "3                 0  6oZ6brjB8x3GoeSYdwJdPc                  Gold   \n",
       "4                41  3ssspRe42CXkhPxdc12xcp  CeeLo's Magic Moment   \n",
       "\n",
       "                         7   \\\n",
       "0  track_album_release_date   \n",
       "1                2001-01-01   \n",
       "2                2017-11-21   \n",
       "3                2005-01-01   \n",
       "4                2012-10-29   \n",
       "\n",
       "                                                  8                       9   \\\n",
       "0                                      playlist_name             playlist_id   \n",
       "1                                 Pinoy Classic Rock  37i9dQZF1DWYDQ8wBxd7xt   \n",
       "2                                  Hard Rock Workout  3YouF0u7waJnolytf9JCXf   \n",
       "3  Back in the day - R&B, New Jack Swing, Swingbe...  3a9y4eeCJRmG9p4YKfqYIx   \n",
       "4                                     Christmas Soul  6FZYc2BvF7tColxO8PBShV   \n",
       "\n",
       "   ...        15    16                   17                  18  \\\n",
       "0  ...  loudness  mode          speechiness        acousticness   \n",
       "1  ...   -10.068     1               0.0236               0.279   \n",
       "2  ...    -4.739     1               0.0442              0.0117   \n",
       "3  ...    -7.504     0  0.21600000000000005             0.00432   \n",
       "4  ...    -5.819     0               0.0341  0.6890000000000001   \n",
       "\n",
       "                     19                   20                  21       22  \\\n",
       "0      instrumentalness             liveness             valence    tempo   \n",
       "1                0.0117               0.0887  0.5660000000000001   97.091   \n",
       "2               0.00994  0.34700000000000003               0.404  135.225   \n",
       "3  0.007229999999999999                0.489                0.65  111.904   \n",
       "4                     0               0.0664               0.405  118.593   \n",
       "\n",
       "            23        24  \n",
       "0  duration_ms  language  \n",
       "1       235440        tl  \n",
       "2       373512        en  \n",
       "3       262467        en  \n",
       "4       243067        en  \n",
       "\n",
       "[5 rows x 25 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lista_ = os.listdir(path)\n",
    "songs = os.path.join(path, lista_[0])\n",
    "print(songs)\n",
    "dataset = pd.read_csv(songs, header=None)\n",
    "dataset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I really can't stay Baby it's cold outside I've got to go away Baby it's cold out there This evening has been Been hoping that you'd drop in So very nice I'll hold your hands, they're just like ice My mother will start to worry Beautiful, what's your hurry? My father will be pacing the floor Listen to that fireplace roar So really I'd better scurry Beautiful, please don't hurry Well maybe just a half a drink more Why don't you put some records on while I pour The neighbors might think Baby, it's bad out there Say, what's in this drink? No cabs to be had out there I wish I knew how Your eyes are like starlight To break the spell I'll take your hat, your hair looks swell I ought to say no, no, no, sir Mind if I move in closer? At least I'm gonna say that I tried What's the sense in hurting my pride? I really can't stay Baby don't hold out Baby it's cold outside I simply must go See that it's cold outside The answer is no I said it's cold out there This welcome has been How lucky that you dropped in So nice and warm Look out the window at that storm My sister will be suspicious Gosh, your lips look delicious My brother will be there at the door Waves upon a tropical shore My maiden aunt's mind is vicious Oh, your lips are delicious Maybe just a cigarette more Never such a blizzard before Hey I've got to go home Baby, you'll freeze out there Say, lend me your coat It's up to your knees out there You've really been grand I'm thrilled when you touch my hand But don't you see How can you do this thing to me? There's bound to be talk tomorrow Think of my life long sorrow At least there will be plenty implied If you caught pneumonia and died I really can't stay Get over that old lie Baby, baby it's cold outside\n"
     ]
    }
   ],
   "source": [
    "fila_5 = dataset.iloc[4, 3]\n",
    "print(fila_5)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import pickle\n",
    "\n",
    "class InvertIndex:\n",
    "    def __init__(self, index_file):\n",
    "        self.index_file = index_file\n",
    "        self.index = {}\n",
    "        self.idf = {}\n",
    "        self.length = {}\n",
    "\n",
    "    def building(self, collection_text, position_text):\n",
    "        total_docs = len(collection_text)\n",
    "        doc_count = {}\n",
    "\n",
    "        # build the inverted index with the collection\n",
    "        for i, row in collection_text.iterrows():\n",
    "          doc_id = i\n",
    "          doc = row.iloc[position_text]\n",
    "          keywords = preprocesamiento(doc)\n",
    "        # compute the tf\n",
    "          tf_per_doc = {}\n",
    "\n",
    "          for keyword in keywords:\n",
    "            if keyword in tf_per_doc:\n",
    "              tf_per_doc[keyword] += 1\n",
    "            else:\n",
    "              tf_per_doc[keyword] = 1\n",
    "          for term in tf_per_doc:\n",
    "            tf_per_doc[term] = math.log10(1 + tf_per_doc[term])\n",
    "\n",
    "        # compute the idf\n",
    "          for term, tf in tf_per_doc.items():\n",
    "            if term not in self.index:\n",
    "                self.index[term] = []\n",
    "            self.index[term].append((doc_id, tf))\n",
    "\n",
    "            if term in doc_count:\n",
    "                doc_count[term] += 1\n",
    "            else:\n",
    "                doc_count[term] = 1\n",
    "\n",
    "        for term, count in doc_count.items():\n",
    "            self.idf[term] = math.log(total_docs / (count))\n",
    "        # compute the length (norm)\n",
    "        for doc_id in range(total_docs):\n",
    "            norm = 0\n",
    "            for term, postings in self.index.items():\n",
    "                for doc, tf in postings:\n",
    "                    if doc == doc_id:\n",
    "                        norm += (tf * self.idf[term]) ** 2\n",
    "            self.length[doc_id] = math.sqrt(norm) #Get sum(tf*idf^2)\n",
    "        # store in disk\n",
    "        with open(self.index_file, 'wb') as f:\n",
    "            pickle.dump((self.index, self.idf, self.length), f)\n",
    "\n",
    "    def retrieval(self, query, k):\n",
    "        self.load_index()\n",
    "        N = len(self.length)\n",
    "        scores = [0] * len(self.length)\n",
    "        # preprocesar la query: extraer los terminos unicos\n",
    "        terms = preprocesamiento(query)\n",
    "        # calcular el tf-idf del query\n",
    "        tf_query = {}\n",
    "        for term in terms:\n",
    "            if term in tf_query:\n",
    "                tf_query[term] += 1\n",
    "            else:\n",
    "                tf_query[term] = 1\n",
    "        tfidf_query = {}\n",
    "        for term, tf in tf_query.items():   \n",
    "            if term in self.idf:\n",
    "                tfidf_query[term] = math.log10(1 + tf) * self.idf[term]\n",
    "\n",
    "        norm_query = math.sqrt(sum(w_tq**2 for w_tq in tfidf_query.values()))\n",
    "\n",
    "        # aplicar similitud de coseno y guardarlo en el diccionario score\n",
    "        for term, w_tq in tfidf_query.items():\n",
    "            if term in self.index:\n",
    "                for doc, tf_td in self.index[term]:\n",
    "                    w_td = tf_td * self.idf[term] #Calculo tf_idf para wt,d\n",
    "                    scores[doc] += w_td * w_tq\n",
    "                    \n",
    "        for d in range(N):\n",
    "            if self.length[d] != 0:\n",
    "                scores[d] /= self.length[d]*norm_query  # Normalización del documento y la consulta\n",
    "    \n",
    "        # ordenar el score de forma descendente\n",
    "        result = sorted(enumerate(scores), key=lambda x: x[1], reverse=True)\n",
    "        # retornamos los k documentos mas relevantes (de mayor similitud al query)\n",
    "        return result[:k]\n",
    "\n",
    "    def load_index(self):\n",
    "        # load index from disk\n",
    "        with open(self.index_file, 'rb') as f:\n",
    "            self.index, self.idf, self.length = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset = dataset.head(40)\n",
    "\n",
    "# def mostrarDocumentos(result):\n",
    "#     for doc, score in result:\n",
    "#         print(\"Documento\", doc, \" con similitud: \", score)\n",
    "#     print(\"_____\")\n",
    "\n",
    "# index = InvertIndex(\"indice.dat\")\n",
    "# index.building(dataset, 3) #El texto a procesar esta en la posicion 1\n",
    "\n",
    "# Query1 = \"I really can't stay Baby it's cold outside \"\n",
    "# result = index.retrieval(Query1, 10)\n",
    "# print(\"Resultados para\", Query1)\n",
    "# mostrarDocumentos(result)\n",
    "\n",
    "# news_value = dataset.iloc[4, 3] # Sacamos el primer doc el cual nos deberia de dar 1\n",
    "# Query2 = news_value\n",
    "# result2 = index.retrieval(Query2, 10)\n",
    "# print(\"Resultados para\", Query2)\n",
    "# mostrarDocumentos(result2)\n",
    "\n",
    "# Query3 = dataset.iloc[7, 3]\n",
    "# result3 = index.retrieval(Query3, 10)\n",
    "# print(\"Resultados para\", Query3)\n",
    "# mostrarDocumentos(result3)\n",
    "\n",
    "# Query3 = dataset.iloc[20, 3]\n",
    "# result3 = index.retrieval(Query3, 10)\n",
    "# print(\"Resultados para\", Query3)\n",
    "# mostrarDocumentos(result3)\n",
    "\n",
    "# Query3 = dataset.iloc[19, 3]\n",
    "# result3 = index.retrieval(Query3, 10)\n",
    "# print(\"Resultados para\", Query3)\n",
    "# mostrarDocumentos(result3)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Index</th>\n",
       "      <th>ProcessedText</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>minsan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>nang</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>ako</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>napalingon</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>hindi</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6906</th>\n",
       "      <td>39</td>\n",
       "      <td>town</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6907</th>\n",
       "      <td>39</td>\n",
       "      <td>santa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6908</th>\n",
       "      <td>39</td>\n",
       "      <td>claus</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6909</th>\n",
       "      <td>39</td>\n",
       "      <td>comin</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6910</th>\n",
       "      <td>39</td>\n",
       "      <td>town</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>6911 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      Index ProcessedText\n",
       "0         1        minsan\n",
       "1         1          nang\n",
       "2         1           ako\n",
       "3         1    napalingon\n",
       "4         1         hindi\n",
       "...     ...           ...\n",
       "6906     39          town\n",
       "6907     39         santa\n",
       "6908     39         claus\n",
       "6909     39         comin\n",
       "6910     39          town\n",
       "\n",
       "[6911 rows x 2 columns]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = dataset.head(40)\n",
    "#Procesar los documentos en pares\n",
    "position_text=3\n",
    "pairs = []\n",
    "\n",
    "for i, row in dataset.iterrows():\n",
    "    if(i!=0):\n",
    "        words = preprocesamiento(row.iloc[position_text])\n",
    "        for text in words:\n",
    "            pairs.append((i, text))\n",
    "\n",
    "pairs_df = pd.DataFrame(pairs, columns=['Index', 'ProcessedText'])\n",
    "pairs_df.head(len(pairs_df))\n",
    "\n",
    "\n",
    "# print(pairs_df[pairs_df[\"ProcessedText\"] == \"6\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "La cantidad de niveles serán 6\n",
      "Cantidad de bloques generados 46\n",
      "Merging files block_nivel0_bloque0.pkl and block_nivel0_bloque1.pkl into new level 1\n",
      "Merging files block_nivel0_bloque10.pkl and block_nivel0_bloque11.pkl into new level 1\n",
      "Merging files block_nivel0_bloque12.pkl and block_nivel0_bloque13.pkl into new level 1\n",
      "Merging files block_nivel0_bloque14.pkl and block_nivel0_bloque15.pkl into new level 1\n",
      "Merging files block_nivel0_bloque16.pkl and block_nivel0_bloque17.pkl into new level 1\n",
      "Merging files block_nivel0_bloque18.pkl and block_nivel0_bloque19.pkl into new level 1\n",
      "Merging files block_nivel0_bloque2.pkl and block_nivel0_bloque20.pkl into new level 1\n",
      "Merging files block_nivel0_bloque21.pkl and block_nivel0_bloque22.pkl into new level 1\n",
      "Merging files block_nivel0_bloque23.pkl and block_nivel0_bloque24.pkl into new level 1\n",
      "Merging files block_nivel0_bloque25.pkl and block_nivel0_bloque26.pkl into new level 1\n",
      "Merging files block_nivel0_bloque27.pkl and block_nivel0_bloque28.pkl into new level 1\n",
      "Merging files block_nivel0_bloque29.pkl and block_nivel0_bloque3.pkl into new level 1\n",
      "Merging files block_nivel0_bloque30.pkl and block_nivel0_bloque31.pkl into new level 1\n",
      "Merging files block_nivel0_bloque32.pkl and block_nivel0_bloque33.pkl into new level 1\n",
      "Merging files block_nivel0_bloque34.pkl and block_nivel0_bloque35.pkl into new level 1\n",
      "Merging files block_nivel0_bloque36.pkl and block_nivel0_bloque37.pkl into new level 1\n",
      "Merging files block_nivel0_bloque38.pkl and block_nivel0_bloque39.pkl into new level 1\n",
      "Merging files block_nivel0_bloque4.pkl and block_nivel0_bloque40.pkl into new level 1\n",
      "Merging files block_nivel0_bloque41.pkl and block_nivel0_bloque42.pkl into new level 1\n",
      "Merging files block_nivel0_bloque43.pkl and block_nivel0_bloque44.pkl into new level 1\n",
      "Merging files block_nivel0_bloque45.pkl and block_nivel0_bloque5.pkl into new level 1\n",
      "Merging files block_nivel0_bloque6.pkl and block_nivel0_bloque7.pkl into new level 1\n",
      "Merging files block_nivel0_bloque8.pkl and block_nivel0_bloque9.pkl into new level 1\n",
      "Merging files block_nivel1_bloque0.pkl and block_nivel1_bloque10.pkl into new level 2\n",
      "Merging files block_nivel1_bloque12.pkl and block_nivel1_bloque14.pkl into new level 2\n",
      "Merging files block_nivel1_bloque16.pkl and block_nivel1_bloque18.pkl into new level 2\n",
      "Merging files block_nivel1_bloque2.pkl and block_nivel1_bloque21.pkl into new level 2\n",
      "Merging files block_nivel1_bloque23.pkl and block_nivel1_bloque25.pkl into new level 2\n",
      "Merging files block_nivel1_bloque27.pkl and block_nivel1_bloque29.pkl into new level 2\n",
      "Merging files block_nivel1_bloque30.pkl and block_nivel1_bloque32.pkl into new level 2\n",
      "Merging files block_nivel1_bloque34.pkl and block_nivel1_bloque36.pkl into new level 2\n",
      "Merging files block_nivel1_bloque38.pkl and block_nivel1_bloque4.pkl into new level 2\n",
      "Merging files block_nivel1_bloque41.pkl and block_nivel1_bloque43.pkl into new level 2\n",
      "Merging files block_nivel1_bloque45.pkl and block_nivel1_bloque6.pkl into new level 2\n",
      "Carrying over block_nivel1_bloque8.pkl to new level 2\n",
      "Merging files block_nivel2_bloque46.pkl and block_nivel2_bloque50.pkl into new level 3\n",
      "Merging files block_nivel2_bloque54.pkl and block_nivel2_bloque58.pkl into new level 3\n",
      "Merging files block_nivel2_bloque62.pkl and block_nivel2_bloque66.pkl into new level 3\n",
      "Carrying over block_nivel3_bloque70.pkl to new level 4\n",
      "Carrying over block_nivel4_bloque76.pkl to new level 5\n",
      "Carrying over block_nivel5_bloque78.pkl to new level 6\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pickle\n",
    "import sys\n",
    "import math\n",
    "from itertools import islice\n",
    "\n",
    "class SPIMI:\n",
    "    def __init__(self, index_dir=\"index_blocks\"):\n",
    "        self.index_dir = index_dir  \n",
    "        self.block_counter = 0      \n",
    "        self.doc_count = 0          \n",
    "        self.idf = {}\n",
    "        self.length = {}\n",
    "        self.disk_limit = 4000  \n",
    "        self.totalLevels = 0 \n",
    "        self.currentL = 0\n",
    "        self.cuntermerged = 0 \n",
    "\n",
    "        if not os.path.exists(self.index_dir):\n",
    "            os.makedirs(self.index_dir)\n",
    "\n",
    "    def spimi_invert(self, token_stream):\n",
    "        dictionary = {}\n",
    "        for _, row in token_stream.iterrows():\n",
    "            doc_id = row['Index']\n",
    "            token = row['ProcessedText']\n",
    "            \n",
    "            if token not in dictionary:\n",
    "                dictionary[token] = {}  \n",
    "            \n",
    "            if doc_id not in dictionary[token]:\n",
    "                dictionary[token][doc_id] = 1  \n",
    "            else:\n",
    "                dictionary[token][doc_id] += 1  \n",
    "            dictionary_size = sys.getsizeof(dictionary)\n",
    "            if dictionary_size >= self.disk_limit:\n",
    "                self.write_block_to_disk(dictionary, level=0)\n",
    "                dictionary.clear()\n",
    "                \n",
    "        if dictionary:\n",
    "            self.write_block_to_disk(dictionary, level=0)\n",
    "\n",
    "        self.totalLevels = math.ceil(math.log2(self.block_counter))\n",
    "        print(\"La cantidad de niveles serán\", self.totalLevels)\n",
    "        print(\"Cantidad de bloques generados\", self.block_counter)\n",
    "        \n",
    "    def write_block_to_disk(self, dictionary, level):\n",
    "        sorted_terms = dict(sorted(dictionary.items())) \n",
    "        file_path = os.path.join(self.index_dir, f\"block_nivel{level}_bloque{self.block_counter}.pkl\")\n",
    "        with open(file_path, \"wb\") as f:\n",
    "            pickle.dump(sorted_terms, f)\n",
    "        \n",
    "        self.block_counter += 1\n",
    "\n",
    "    def merge_blocks(self, block1, block2):\n",
    "        merged = block1.copy()\n",
    "        for term, postings in block2.items():\n",
    "            if term in merged:\n",
    "                for doc_id, freq in postings.items():\n",
    "                    if doc_id in merged[term]:\n",
    "                        merged[term][doc_id] += freq\n",
    "                    else:\n",
    "                        merged[term][doc_id] = freq\n",
    "            else:\n",
    "                merged[term] = postings\n",
    "        return dict(sorted(merged.items()))\n",
    "\n",
    "    def split_and_save_block(self, merged_data, level):\n",
    "        # Divide the merged dictionary into two roughly equal parts\n",
    "        items = list(merged_data.items())\n",
    "        midpoint = len(items) // 2\n",
    "\n",
    "        part1 = dict(items[:midpoint])\n",
    "        part2 = dict(items[midpoint:])\n",
    "\n",
    "        # Save both parts as separate blocks\n",
    "        for part in (part1, part2):\n",
    "            file_path = os.path.join(self.index_dir, f\"block_nivel{level}_bloque{self.cuntermerged}.pkl\")\n",
    "            with open(file_path, \"wb\") as f:\n",
    "                pickle.dump(part, f)\n",
    "            self.cuntermerged += 1\n",
    "\n",
    "    def load_block(self, filepath):\n",
    "        with open(filepath, \"rb\") as f:\n",
    "            return pickle.load(f)\n",
    "\n",
    "    def merge(self):\n",
    "        for level in range(self.totalLevels):\n",
    "            distance = 2 ** level\n",
    "            files = sorted(os.listdir(self.index_dir))\n",
    "            next_level_files = []\n",
    "            \n",
    "            for i in range(0, len(files), distance * 2):\n",
    "                file1_path = os.path.join(self.index_dir, files[i])\n",
    "                block1 = self.load_block(file1_path)\n",
    "                \n",
    "                if i + distance < len(files):\n",
    "                    file2_path = os.path.join(self.index_dir, files[i + distance])\n",
    "                    block2 = self.load_block(file2_path)\n",
    "                    merged_block = self.merge_blocks(block1, block2)\n",
    "                    print(f\"Merging files {files[i]} and {files[i + distance]} into new level {level + 1}\")\n",
    "                else:\n",
    "                    merged_block = block1  \n",
    "                    print(f\"Carrying over {files[i]} to new level {level + 1}\")\n",
    "                \n",
    "                # Split the merged block into two parts and save them\n",
    "                self.split_and_save_block(merged_block, level + 1)\n",
    "                next_level_files.extend([\n",
    "                    f\"block_nivel{level + 1}_bloque{self.cuntermerged - 2}.pkl\",\n",
    "                    f\"block_nivel{level + 1}_bloque{self.cuntermerged - 1}.pkl\"\n",
    "                ])\n",
    "\n",
    "            # Delete old blocks of the current level\n",
    "            for filename in files:\n",
    "                if filename.startswith(f\"block_nivel{level}_\"):\n",
    "                    os.remove(os.path.join(self.index_dir, filename))\n",
    "            \n",
    "            # Prepare for next level\n",
    "            files = next_level_files\n",
    "\n",
    "# Ejemplo de uso\n",
    "# s = SPIMI()  \n",
    "# s.spimi_invert(pairs_df)  # `pairs_df` debe ser un DataFrame con las columnas 'Index' y 'ProcessedText'\n",
    "# s.merge()\n",
    "\n",
    "\n",
    "s = SPIMI()  \n",
    "s.spimi_invert(pairs_df)\n",
    "s.merge()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Term: 6\n",
      "  DocID: 2, TF: 1\n",
      "Term: aaa\n",
      "  DocID: 3, TF: 1\n",
      "Term: abr\n",
      "  DocID: 25, TF: 1\n",
      "Term: actin\n",
      "  DocID: 14, TF: 1\n",
      "Term: ahh\n",
      "  DocID: 26, TF: 4\n",
      "Term: ain\n",
      "  DocID: 14, TF: 4\n",
      "Term: aisl\n",
      "  DocID: 16, TF: 1\n",
      "Term: ake\n",
      "  DocID: 1, TF: 1\n",
      "Term: akin\n",
      "  DocID: 1, TF: 4\n",
      "Term: ako\n",
      "  DocID: 1, TF: 2\n",
      "Term: alam\n",
      "  DocID: 1, TF: 2\n",
      "Term: aliv\n",
      "  DocID: 2, TF: 2\n",
      "  DocID: 26, TF: 4\n",
      "Term: alright\n",
      "  DocID: 15, TF: 2\n",
      "Term: amarradito\n",
      "  DocID: 13, TF: 1\n",
      "Term: amplif\n",
      "  DocID: 16, TF: 1\n",
      "Term: ando\n",
      "  DocID: 15, TF: 1\n",
      "Term: ang\n",
      "  DocID: 1, TF: 7\n",
      "Term: angel\n",
      "  DocID: 2, TF: 3\n",
      "  DocID: 15, TF: 1\n",
      "Term: appeal\n",
      "  DocID: 14, TF: 1\n",
      "Term: aqui\n",
      "  DocID: 25, TF: 1\n",
      "Term: aww\n",
      "  DocID: 14, TF: 4\n",
      "Term: ay\n",
      "  DocID: 1, TF: 5\n",
      "  DocID: 15, TF: 1\n",
      "Term: babi\n",
      "  DocID: 15, TF: 4\n",
      "Term: backstag\n",
      "  DocID: 26, TF: 2\n",
      "Term: bailar\n",
      "  DocID: 15, TF: 1\n",
      "Term: bawat\n",
      "  DocID: 1, TF: 1\n",
      "Term: bañarno\n",
      "  DocID: 13, TF: 2\n",
      "Term: beat\n",
      "  DocID: 26, TF: 2\n",
      "  DocID: 15, TF: 1\n",
      "Term: beauti\n",
      "  DocID: 3, TF: 1\n",
      "Term: bein\n",
      "  DocID: 14, TF: 2\n",
      "Term: bella\n",
      "  DocID: 15, TF: 3\n",
      "Term: bench\n",
      "  DocID: 2, TF: 1\n",
      "Term: besito\n",
      "  DocID: 15, TF: 1\n",
      "Term: besot\n",
      "  DocID: 25, TF: 1\n",
      "Term: bewar\n",
      "  DocID: 3, TF: 1\n",
      "Term: bien\n",
      "  DocID: 13, TF: 3\n",
      "Term: bit\n",
      "  DocID: 16, TF: 1\n",
      "Term: biv\n",
      "  DocID: 3, TF: 1\n",
      "Term: black\n",
      "  DocID: 14, TF: 4\n",
      "  DocID: 15, TF: 1\n",
      "Term: blast\n",
      "  DocID: 14, TF: 1\n",
      "Term: blew\n",
      "  DocID: 26, TF: 2\n",
      "Term: blind\n",
      "  DocID: 3, TF: 1\n",
      "Term: blue\n",
      "  DocID: 2, TF: 1\n",
      "Term: boca\n",
      "  DocID: 25, TF: 1\n",
      "Term: bodi\n",
      "  DocID: 15, TF: 1\n",
      "  DocID: 16, TF: 1\n",
      "Term: boogi\n",
      "  DocID: 25, TF: 1\n",
      "Term: boquita\n",
      "  DocID: 13, TF: 3\n",
      "Term: bother\n",
      "  DocID: 14, TF: 1\n",
      "Term: bought\n",
      "  DocID: 26, TF: 2\n",
      "Term: bout\n",
      "  DocID: 27, TF: 1\n",
      "Term: box\n",
      "  DocID: 26, TF: 26\n",
      "Term: boy\n",
      "  DocID: 15, TF: 2\n",
      "Term: break\n",
      "  DocID: 3, TF: 1\n",
      "Term: breakin\n",
      "  DocID: 3, TF: 1\n",
      "Term: brother\n",
      "  DocID: 14, TF: 6\n",
      "Term: bumulong\n",
      "  DocID: 1, TF: 1\n",
      "Term: bust\n",
      "  DocID: 14, TF: 1\n",
      "Term: butt\n",
      "  DocID: 3, TF: 1\n",
      "Term: búscame\n",
      "  DocID: 13, TF: 1\n",
      "Term: cama\n",
      "  DocID: 15, TF: 1\n",
      "Term: care\n",
      "  DocID: 2, TF: 1\n",
      "  DocID: 14, TF: 1\n",
      "Term: cash\n",
      "  DocID: 27, TF: 1\n",
      "Term: catch\n",
      "  DocID: 27, TF: 1\n",
      "Term: caution\n",
      "  DocID: 3, TF: 1\n",
      "Term: chainz\n",
      "  DocID: 27, TF: 1\n",
      "Term: chang\n",
      "  DocID: 26, TF: 2\n",
      "  DocID: 14, TF: 6\n",
      "Term: che\n",
      "  DocID: 15, TF: 1\n",
      "Term: checkin\n",
      "  DocID: 3, TF: 1\n",
      "Term: chest\n",
      "  DocID: 3, TF: 1\n",
      "Term: chez\n",
      "  DocID: 15, TF: 5\n",
      "Term: chill\n",
      "  DocID: 14, TF: 1\n",
      "Term: chose\n",
      "  DocID: 27, TF: 1\n",
      "Term: cielo\n",
      "  DocID: 15, TF: 1\n",
      "Term: cintura\n",
      "  DocID: 15, TF: 1\n",
      "Term: clockin\n",
      "  DocID: 3, TF: 1\n",
      "Term: close\n",
      "  DocID: 2, TF: 1\n",
      "  DocID: 27, TF: 2\n",
      "  DocID: 14, TF: 1\n",
      "Term: coco\n",
      "  DocID: 25, TF: 1\n",
      "Term: conceal\n",
      "  DocID: 14, TF: 1\n",
      "Term: confort\n",
      "  DocID: 15, TF: 1\n",
      "Term: contigo\n",
      "  DocID: 15, TF: 1\n",
      "Term: convers\n",
      "  DocID: 27, TF: 1\n",
      "Term: cook\n",
      "  DocID: 27, TF: 1\n",
      "Term: cop\n",
      "  DocID: 14, TF: 2\n",
      "Term: crack\n",
      "  DocID: 14, TF: 4\n",
      "Term: crash\n",
      "  DocID: 27, TF: 1\n",
      "Term: crazi\n",
      "  DocID: 16, TF: 1\n",
      "Term: crew\n",
      "  DocID: 3, TF: 1\n",
      "Term: crime\n",
      "  DocID: 14, TF: 1\n",
      "Term: crowd\n",
      "  DocID: 26, TF: 2\n",
      "Term: cruella\n",
      "  DocID: 16, TF: 1\n",
      "Term: cumpl\n",
      "  DocID: 15, TF: 1\n",
      "Term: cura\n",
      "  DocID: 15, TF: 2\n",
      "Term: cure\n",
      "  DocID: 3, TF: 1\n",
      "Term: cut\n",
      "  DocID: 3, TF: 1\n",
      "Term: dale\n",
      "  DocID: 25, TF: 2\n",
      "Term: dame\n",
      "  DocID: 25, TF: 1\n",
      "  DocID: 15, TF: 1\n",
      "Term: damn\n",
      "  DocID: 14, TF: 1\n",
      "Term: danc\n",
      "  DocID: 2, TF: 1\n",
      "  DocID: 16, TF: 1\n",
      "Term: darat\n",
      "  DocID: 1, TF: 1\n",
      "Term: dark\n",
      "  DocID: 27, TF: 1\n",
      "  DocID: 14, TF: 1\n",
      "Term: day\n",
      "  DocID: 26, TF: 4\n",
      "Term: dead\n",
      "  DocID: 3, TF: 3\n",
      "  DocID: 14, TF: 1\n",
      "Term: deal\n",
      "  DocID: 14, TF: 1\n",
      "Term: death\n",
      "  DocID: 27, TF: 1\n",
      "Term: deci\n",
      "  DocID: 25, TF: 1\n",
      "Term: dejast\n",
      "  DocID: 15, TF: 2\n",
      "Term: del\n",
      "  DocID: 25, TF: 1\n",
      "Term: dement\n",
      "  DocID: 13, TF: 2\n",
      "Term: demon\n",
      "  DocID: 3, TF: 1\n",
      "Term: despacito\n",
      "  DocID: 13, TF: 1\n",
      "Term: devic\n",
      "  DocID: 16, TF: 1\n",
      "Term: devil\n",
      "  DocID: 14, TF: 1\n",
      "Term: devill\n",
      "  DocID: 16, TF: 1\n",
      "Term: devo\n",
      "  DocID: 3, TF: 1\n",
      "Term: dice\n",
      "  DocID: 15, TF: 2\n",
      "Term: die\n",
      "  DocID: 27, TF: 1\n",
      "Term: dijist\n",
      "  DocID: 15, TF: 1\n",
      "Term: dile\n",
      "  DocID: 15, TF: 1\n",
      "Term: dirti\n",
      "  DocID: 15, TF: 1\n",
      "Term: disgrac\n",
      "  DocID: 14, TF: 1\n",
      "Term: dist\n",
      "  DocID: 15, TF: 1\n",
      "Term: distant\n",
      "  DocID: 26, TF: 2\n",
      "  DocID: 14, TF: 1\n",
      "Term: dito\n",
      "  DocID: 1, TF: 1\n",
      "Term: divorc\n",
      "  DocID: 16, TF: 1\n",
      "Term: door\n",
      "  DocID: 26, TF: 2\n",
      "Term: dope\n",
      "  DocID: 14, TF: 2\n",
      "Term: downpour\n",
      "  DocID: 26, TF: 2\n",
      "Term: dreamin\n",
      "  DocID: 3, TF: 1\n",
      "Term: drive\n",
      "  DocID: 3, TF: 1\n",
      "Term: drivin\n",
      "  DocID: 3, TF: 1\n",
      "Term: drug\n",
      "  DocID: 14, TF: 1\n",
      "Term: dura\n",
      "  DocID: 15, TF: 4\n",
      "Term: dé\n",
      "  DocID: 13, TF: 3\n",
      "Term: día\n",
      "  DocID: 13, TF: 2\n",
      "Term: díselo\n",
      "  DocID: 15, TF: 1\n",
      "Term: ear\n",
      "  DocID: 26, TF: 2\n",
      "Term: easi\n",
      "  DocID: 14, TF: 1\n",
      "Term: east\n",
      "  DocID: 14, TF: 1\n",
      "Term: eat\n",
      "  DocID: 14, TF: 1\n",
      "Term: el\n",
      "  DocID: 13, TF: 2\n",
      "  DocID: 15, TF: 3\n",
      "Term: elektra\n",
      "  DocID: 16, TF: 1\n",
      "Term: ella\n",
      "  DocID: 25, TF: 3\n",
      "Term: em\n",
      "  DocID: 3, TF: 1\n",
      "  DocID: 14, TF: 5\n",
      "Term: emi\n",
      "  DocID: 16, TF: 1\n",
      "Term: empezando\n",
      "  DocID: 15, TF: 1\n",
      "Term: en\n",
      "  DocID: 13, TF: 2\n",
      "  DocID: 15, TF: 2\n",
      "Term: entendido\n",
      "  DocID: 15, TF: 1\n",
      "Term: entero\n",
      "  DocID: 15, TF: 1\n",
      "Term: entertain\n",
      "  DocID: 16, TF: 5\n",
      "Term: envida\n",
      "  DocID: 15, TF: 1\n",
      "Term: eras\n",
      "  DocID: 14, TF: 1\n",
      "Term: ere\n",
      "  DocID: 25, TF: 1\n",
      "  DocID: 15, TF: 1\n",
      "Term: esperaban\n",
      "  DocID: 15, TF: 1\n",
      "Term: esperando\n",
      "  DocID: 15, TF: 1\n",
      "Term: estoy\n",
      "  DocID: 13, TF: 1\n",
      "Term: estrella\n",
      "  DocID: 15, TF: 2\n",
      "Term: está\n",
      "  DocID: 13, TF: 4\n",
      "Term: evil\n",
      "  DocID: 14, TF: 1\n",
      "Term: explico\n",
      "  DocID: 15, TF: 1\n",
      "Term: ey\n",
      "  DocID: 15, TF: 3\n",
      "Term: eye\n",
      "  DocID: 3, TF: 1\n",
      "  DocID: 26, TF: 20\n",
      "  DocID: 27, TF: 3\n",
      "Term: fall\n",
      "  DocID: 3, TF: 1\n",
      "Term: fam\n",
      "  DocID: 27, TF: 1\n",
      "Term: fear\n",
      "  DocID: 27, TF: 2\n",
      "Term: feel\n",
      "  DocID: 2, TF: 2\n",
      "Term: feelin\n",
      "  DocID: 15, TF: 1\n",
      "Term: fella\n",
      "  DocID: 3, TF: 1\n",
      "Term: fellow\n",
      "  DocID: 3, TF: 1\n",
      "Term: fenc\n",
      "  DocID: 2, TF: 1\n",
      "Term: fiesta\n",
      "  DocID: 25, TF: 1\n",
      "  DocID: 13, TF: 2\n",
      "Term: fight\n",
      "  DocID: 14, TF: 1\n",
      "Term: fill\n",
      "  DocID: 14, TF: 1\n",
      "Term: fina\n",
      "  DocID: 15, TF: 2\n",
      "Term: fli\n",
      "  DocID: 3, TF: 2\n",
      "Term: forc\n",
      "  DocID: 16, TF: 1\n",
      "Term: freez\n",
      "  DocID: 3, TF: 1\n",
      "Term: fro\n",
      "  DocID: 3, TF: 1\n",
      "Term: fue\n",
      "  DocID: 15, TF: 1\n",
      "Term: fumando\n",
      "  DocID: 15, TF: 1\n",
      "Term: game\n",
      "  DocID: 14, TF: 1\n",
      "Term: gawin\n",
      "  DocID: 1, TF: 1\n",
      "Term: gent\n",
      "  DocID: 13, TF: 1\n",
      "Term: gentlemen\n",
      "  DocID: 16, TF: 1\n",
      "Term: gettin\n",
      "  DocID: 3, TF: 1\n",
      "  DocID: 27, TF: 1\n",
      "Term: girl\n",
      "  DocID: 3, TF: 5\n",
      "  DocID: 27, TF: 1\n",
      "Term: givin\n",
      "  DocID: 14, TF: 1\n",
      "Term: god\n",
      "  DocID: 27, TF: 1\n",
      "Term: gon\n",
      "  DocID: 26, TF: 6\n",
      "  DocID: 27, TF: 1\n",
      "  DocID: 16, TF: 2\n",
      "Term: ground\n",
      "  DocID: 16, TF: 1\n",
      "Term: guitar\n",
      "  DocID: 26, TF: 12\n",
      "Term: gun\n",
      "  DocID: 14, TF: 1\n",
      "Term: gunshot\n",
      "  DocID: 27, TF: 1\n",
      "Term: ha\n",
      "  DocID: 16, TF: 4\n",
      "Term: habitación\n",
      "  DocID: 15, TF: 1\n",
      "Term: hace\n",
      "  DocID: 25, TF: 2\n",
      "Term: hah\n",
      "  DocID: 25, TF: 2\n",
      "Term: hand\n",
      "  DocID: 2, TF: 1\n",
      "  DocID: 26, TF: 2\n",
      "Term: hanggang\n",
      "  DocID: 1, TF: 1\n",
      "Term: hangin\n",
      "  DocID: 1, TF: 1\n",
      "Term: hard\n",
      "  DocID: 3, TF: 1\n",
      "Term: hate\n",
      "  DocID: 14, TF: 1\n",
      "Term: hay\n",
      "  DocID: 15, TF: 1\n",
      "Term: head\n",
      "  DocID: 3, TF: 1\n",
      "  DocID: 26, TF: 2\n",
      "Term: heal\n",
      "  DocID: 14, TF: 1\n",
      "Term: heard\n",
      "  DocID: 26, TF: 4\n",
      "Term: heart\n",
      "  DocID: 3, TF: 2\n",
      "Term: heaven\n",
      "  DocID: 14, TF: 1\n",
      "Term: heavi\n",
      "  DocID: 26, TF: 2\n",
      "Term: hero\n",
      "  DocID: 26, TF: 26\n",
      "  DocID: 14, TF: 1\n",
      "Term: hey\n",
      "  DocID: 2, TF: 2\n",
      "  DocID: 14, TF: 1\n",
      "  DocID: 16, TF: 3\n",
      "Term: hice\n",
      "  DocID: 15, TF: 1\n",
      "Term: high\n",
      "  DocID: 3, TF: 1\n",
      "Term: hindi\n",
      "  DocID: 1, TF: 2\n",
      "Term: ho\n",
      "  DocID: 3, TF: 1\n",
      "Term: hoe\n",
      "  DocID: 3, TF: 1\n",
      "Term: hold\n",
      "  DocID: 14, TF: 1\n",
      "Term: hoo\n",
      "  DocID: 3, TF: 1\n",
      "Term: hous\n",
      "  DocID: 3, TF: 1\n",
      "Term: huey\n",
      "  DocID: 14, TF: 2\n",
      "Term: huh\n",
      "  DocID: 3, TF: 2\n",
      "Term: hung\n",
      "  DocID: 26, TF: 2\n",
      "Term: hungri\n",
      "  DocID: 14, TF: 1\n",
      "Term: hurt\n",
      "  DocID: 14, TF: 1\n",
      "Term: ika\n",
      "  DocID: 1, TF: 1\n",
      "Term: ikaw\n",
      "  DocID: 1, TF: 1\n",
      "Term: inocent\n",
      "  DocID: 15, TF: 1\n",
      "Term: isang\n",
      "  DocID: 1, TF: 2\n",
      "Term: ita\n",
      "  DocID: 13, TF: 12\n",
      "Term: ito\n",
      "  DocID: 1, TF: 1\n",
      "Term: iyong\n",
      "  DocID: 1, TF: 4\n",
      "Term: ja\n",
      "  DocID: 13, TF: 3\n",
      "Term: jack\n",
      "  DocID: 14, TF: 1\n",
      "Term: jaja\n",
      "  DocID: 15, TF: 1\n",
      "Term: jazz\n",
      "  DocID: 16, TF: 1\n",
      "Term: jealous\n",
      "  DocID: 14, TF: 1\n",
      "Term: judg\n",
      "  DocID: 27, TF: 1\n",
      "Term: juke\n",
      "  DocID: 26, TF: 26\n",
      "Term: jump\n",
      "  DocID: 2, TF: 1\n",
      "Term: juro\n",
      "  DocID: 13, TF: 1\n",
      "Term: ka\n",
      "  DocID: 1, TF: 5\n",
      "Term: kailan\n",
      "  DocID: 1, TF: 1\n",
      "Term: kay\n",
      "  DocID: 1, TF: 4\n",
      "Term: kaya\n",
      "  DocID: 1, TF: 2\n",
      "Term: keepin\n",
      "  DocID: 3, TF: 1\n",
      "Term: khalifa\n",
      "  DocID: 27, TF: 1\n",
      "Term: kid\n",
      "  DocID: 14, TF: 3\n",
      "Term: kill\n",
      "  DocID: 14, TF: 3\n",
      "Term: kiss\n",
      "  DocID: 2, TF: 3\n",
      "  DocID: 3, TF: 1\n",
      "  DocID: 27, TF: 1\n",
      "Term: know\n",
      "  DocID: 2, TF: 1\n",
      "Term: ko\n",
      "  DocID: 1, TF: 3\n",
      "Term: kong\n",
      "  DocID: 1, TF: 1\n",
      "Term: kung\n",
      "  DocID: 1, TF: 1\n",
      "Term: kénsel\n",
      "  DocID: 15, TF: 1\n",
      "Term: ladi\n",
      "  DocID: 16, TF: 1\n",
      "Term: laid\n",
      "  DocID: 3, TF: 1\n",
      "Term: lamig\n",
      "  DocID: 1, TF: 4\n",
      "Term: lang\n",
      "  DocID: 1, TF: 3\n",
      "Term: langit\n",
      "  DocID: 1, TF: 1\n",
      "Term: las\n",
      "  DocID: 15, TF: 3\n",
      "Term: latina\n",
      "  DocID: 15, TF: 1\n",
      "Term: laugh\n",
      "  DocID: 2, TF: 1\n",
      "Term: lay\n",
      "  DocID: 3, TF: 1\n",
      "Term: learn\n",
      "  DocID: 14, TF: 2\n",
      "Term: lengua\n",
      "  DocID: 25, TF: 2\n",
      "Term: life\n",
      "  DocID: 26, TF: 2\n",
      "  DocID: 27, TF: 1\n",
      "  DocID: 14, TF: 1\n",
      "Term: light\n",
      "  DocID: 16, TF: 1\n",
      "Term: lilingon\n",
      "  DocID: 1, TF: 1\n",
      "Term: lip\n",
      "  DocID: 2, TF: 1\n",
      "Term: listen\n",
      "  DocID: 16, TF: 1\n",
      "Term: live\n",
      "  DocID: 14, TF: 1\n",
      "Term: livin\n",
      "  DocID: 14, TF: 1\n",
      "Term: llamada\n",
      "  DocID: 15, TF: 1\n",
      "Term: llegaba\n",
      "  DocID: 15, TF: 1\n",
      "Term: llego\n",
      "  DocID: 13, TF: 2\n",
      "Term: llevara\n",
      "  DocID: 15, TF: 1\n",
      "Term: llevé\n",
      "  DocID: 15, TF: 2\n",
      "Term: lo\n",
      "  DocID: 13, TF: 1\n",
      "  DocID: 15, TF: 3\n",
      "Term: loca\n",
      "  DocID: 25, TF: 1\n",
      "Term: locat\n",
      "  DocID: 16, TF: 1\n",
      "Term: lone\n",
      "  DocID: 2, TF: 1\n",
      "Term: lookin\n",
      "  DocID: 3, TF: 1\n",
      "  DocID: 14, TF: 1\n",
      "Term: los\n",
      "  DocID: 15, TF: 1\n",
      "Term: loser\n",
      "  DocID: 3, TF: 1\n",
      "Term: lot\n",
      "  DocID: 16, TF: 1\n",
      "Term: love\n",
      "  DocID: 2, TF: 1\n",
      "  DocID: 3, TF: 3\n",
      "  DocID: 14, TF: 2\n",
      "  DocID: 15, TF: 1\n",
      "Term: low\n",
      "  DocID: 3, TF: 1\n",
      "Term: luna\n",
      "  DocID: 15, TF: 2\n",
      "Term: líder\n",
      "  DocID: 15, TF: 1\n",
      "Term: makin\n",
      "  DocID: 14, TF: 2\n",
      "Term: maluma\n",
      "  DocID: 15, TF: 4\n",
      "Term: mamacita\n",
      "  DocID: 15, TF: 1\n",
      "Term: mami\n",
      "  DocID: 25, TF: 2\n",
      "Term: man2\n",
      "  DocID: 27, TF: 1\n",
      "Term: mangarap\n",
      "  DocID: 1, TF: 1\n",
      "Term: marat\n",
      "  DocID: 1, TF: 1\n",
      "Term: marea\n",
      "  DocID: 13, TF: 2\n",
      "Term: mari\n",
      "  DocID: 15, TF: 1\n",
      "Term: meet\n",
      "  DocID: 3, TF: 1\n",
      "Term: mellow\n",
      "  DocID: 3, TF: 1\n",
      "Term: merchandis\n",
      "  DocID: 16, TF: 1\n",
      "Term: mga\n",
      "  DocID: 1, TF: 5\n",
      "Term: mi\n",
      "  DocID: 25, TF: 2\n",
      "  DocID: 15, TF: 3\n",
      "Term: middl\n",
      "  DocID: 14, TF: 1\n",
      "Term: midnight\n",
      "  DocID: 2, TF: 1\n",
      "Term: mind\n",
      "  DocID: 3, TF: 3\n",
      "Term: minsan\n",
      "  DocID: 1, TF: 1\n",
      "Term: misplac\n",
      "  DocID: 14, TF: 1\n",
      "Term: mobil\n",
      "  DocID: 14, TF: 1\n",
      "Term: modert\n",
      "  DocID: 13, TF: 3\n",
      "Term: mon\n",
      "  DocID: 16, TF: 1\n",
      "Term: money\n",
      "  DocID: 3, TF: 1\n",
      "  DocID: 27, TF: 2\n",
      "Term: mong\n",
      "  DocID: 1, TF: 4\n",
      "Term: mordidita\n",
      "  DocID: 13, TF: 6\n",
      "Term: morn\n",
      "  DocID: 14, TF: 1\n",
      "Term: mother\n",
      "  DocID: 14, TF: 1\n",
      "Term: motiv\n",
      "  DocID: 27, TF: 1\n",
      "Term: moto\n",
      "  DocID: 25, TF: 1\n",
      "Term: mouth\n",
      "  DocID: 14, TF: 1\n",
      "Term: movin\n",
      "  DocID: 3, TF: 1\n",
      "Term: muli\n",
      "  DocID: 1, TF: 1\n",
      "Term: mulona\n",
      "  DocID: 25, TF: 1\n",
      "Term: mundo\n",
      "  DocID: 15, TF: 1\n",
      "Term: muy\n",
      "  DocID: 25, TF: 1\n",
      "Term: má\n",
      "  DocID: 15, TF: 1\n",
      "Term: más\n",
      "  DocID: 15, TF: 8\n",
      "Term: nabitawan\n",
      "  DocID: 1, TF: 1\n",
      "Term: nada\n",
      "  DocID: 25, TF: 2\n",
      "Term: nakapagbigay\n",
      "  DocID: 1, TF: 4\n",
      "Term: nalgona\n",
      "  DocID: 25, TF: 1\n",
      "Term: nang\n",
      "  DocID: 1, TF: 1\n",
      "Term: napalingon\n",
      "  DocID: 1, TF: 1\n",
      "Term: negro\n",
      "  DocID: 14, TF: 1\n",
      "Term: netflix\n",
      "  DocID: 15, TF: 1\n",
      "Term: ngiti\n",
      "  DocID: 1, TF: 4\n",
      "Term: nigga\n",
      "  DocID: 14, TF: 1\n",
      "Term: night\n",
      "  DocID: 3, TF: 1\n",
      "Term: noch\n",
      "  DocID: 13, TF: 2\n",
      "Term: olvid\n",
      "  DocID: 15, TF: 1\n",
      "Term: oper\n",
      "  DocID: 14, TF: 1\n",
      "Term: orillita\n",
      "  DocID: 13, TF: 2\n",
      "Term: pack\n",
      "  DocID: 14, TF: 1\n",
      "Term: paid\n",
      "  DocID: 3, TF: 1\n",
      "  DocID: 27, TF: 1\n",
      "  DocID: 14, TF: 1\n",
      "Term: pangarap\n",
      "  DocID: 1, TF: 6\n",
      "Term: pansin\n",
      "  DocID: 1, TF: 2\n",
      "Term: paqu\n",
      "  DocID: 25, TF: 1\n",
      "Term: parcero\n",
      "  DocID: 15, TF: 1\n",
      "Term: park\n",
      "  DocID: 2, TF: 1\n",
      "  DocID: 3, TF: 1\n",
      "Term: parti\n",
      "  DocID: 15, TF: 1\n",
      "Term: pass\n",
      "  DocID: 26, TF: 2\n",
      "  DocID: 27, TF: 1\n",
      "Term: passion\n",
      "  DocID: 27, TF: 1\n",
      "Term: paycat\n",
      "  DocID: 27, TF: 1\n",
      "Term: peac\n",
      "  DocID: 14, TF: 1\n",
      "Term: penitentiari\n",
      "  DocID: 14, TF: 1\n",
      "Term: peopl\n",
      "  DocID: 27, TF: 1\n",
      "  DocID: 14, TF: 2\n",
      "Term: perfect\n",
      "  DocID: 27, TF: 1\n",
      "Term: perform\n",
      "  DocID: 16, TF: 1\n",
      "Term: pero\n",
      "  DocID: 25, TF: 1\n",
      "Term: phone\n",
      "  DocID: 14, TF: 1\n",
      "Term: pica\n",
      "  DocID: 13, TF: 4\n",
      "Term: pictur\n",
      "  DocID: 26, TF: 2\n",
      "Term: piec\n",
      "  DocID: 16, TF: 1\n",
      "Term: pill\n",
      "  DocID: 16, TF: 1\n",
      "Term: pimp\n",
      "  DocID: 14, TF: 1\n",
      "Term: plastic\n",
      "  DocID: 27, TF: 1\n",
      "Term: play\n",
      "  DocID: 3, TF: 1\n",
      "  DocID: 26, TF: 2\n",
      "  DocID: 14, TF: 1\n",
      "Term: poison\n",
      "  DocID: 3, TF: 21\n",
      "Term: polic\n",
      "  DocID: 14, TF: 1\n",
      "Term: poniendo\n",
      "  DocID: 15, TF: 1\n",
      "Term: poor\n",
      "  DocID: 14, TF: 1\n",
      "Term: por\n",
      "  DocID: 15, TF: 1\n",
      "Term: portion\n",
      "  DocID: 3, TF: 1\n",
      "Term: poverti\n",
      "  DocID: 14, TF: 1\n",
      "Term: power\n",
      "  DocID: 3, TF: 1\n",
      "Term: prefiero\n",
      "  DocID: 15, TF: 1\n",
      "Term: preocupacion\n",
      "  DocID: 25, TF: 2\n",
      "Term: presid\n",
      "  DocID: 14, TF: 1\n",
      "Term: pretti\n",
      "  DocID: 15, TF: 1\n",
      "  DocID: 16, TF: 1\n",
      "Term: pro\n",
      "  DocID: 3, TF: 1\n",
      "Term: probada\n",
      "  DocID: 15, TF: 1\n",
      "Term: probart\n",
      "  DocID: 15, TF: 1\n",
      "Term: pull\n",
      "  DocID: 14, TF: 1\n",
      "  DocID: 16, TF: 1\n",
      "Term: purs\n",
      "  DocID: 14, TF: 1\n",
      "Term: querían\n",
      "  DocID: 15, TF: 1\n",
      "Term: quier\n",
      "  DocID: 25, TF: 2\n",
      "Term: qué\n",
      "  DocID: 15, TF: 2\n",
      "Term: race\n",
      "  DocID: 14, TF: 1\n",
      "Term: racist\n",
      "  DocID: 14, TF: 1\n",
      "Term: rain\n",
      "  DocID: 26, TF: 4\n",
      "Term: razzamatazz\n",
      "  DocID: 16, TF: 1\n",
      "Term: readi\n",
      "  DocID: 3, TF: 4\n",
      "  DocID: 14, TF: 1\n",
      "  DocID: 16, TF: 2\n",
      "Term: real\n",
      "  DocID: 14, TF: 2\n",
      "Term: recompensaba\n",
      "  DocID: 15, TF: 1\n",
      "Term: relationship\n",
      "  DocID: 3, TF: 1\n",
      "Term: requisito\n",
      "  DocID: 15, TF: 1\n",
      "Term: resist\n",
      "  DocID: 16, TF: 1\n",
      "Term: rest\n",
      "  DocID: 3, TF: 1\n",
      "Term: reykon\n",
      "  DocID: 15, TF: 4\n",
      "Term: rico\n",
      "  DocID: 15, TF: 1\n",
      "Term: roar\n",
      "  DocID: 26, TF: 2\n",
      "Term: rock\n",
      "  DocID: 26, TF: 8\n",
      "  DocID: 16, TF: 1\n",
      "Term: rockin\n",
      "  DocID: 26, TF: 4\n",
      "Term: roll\n",
      "  DocID: 16, TF: 1\n",
      "Term: ron\n",
      "  DocID: 3, TF: 1\n",
      "Term: rrrrah\n",
      "  DocID: 25, TF: 1\n",
      "Term: run\n",
      "  DocID: 3, TF: 1\n",
      "Term: rush\n",
      "  DocID: 14, TF: 1\n",
      "Term: sabe\n",
      "  DocID: 15, TF: 1\n",
      "Term: sale\n",
      "  DocID: 25, TF: 1\n",
      "Term: salir\n",
      "  DocID: 15, TF: 3\n",
      "Term: sana\n",
      "  DocID: 1, TF: 1\n",
      "Term: sandali\n",
      "  DocID: 1, TF: 2\n",
      "Term: sayin\n",
      "  DocID: 3, TF: 1\n",
      "Term: sayo\n",
      "  DocID: 1, TF: 1\n",
      "Term: scene\n",
      "  DocID: 26, TF: 2\n",
      "Term: schemin\n",
      "  DocID: 3, TF: 2\n",
      "Term: scream\n",
      "  DocID: 26, TF: 2\n",
      "Term: screamin\n",
      "  DocID: 3, TF: 1\n",
      "Term: secondhand\n",
      "  DocID: 26, TF: 2\n",
      "Term: secret\n",
      "  DocID: 14, TF: 1\n",
      "Term: sell\n",
      "  DocID: 16, TF: 1\n",
      "Term: sellin\n",
      "  DocID: 14, TF: 1\n",
      "Term: sellout\n",
      "  DocID: 16, TF: 1\n",
      "Term: sens\n",
      "  DocID: 3, TF: 1\n",
      "Term: sepa\n",
      "  DocID: 15, TF: 1\n",
      "Term: shadow\n",
      "  DocID: 26, TF: 2\n",
      "Term: shake\n",
      "  DocID: 3, TF: 1\n",
      "Term: share\n",
      "  DocID: 14, TF: 1\n",
      "Term: ship\n",
      "  DocID: 14, TF: 1\n",
      "Term: shot\n",
      "  DocID: 27, TF: 1\n",
      "  DocID: 14, TF: 1\n",
      "Term: sient\n",
      "  DocID: 15, TF: 1\n",
      "Term: sientat\n",
      "  DocID: 25, TF: 1\n",
      "Term: sign\n",
      "  DocID: 2, TF: 1\n",
      "Term: sin\n",
      "  DocID: 25, TF: 2\n",
      "  DocID: 15, TF: 1\n",
      "Term: sing\n",
      "  DocID: 2, TF: 1\n",
      "Term: situat\n",
      "  DocID: 3, TF: 1\n",
      "Term: skill\n",
      "  DocID: 14, TF: 1\n",
      "Term: sky\n",
      "  DocID: 2, TF: 1\n",
      "Term: sleazi\n",
      "  DocID: 14, TF: 1\n",
      "Term: slick\n",
      "  DocID: 3, TF: 1\n",
      "Term: slow\n",
      "  DocID: 3, TF: 1\n",
      "  DocID: 27, TF: 1\n",
      "Term: slung\n",
      "  DocID: 26, TF: 2\n",
      "Term: smack\n",
      "  DocID: 14, TF: 1\n",
      "Term: smile\n",
      "  DocID: 2, TF: 3\n",
      "  DocID: 3, TF: 1\n",
      "Term: smokin\n",
      "  DocID: 14, TF: 1\n",
      "Term: snatch\n",
      "  DocID: 14, TF: 1\n",
      "Term: soft\n",
      "  DocID: 2, TF: 2\n",
      "Term: sold\n",
      "  DocID: 26, TF: 2\n",
      "Term: sound\n",
      "  DocID: 14, TF: 1\n",
      "  DocID: 16, TF: 1\n",
      "Term: soy\n",
      "  DocID: 15, TF: 1\n",
      "Term: spyderman\n",
      "  DocID: 3, TF: 1\n",
      "Term: stand\n",
      "  DocID: 26, TF: 2\n",
      "Term: star\n",
      "  DocID: 26, TF: 20\n",
      "Term: start\n",
      "  DocID: 3, TF: 2\n",
      "  DocID: 26, TF: 2\n",
      "  DocID: 14, TF: 2\n",
      "Term: stayin\n",
      "  DocID: 14, TF: 1\n",
      "Term: steal\n",
      "  DocID: 3, TF: 1\n",
      "Term: step\n",
      "  DocID: 14, TF: 1\n",
      "Term: stickel\n",
      "  DocID: 16, TF: 1\n",
      "Term: stomach\n",
      "  DocID: 14, TF: 1\n",
      "Term: stood\n",
      "  DocID: 3, TF: 1\n",
      "Term: store\n",
      "  DocID: 26, TF: 2\n",
      "Term: stove\n",
      "  DocID: 27, TF: 1\n",
      "Term: strang\n",
      "  DocID: 3, TF: 1\n",
      "Term: stranger\n",
      "  DocID: 14, TF: 1\n",
      "Term: street\n",
      "  DocID: 14, TF: 1\n",
      "Term: string\n",
      "  DocID: 26, TF: 2\n",
      "Term: style\n",
      "  DocID: 16, TF: 1\n",
      "Term: sun\n",
      "  DocID: 2, TF: 1\n",
      "Term: suppos\n",
      "  DocID: 14, TF: 1\n",
      "Term: surviv\n",
      "  DocID: 14, TF: 1\n",
      "Term: sólo\n",
      "  DocID: 15, TF: 1\n",
      "Term: ta\n",
      "  DocID: 26, TF: 6\n",
      "  DocID: 14, TF: 6\n",
      "Term: take\n",
      "  DocID: 14, TF: 2\n",
      "Term: takin\n",
      "  DocID: 3, TF: 1\n",
      "Term: tanong\n",
      "  DocID: 1, TF: 1\n",
      "Term: te\n",
      "  DocID: 25, TF: 3\n",
      "  DocID: 13, TF: 1\n",
      "  DocID: 15, TF: 6\n",
      "Term: tenso\n",
      "  DocID: 15, TF: 1\n",
      "Term: thrill\n",
      "  DocID: 16, TF: 1\n",
      "Term: ti\n",
      "  DocID: 15, TF: 1\n",
      "Term: ticket\n",
      "  DocID: 26, TF: 4\n",
      "Term: tien\n",
      "  DocID: 25, TF: 1\n",
      "Term: time\n",
      "  DocID: 3, TF: 1\n",
      "  DocID: 14, TF: 4\n",
      "Term: tingin\n",
      "  DocID: 1, TF: 1\n",
      "Term: tinig\n",
      "  DocID: 1, TF: 4\n",
      "Term: tire\n",
      "  DocID: 14, TF: 1\n",
      "Term: toda\n",
      "  DocID: 13, TF: 2\n",
      "Term: todito\n",
      "  DocID: 13, TF: 2\n",
      "Term: todo\n",
      "  DocID: 25, TF: 2\n",
      "  DocID: 15, TF: 3\n",
      "Term: tom\n",
      "  DocID: 15, TF: 4\n",
      "Term: tonight\n",
      "  DocID: 26, TF: 4\n",
      "  DocID: 14, TF: 1\n",
      "Term: totoo\n",
      "  DocID: 1, TF: 1\n",
      "Term: touch\n",
      "  DocID: 14, TF: 1\n",
      "Term: tour\n",
      "  DocID: 16, TF: 1\n",
      "Term: town\n",
      "  DocID: 26, TF: 2\n",
      "Term: trago\n",
      "  DocID: 25, TF: 1\n",
      "Term: treat\n",
      "  DocID: 14, TF: 1\n",
      "Term: tree\n",
      "  DocID: 2, TF: 2\n",
      "Term: tremenda\n",
      "  DocID: 15, TF: 1\n",
      "Term: tremendo\n",
      "  DocID: 25, TF: 2\n",
      "Term: trigger\n",
      "  DocID: 14, TF: 1\n",
      "Term: trip\n",
      "  DocID: 26, TF: 2\n",
      "Term: trust\n",
      "  DocID: 3, TF: 1\n",
      "  DocID: 14, TF: 1\n",
      "Term: tu\n",
      "  DocID: 25, TF: 3\n",
      "  DocID: 13, TF: 3\n",
      "  DocID: 15, TF: 3\n",
      "Term: tus\n",
      "  DocID: 15, TF: 1\n",
      "Term: tutugon\n",
      "  DocID: 1, TF: 1\n",
      "Term: tú\n",
      "  DocID: 15, TF: 6\n",
      "Term: uh\n",
      "  DocID: 3, TF: 1\n",
      "  DocID: 15, TF: 6\n",
      "Term: ulit\n",
      "  DocID: 1, TF: 1\n",
      "Term: una\n",
      "  DocID: 25, TF: 1\n",
      "  DocID: 13, TF: 6\n",
      "  DocID: 15, TF: 1\n",
      "Term: understand\n",
      "  DocID: 26, TF: 2\n",
      "Term: vacat\n",
      "  DocID: 27, TF: 1\n",
      "Term: vacilon\n",
      "  DocID: 25, TF: 2\n",
      "Term: vamo\n",
      "  DocID: 25, TF: 4\n",
      "  DocID: 13, TF: 2\n",
      "Term: vampiro\n",
      "  DocID: 13, TF: 1\n",
      "Term: vea\n",
      "  DocID: 25, TF: 1\n",
      "Term: veo\n",
      "  DocID: 25, TF: 6\n",
      "Term: vida\n",
      "  DocID: 25, TF: 4\n",
      "Term: vien\n",
      "  DocID: 25, TF: 1\n",
      "Term: viendo\n",
      "  DocID: 15, TF: 1\n",
      "Term: vivir\n",
      "  DocID: 25, TF: 4\n",
      "Term: voto\n",
      "  DocID: 25, TF: 1\n",
      "Term: vuelto\n",
      "  DocID: 15, TF: 1\n",
      "Term: wake\n",
      "  DocID: 14, TF: 1\n",
      "Term: wall\n",
      "  DocID: 3, TF: 1\n",
      "  DocID: 26, TF: 2\n",
      "Term: war\n",
      "  DocID: 14, TF: 4\n",
      "Term: warn\n",
      "  DocID: 3, TF: 1\n",
      "Term: wast\n",
      "  DocID: 14, TF: 1\n",
      "Term: watch\n",
      "  DocID: 14, TF: 1\n",
      "Term: welfar\n",
      "  DocID: 14, TF: 1\n",
      "Term: white\n",
      "  DocID: 14, TF: 1\n",
      "Term: wind\n",
      "  DocID: 2, TF: 1\n",
      "Term: winner\n",
      "  DocID: 3, TF: 1\n",
      "Term: woah\n",
      "  DocID: 26, TF: 2\n",
      "Term: workin\n",
      "  DocID: 14, TF: 1\n",
      "Term: wors\n",
      "  DocID: 14, TF: 1\n",
      "Term: worth\n",
      "  DocID: 14, TF: 1\n",
      "Term: wow\n",
      "  DocID: 15, TF: 1\n",
      "Term: wrong\n",
      "  DocID: 3, TF: 1\n",
      "Term: wuoh\n",
      "  DocID: 15, TF: 1\n",
      "Term: yata\n",
      "  DocID: 1, TF: 1\n",
      "Term: yeah\n",
      "  DocID: 3, TF: 2\n",
      "  DocID: 25, TF: 1\n",
      "  DocID: 26, TF: 4\n",
      "  DocID: 27, TF: 1\n",
      "  DocID: 14, TF: 4\n",
      "Term: yo\n",
      "  DocID: 25, TF: 4\n",
      "  DocID: 15, TF: 2\n",
      "Term: zona\n",
      "  DocID: 15, TF: 1\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pickle\n",
    "\n",
    "def print_full_index(index_dir):\n",
    "    full_index = {}\n",
    "\n",
    "    # Recorremos los archivos de los bloques fusionados en la carpeta\n",
    "    files = sorted(os.listdir(index_dir))\n",
    "\n",
    "    for file in files:\n",
    "        if file.endswith(\".pkl\"):  # Asegurarnos de que es un archivo pickle\n",
    "            block_path = os.path.join(index_dir, file)\n",
    "\n",
    "            # Abrimos el archivo pickle y cargamos el bloque\n",
    "            with open(block_path, \"rb\") as f:\n",
    "                block = pickle.load(f)\n",
    "                \n",
    "                # Fusionamos los términos de este bloque con el índice completo\n",
    "                for term, postings in block.items():\n",
    "                    if term not in full_index:\n",
    "                        full_index[term] = postings\n",
    "                    else:\n",
    "                        # Si el término ya existe, combinamos las frecuencias de los docID\n",
    "                        for doc_id, freq in postings.items():\n",
    "                            if doc_id in full_index[term]:\n",
    "                                full_index[term][doc_id] += freq\n",
    "                            else:\n",
    "                                full_index[term][doc_id] = freq\n",
    "\n",
    "    # Ahora 'full_index' contiene todos los términos y sus frecuencias\n",
    "    # Puedes imprimirlo o retornar el diccionario completo\n",
    "    for term, postings in full_index.items():\n",
    "        print(f\"Term: {term}\")\n",
    "        for doc_id, freq in postings.items():\n",
    "            print(f\"  DocID: {doc_id}, TF: {freq}\")\n",
    "\n",
    "    return full_index\n",
    "\n",
    "full_index = print_full_index(s.index_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "El índice está ordenado en términos.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pickle\n",
    "\n",
    "def print_full_index_and_check_order(index_dir):\n",
    "    last_term = None  # Variable para verificar el orden de términos\n",
    "\n",
    "    # Recorremos los archivos de los bloques fusionados en la carpeta\n",
    "    files = sorted(os.listdir(index_dir))\n",
    "    for file in files:\n",
    "        if file.endswith(\".pkl\"):  # Asegurarnos de que es un archivo pickle\n",
    "            print(file) \n",
    "\n",
    "            block_path = os.path.join(index_dir, file)\n",
    "\n",
    "            # Abrimos el archivo pickle y cargamos el bloque\n",
    "            with open(block_path, \"rb\") as f:\n",
    "                block = pickle.load(f)\n",
    "                \n",
    "                # Verificar que los términos están en orden alfabético\n",
    "                for term in block.keys():\n",
    "                    if last_term is not None and term < last_term:\n",
    "                        print(\"El índice no está ordenado en términos.\")\n",
    "                        print(f\"Error: '{term}' aparece después de '{last_term}'\")\n",
    "                        return\n",
    "\n",
    "                    last_term = term  # Actualizar `last_term` para la siguiente comparación\n",
    "\n",
    "    print(\"El índice está ordenado en términos.\")\n",
    "\n",
    "# Ejecuta la función\n",
    "print_full_index_and_check_order(\"index_blocks\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
