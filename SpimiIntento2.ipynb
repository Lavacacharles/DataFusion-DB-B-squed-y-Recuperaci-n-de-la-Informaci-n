{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Proyecto jiji"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: kagglehub in c:\\users\\ce mar\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (0.3.4)\n",
      "Requirement already satisfied: packaging in c:\\users\\ce mar\\appdata\\roaming\\python\\python310\\site-packages (from kagglehub) (23.2)\n",
      "Requirement already satisfied: tqdm in c:\\users\\ce mar\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from kagglehub) (4.66.1)\n",
      "Requirement already satisfied: requests in c:\\users\\ce mar\\appdata\\roaming\\python\\python310\\site-packages (from kagglehub) (2.28.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\ce mar\\appdata\\roaming\\python\\python310\\site-packages (from requests->kagglehub) (3.4)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\ce mar\\appdata\\roaming\\python\\python310\\site-packages (from requests->kagglehub) (1.26.13)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in c:\\users\\ce mar\\appdata\\roaming\\python\\python310\\site-packages (from requests->kagglehub) (2.1.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\ce mar\\appdata\\roaming\\python\\python310\\site-packages (from requests->kagglehub) (2022.12.7)\n",
      "Requirement already satisfied: colorama in c:\\users\\ce mar\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from tqdm->kagglehub) (0.4.6)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip available: 22.2.1 -> 24.3.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "!pip install kagglehub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Ce mar\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\scipy\\__init__.py:177: UserWarning: A NumPy version >=1.18.5 and <1.26.0 is required for this version of SciPy (detected version 1.26.2\n",
      "  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n",
      "[nltk_data] Downloading package punkt to C:\\Users\\Ce\n",
      "[nltk_data]     mar\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "import numpy as np\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "nltk.download('punkt')\n",
    "import regex as re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "stemmer = SnowballStemmer('english')\n",
    "import os\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Ce mar\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Path to dataset files: C:\\Users\\Ce mar\\.cache\\kagglehub\\datasets\\imuhammad\\audio-features-and-lyrics-of-spotify-songs\\versions\\1\n"
     ]
    }
   ],
   "source": [
    "import kagglehub\n",
    "path = kagglehub.dataset_download(\"imuhammad/audio-features-and-lyrics-of-spotify-songs\")\n",
    "print(\"Path to dataset files:\", path)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"stopwords-en.txt\", encoding=\"latin1\") as file:\n",
    "    stoplist = [line.rstrip().lower() for line in file]\n",
    "\n",
    "def preprocesamiento(texto, stemming=True):\n",
    "  words = []\n",
    "  texto = texto.lower()\n",
    "  texto = re.sub(r'[^a-zA-Z0-9_À-ÿ]', ' ', texto)\n",
    "  # tokenizar\n",
    "  words = nltk.word_tokenize(texto, language='spanish')\n",
    "  # filtrar stopwords\n",
    "  words = [word for word in words if word not in stoplist]\n",
    "  # reducir palabras (stemming)\n",
    "  if stemming:\n",
    "      words = [stemmer.stem(word) for word in words]\n",
    "  return words\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Ce mar\\.cache\\kagglehub\\datasets\\imuhammad\\audio-features-and-lyrics-of-spotify-songs\\versions\\1\\spotify_songs.csv\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>15</th>\n",
       "      <th>16</th>\n",
       "      <th>17</th>\n",
       "      <th>18</th>\n",
       "      <th>19</th>\n",
       "      <th>20</th>\n",
       "      <th>21</th>\n",
       "      <th>22</th>\n",
       "      <th>23</th>\n",
       "      <th>24</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>track_id</td>\n",
       "      <td>track_name</td>\n",
       "      <td>track_artist</td>\n",
       "      <td>lyrics</td>\n",
       "      <td>track_popularity</td>\n",
       "      <td>track_album_id</td>\n",
       "      <td>track_album_name</td>\n",
       "      <td>track_album_release_date</td>\n",
       "      <td>playlist_name</td>\n",
       "      <td>playlist_id</td>\n",
       "      <td>...</td>\n",
       "      <td>loudness</td>\n",
       "      <td>mode</td>\n",
       "      <td>speechiness</td>\n",
       "      <td>acousticness</td>\n",
       "      <td>instrumentalness</td>\n",
       "      <td>liveness</td>\n",
       "      <td>valence</td>\n",
       "      <td>tempo</td>\n",
       "      <td>duration_ms</td>\n",
       "      <td>language</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0017A6SJgTbfQVU2EtsPNo</td>\n",
       "      <td>Pangarap</td>\n",
       "      <td>Barbie's Cradle</td>\n",
       "      <td>Minsan pa Nang ako'y napalingon Hindi ko alam ...</td>\n",
       "      <td>41</td>\n",
       "      <td>1srJQ0njEQgd8w4XSqI4JQ</td>\n",
       "      <td>Trip</td>\n",
       "      <td>2001-01-01</td>\n",
       "      <td>Pinoy Classic Rock</td>\n",
       "      <td>37i9dQZF1DWYDQ8wBxd7xt</td>\n",
       "      <td>...</td>\n",
       "      <td>-10.068</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0236</td>\n",
       "      <td>0.279</td>\n",
       "      <td>0.0117</td>\n",
       "      <td>0.0887</td>\n",
       "      <td>0.5660000000000001</td>\n",
       "      <td>97.091</td>\n",
       "      <td>235440</td>\n",
       "      <td>tl</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>004s3t0ONYlzxII9PLgU6z</td>\n",
       "      <td>I Feel Alive</td>\n",
       "      <td>Steady Rollin</td>\n",
       "      <td>The trees, are singing in the wind The sky blu...</td>\n",
       "      <td>28</td>\n",
       "      <td>3z04Lb9Dsilqw68SHt6jLB</td>\n",
       "      <td>Love &amp; Loss</td>\n",
       "      <td>2017-11-21</td>\n",
       "      <td>Hard Rock Workout</td>\n",
       "      <td>3YouF0u7waJnolytf9JCXf</td>\n",
       "      <td>...</td>\n",
       "      <td>-4.739</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0442</td>\n",
       "      <td>0.0117</td>\n",
       "      <td>0.00994</td>\n",
       "      <td>0.34700000000000003</td>\n",
       "      <td>0.404</td>\n",
       "      <td>135.225</td>\n",
       "      <td>373512</td>\n",
       "      <td>en</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>00chLpzhgVjxs1zKC9UScL</td>\n",
       "      <td>Poison</td>\n",
       "      <td>Bell Biv DeVoe</td>\n",
       "      <td>NA Yeah, Spyderman and Freeze in full effect U...</td>\n",
       "      <td>0</td>\n",
       "      <td>6oZ6brjB8x3GoeSYdwJdPc</td>\n",
       "      <td>Gold</td>\n",
       "      <td>2005-01-01</td>\n",
       "      <td>Back in the day - R&amp;B, New Jack Swing, Swingbe...</td>\n",
       "      <td>3a9y4eeCJRmG9p4YKfqYIx</td>\n",
       "      <td>...</td>\n",
       "      <td>-7.504</td>\n",
       "      <td>0</td>\n",
       "      <td>0.21600000000000005</td>\n",
       "      <td>0.00432</td>\n",
       "      <td>0.007229999999999999</td>\n",
       "      <td>0.489</td>\n",
       "      <td>0.65</td>\n",
       "      <td>111.904</td>\n",
       "      <td>262467</td>\n",
       "      <td>en</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>00cqd6ZsSkLZqGMlQCR0Zo</td>\n",
       "      <td>Baby It's Cold Outside (feat. Christina Aguilera)</td>\n",
       "      <td>CeeLo Green</td>\n",
       "      <td>I really can't stay Baby it's cold outside I'v...</td>\n",
       "      <td>41</td>\n",
       "      <td>3ssspRe42CXkhPxdc12xcp</td>\n",
       "      <td>CeeLo's Magic Moment</td>\n",
       "      <td>2012-10-29</td>\n",
       "      <td>Christmas Soul</td>\n",
       "      <td>6FZYc2BvF7tColxO8PBShV</td>\n",
       "      <td>...</td>\n",
       "      <td>-5.819</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0341</td>\n",
       "      <td>0.6890000000000001</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0664</td>\n",
       "      <td>0.405</td>\n",
       "      <td>118.593</td>\n",
       "      <td>243067</td>\n",
       "      <td>en</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 25 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                       0                                                  1   \\\n",
       "0                track_id                                         track_name   \n",
       "1  0017A6SJgTbfQVU2EtsPNo                                           Pangarap   \n",
       "2  004s3t0ONYlzxII9PLgU6z                                       I Feel Alive   \n",
       "3  00chLpzhgVjxs1zKC9UScL                                             Poison   \n",
       "4  00cqd6ZsSkLZqGMlQCR0Zo  Baby It's Cold Outside (feat. Christina Aguilera)   \n",
       "\n",
       "                2                                                  3   \\\n",
       "0     track_artist                                             lyrics   \n",
       "1  Barbie's Cradle  Minsan pa Nang ako'y napalingon Hindi ko alam ...   \n",
       "2    Steady Rollin  The trees, are singing in the wind The sky blu...   \n",
       "3   Bell Biv DeVoe  NA Yeah, Spyderman and Freeze in full effect U...   \n",
       "4      CeeLo Green  I really can't stay Baby it's cold outside I'v...   \n",
       "\n",
       "                 4                       5                     6   \\\n",
       "0  track_popularity          track_album_id      track_album_name   \n",
       "1                41  1srJQ0njEQgd8w4XSqI4JQ                  Trip   \n",
       "2                28  3z04Lb9Dsilqw68SHt6jLB           Love & Loss   \n",
       "3                 0  6oZ6brjB8x3GoeSYdwJdPc                  Gold   \n",
       "4                41  3ssspRe42CXkhPxdc12xcp  CeeLo's Magic Moment   \n",
       "\n",
       "                         7   \\\n",
       "0  track_album_release_date   \n",
       "1                2001-01-01   \n",
       "2                2017-11-21   \n",
       "3                2005-01-01   \n",
       "4                2012-10-29   \n",
       "\n",
       "                                                  8                       9   \\\n",
       "0                                      playlist_name             playlist_id   \n",
       "1                                 Pinoy Classic Rock  37i9dQZF1DWYDQ8wBxd7xt   \n",
       "2                                  Hard Rock Workout  3YouF0u7waJnolytf9JCXf   \n",
       "3  Back in the day - R&B, New Jack Swing, Swingbe...  3a9y4eeCJRmG9p4YKfqYIx   \n",
       "4                                     Christmas Soul  6FZYc2BvF7tColxO8PBShV   \n",
       "\n",
       "   ...        15    16                   17                  18  \\\n",
       "0  ...  loudness  mode          speechiness        acousticness   \n",
       "1  ...   -10.068     1               0.0236               0.279   \n",
       "2  ...    -4.739     1               0.0442              0.0117   \n",
       "3  ...    -7.504     0  0.21600000000000005             0.00432   \n",
       "4  ...    -5.819     0               0.0341  0.6890000000000001   \n",
       "\n",
       "                     19                   20                  21       22  \\\n",
       "0      instrumentalness             liveness             valence    tempo   \n",
       "1                0.0117               0.0887  0.5660000000000001   97.091   \n",
       "2               0.00994  0.34700000000000003               0.404  135.225   \n",
       "3  0.007229999999999999                0.489                0.65  111.904   \n",
       "4                     0               0.0664               0.405  118.593   \n",
       "\n",
       "            23        24  \n",
       "0  duration_ms  language  \n",
       "1       235440        tl  \n",
       "2       373512        en  \n",
       "3       262467        en  \n",
       "4       243067        en  \n",
       "\n",
       "[5 rows x 25 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lista_ = os.listdir(path)\n",
    "songs = os.path.join(path, lista_[0])\n",
    "print(songs)\n",
    "dataset = pd.read_csv(songs, header=None)\n",
    "dataset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I really can't stay Baby it's cold outside I've got to go away Baby it's cold out there This evening has been Been hoping that you'd drop in So very nice I'll hold your hands, they're just like ice My mother will start to worry Beautiful, what's your hurry? My father will be pacing the floor Listen to that fireplace roar So really I'd better scurry Beautiful, please don't hurry Well maybe just a half a drink more Why don't you put some records on while I pour The neighbors might think Baby, it's bad out there Say, what's in this drink? No cabs to be had out there I wish I knew how Your eyes are like starlight To break the spell I'll take your hat, your hair looks swell I ought to say no, no, no, sir Mind if I move in closer? At least I'm gonna say that I tried What's the sense in hurting my pride? I really can't stay Baby don't hold out Baby it's cold outside I simply must go See that it's cold outside The answer is no I said it's cold out there This welcome has been How lucky that you dropped in So nice and warm Look out the window at that storm My sister will be suspicious Gosh, your lips look delicious My brother will be there at the door Waves upon a tropical shore My maiden aunt's mind is vicious Oh, your lips are delicious Maybe just a cigarette more Never such a blizzard before Hey I've got to go home Baby, you'll freeze out there Say, lend me your coat It's up to your knees out there You've really been grand I'm thrilled when you touch my hand But don't you see How can you do this thing to me? There's bound to be talk tomorrow Think of my life long sorrow At least there will be plenty implied If you caught pneumonia and died I really can't stay Get over that old lie Baby, baby it's cold outside\n"
     ]
    }
   ],
   "source": [
    "fila_5 = dataset.iloc[4, 3]\n",
    "print(fila_5)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import pickle\n",
    "\n",
    "class InvertIndex:\n",
    "    def __init__(self, index_file):\n",
    "        self.index_file = index_file\n",
    "        self.index = {}\n",
    "        self.idf = {}\n",
    "        self.length = {}\n",
    "\n",
    "    def building(self, collection_text, position_text):\n",
    "        total_docs = len(collection_text)\n",
    "        doc_count = {}\n",
    "\n",
    "        # build the inverted index with the collection\n",
    "        for i, row in collection_text.iterrows():\n",
    "          doc_id = i\n",
    "          doc = row.iloc[position_text]\n",
    "          keywords = preprocesamiento(doc)\n",
    "        # compute the tf\n",
    "          tf_per_doc = {}\n",
    "\n",
    "          for keyword in keywords:\n",
    "            if keyword in tf_per_doc:\n",
    "              tf_per_doc[keyword] += 1\n",
    "            else:\n",
    "              tf_per_doc[keyword] = 1\n",
    "          for term in tf_per_doc:\n",
    "            tf_per_doc[term] = math.log10(1 + tf_per_doc[term])\n",
    "\n",
    "        # compute the idf\n",
    "          for term, tf in tf_per_doc.items():\n",
    "            if term not in self.index:\n",
    "                self.index[term] = []\n",
    "            self.index[term].append((doc_id, tf))\n",
    "\n",
    "            if term in doc_count:\n",
    "                doc_count[term] += 1\n",
    "            else:\n",
    "                doc_count[term] = 1\n",
    "\n",
    "        for term, count in doc_count.items():\n",
    "            self.idf[term] = math.log(total_docs / (count))\n",
    "        # compute the length (norm)\n",
    "        for doc_id in range(total_docs):\n",
    "            norm = 0\n",
    "            for term, postings in self.index.items():\n",
    "                for doc, tf in postings:\n",
    "                    if doc == doc_id:\n",
    "                        norm += (tf * self.idf[term]) ** 2\n",
    "            self.length[doc_id] = math.sqrt(norm) #Get sum(tf*idf^2)\n",
    "        # store in disk\n",
    "        with open(self.index_file, 'wb') as f:\n",
    "            pickle.dump((self.index, self.idf, self.length), f)\n",
    "\n",
    "    def retrieval(self, query, k):\n",
    "        self.load_index()\n",
    "        N = len(self.length)\n",
    "        scores = [0] * len(self.length)\n",
    "        # preprocesar la query: extraer los terminos unicos\n",
    "        terms = preprocesamiento(query)\n",
    "        # calcular el tf-idf del query\n",
    "        tf_query = {}\n",
    "        for term in terms:\n",
    "            if term in tf_query:\n",
    "                tf_query[term] += 1\n",
    "            else:\n",
    "                tf_query[term] = 1\n",
    "        tfidf_query = {}\n",
    "        for term, tf in tf_query.items():   \n",
    "            if term in self.idf:\n",
    "                tfidf_query[term] = math.log10(1 + tf) * self.idf[term]\n",
    "\n",
    "        norm_query = math.sqrt(sum(w_tq**2 for w_tq in tfidf_query.values()))\n",
    "\n",
    "        # aplicar similitud de coseno y guardarlo en el diccionario score\n",
    "        for term, w_tq in tfidf_query.items():\n",
    "            if term in self.index:\n",
    "                for doc, tf_td in self.index[term]:\n",
    "                    w_td = tf_td * self.idf[term] #Calculo tf_idf para wt,d\n",
    "                    scores[doc] += w_td * w_tq\n",
    "                    \n",
    "        for d in range(N):\n",
    "            if self.length[d] != 0:\n",
    "                scores[d] /= self.length[d]*norm_query  # Normalización del documento y la consulta\n",
    "    \n",
    "        # ordenar el score de forma descendente\n",
    "        result = sorted(enumerate(scores), key=lambda x: x[1], reverse=True)\n",
    "        # retornamos los k documentos mas relevantes (de mayor similitud al query)\n",
    "        return result[:k]\n",
    "\n",
    "    def load_index(self):\n",
    "        # load index from disk\n",
    "        with open(self.index_file, 'rb') as f:\n",
    "            self.index, self.idf, self.length = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset = dataset.head(40)\n",
    "\n",
    "# def mostrarDocumentos(result):\n",
    "#     for doc, score in result:\n",
    "#         print(\"Documento\", doc, \" con similitud: \", score)\n",
    "#     print(\"_____\")\n",
    "\n",
    "# index = InvertIndex(\"indice.dat\")\n",
    "# index.building(dataset, 3) #El texto a procesar esta en la posicion 1\n",
    "\n",
    "# Query1 = \"I really can't stay Baby it's cold outside \"\n",
    "# result = index.retrieval(Query1, 10)\n",
    "# print(\"Resultados para\", Query1)\n",
    "# mostrarDocumentos(result)\n",
    "\n",
    "# news_value = dataset.iloc[4, 3] # Sacamos el primer doc el cual nos deberia de dar 1\n",
    "# Query2 = news_value\n",
    "# result2 = index.retrieval(Query2, 10)\n",
    "# print(\"Resultados para\", Query2)\n",
    "# mostrarDocumentos(result2)\n",
    "\n",
    "# Query3 = dataset.iloc[7, 3]\n",
    "# result3 = index.retrieval(Query3, 10)\n",
    "# print(\"Resultados para\", Query3)\n",
    "# mostrarDocumentos(result3)\n",
    "\n",
    "# Query3 = dataset.iloc[20, 3]\n",
    "# result3 = index.retrieval(Query3, 10)\n",
    "# print(\"Resultados para\", Query3)\n",
    "# mostrarDocumentos(result3)\n",
    "\n",
    "# Query3 = dataset.iloc[19, 3]\n",
    "# result3 = index.retrieval(Query3, 10)\n",
    "# print(\"Resultados para\", Query3)\n",
    "# mostrarDocumentos(result3)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Index</th>\n",
       "      <th>ProcessedText</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>minsan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>nang</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>ako</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>napalingon</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>hindi</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6906</th>\n",
       "      <td>39</td>\n",
       "      <td>town</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6907</th>\n",
       "      <td>39</td>\n",
       "      <td>santa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6908</th>\n",
       "      <td>39</td>\n",
       "      <td>claus</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6909</th>\n",
       "      <td>39</td>\n",
       "      <td>comin</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6910</th>\n",
       "      <td>39</td>\n",
       "      <td>town</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>6911 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      Index ProcessedText\n",
       "0         1        minsan\n",
       "1         1          nang\n",
       "2         1           ako\n",
       "3         1    napalingon\n",
       "4         1         hindi\n",
       "...     ...           ...\n",
       "6906     39          town\n",
       "6907     39         santa\n",
       "6908     39         claus\n",
       "6909     39         comin\n",
       "6910     39          town\n",
       "\n",
       "[6911 rows x 2 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = dataset.head(40)\n",
    "#Procesar los documentos en pares\n",
    "position_text=3\n",
    "pairs = []\n",
    "\n",
    "for i, row in dataset.iterrows():\n",
    "    if(i!=0):\n",
    "        words = preprocesamiento(row.iloc[position_text])\n",
    "        for text in words:\n",
    "            pairs.append((i, text))\n",
    "\n",
    "pairs_df = pd.DataFrame(pairs, columns=['Index', 'ProcessedText'])\n",
    "pairs_df.head(len(pairs_df))\n",
    "\n",
    "\n",
    "# print(pairs_df[pairs_df[\"ProcessedText\"] == \"6\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "La cantidad de niveles serán 6\n",
      "Cantidad de bloques generados 46\n",
      "------------------------------------\n",
      "Comenzando merge en el nivel 0\n",
      "------------------------------------\n",
      "Comenzando merge en el nivel 1\n",
      "------------------------------------\n",
      "Comenzando merge en el nivel 2\n",
      "------------------------------------\n",
      "Comenzando merge en el nivel 3\n",
      "------------------------------------\n",
      "Comenzando merge en el nivel 4\n",
      "------------------------------------\n",
      "Comenzando merge en el nivel 5\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pickle\n",
    "import sys\n",
    "import math\n",
    "\n",
    "class SPIMI:\n",
    "    def __init__(self, index_dir=\"index_blocks\"):\n",
    "        self.index_dir = index_dir  \n",
    "        self.block_counter = 0      \n",
    "        self.doc_count = 0          \n",
    "        self.idf = {}\n",
    "        self.length = {}\n",
    "        self.disk_limit = 4000  \n",
    "        self.totalLevels = 0 \n",
    "        self.currentL = 0\n",
    "        self.cuntermerged = 0 \n",
    "\n",
    "        if not os.path.exists(self.index_dir):\n",
    "            os.makedirs(self.index_dir)\n",
    "\n",
    "    def spimi_invert(self, token_stream):\n",
    "        dictionary = {}\n",
    "        for _, row in token_stream.iterrows():\n",
    "            doc_id = row['Index']\n",
    "            token = row['ProcessedText']\n",
    "            \n",
    "            if token not in dictionary:\n",
    "                dictionary[token] = {}  \n",
    "            \n",
    "            if doc_id not in dictionary[token]:\n",
    "                dictionary[token][doc_id] = 1  \n",
    "            else:\n",
    "                dictionary[token][doc_id] += 1  \n",
    "            dictionary_size = sys.getsizeof(dictionary)\n",
    "            if dictionary_size >= self.disk_limit:\n",
    "                self.write_block_to_disk(dictionary, level=0)\n",
    "                dictionary.clear()\n",
    "                \n",
    "        if dictionary:\n",
    "            self.write_block_to_disk(dictionary, level=0)\n",
    "\n",
    "        self.totalLevels = math.ceil(math.log2(self.block_counter))\n",
    "        print(\"La cantidad de niveles serán\", self.totalLevels)\n",
    "        print(\"Cantidad de bloques generados\", self.block_counter)\n",
    "        \n",
    "    def write_block_to_disk(self, dictionary, level):\n",
    "        sorted_terms = dict(sorted(dictionary.items())) \n",
    "        file_path = os.path.join(self.index_dir, f\"block_nivel{level}_bloque{self.block_counter}.pkl\")\n",
    "        with open(file_path, \"wb\") as f:\n",
    "            pickle.dump(sorted_terms, f)\n",
    "        \n",
    "        self.block_counter += 1\n",
    "\n",
    "    def merge(self):\n",
    "        files = sorted(os.listdir(self.index_dir))\n",
    "        \n",
    "        # Iteramos sobre los niveles de mergeo\n",
    "        for level in range(self.totalLevels):\n",
    "            print(\"------------------------------------\")\n",
    "            print(f\"Comenzando merge en el nivel {level}\")\n",
    "    \n",
    "            distance = 2 ** level  # Distancia entre archivos a combinar en este nivel\n",
    "            new_files = []  # Lista para almacenar archivos nuevos creados en este nivel\n",
    "    \n",
    "            # Iteramos a través de los archivos en el directorio en pasos de `distance * 2`\n",
    "            for i in range(0, len(files), distance * 2):\n",
    "                inicio1 = i\n",
    "                inicio2 = i + distance\n",
    "                \n",
    "                # Verificamos si inicio2 está dentro del rango\n",
    "                if inicio2 >= len(files):\n",
    "                    break  # No hay par para combinar\n",
    "                \n",
    "                # Cargar los bloques a combinar\n",
    "                with open(os.path.join(self.index_dir, files[inicio1]), \"rb\") as f1:\n",
    "                    dict1 = pickle.load(f1)\n",
    "                with open(os.path.join(self.index_dir, files[inicio2]), \"rb\") as f2:\n",
    "                    dict2 = pickle.load(f2)\n",
    "    \n",
    "                # Inicializamos el diccionario combinado\n",
    "                merged_dict = {}\n",
    "                keys1 = iter(sorted(dict1.keys()))\n",
    "                keys2 = iter(sorted(dict2.keys()))\n",
    "                term1 = next(keys1, None)\n",
    "                term2 = next(keys2, None)\n",
    "    \n",
    "                # Comenzamos el merge sort\n",
    "                while term1 is not None or term2 is not None:\n",
    "                    # Determinamos el término menor actual\n",
    "                    if term1 is not None and (term2 is None or term1 < term2):\n",
    "                        merged_dict[term1] = dict1[term1]\n",
    "                        term1 = next(keys1, None)\n",
    "                    elif term2 is not None and (term1 is None or term2 < term1):\n",
    "                        merged_dict[term2] = dict2[term2]\n",
    "                        term2 = next(keys2, None)\n",
    "                    else:\n",
    "                        # Si ambos términos son iguales, combinamos las listas de documentos\n",
    "                        merged_dict[term1] = {**dict1[term1], **dict2[term2]}\n",
    "                        term1 = next(keys1, None)\n",
    "                        term2 = next(keys2, None)\n",
    "    \n",
    "                    # Guardamos el bloque si se alcanza el límite de tamaño\n",
    "                    if sys.getsizeof(merged_dict) >= self.disk_limit:\n",
    "                        self.write_block_to_disk(merged_dict, level + 1)\n",
    "                        merged_dict.clear()  # Limpiamos para continuar\n",
    "    \n",
    "                # Guardamos cualquier contenido restante en `merged_dict`\n",
    "                if merged_dict:\n",
    "                    self.write_block_to_disk(merged_dict, level + 1)\n",
    "                \n",
    "                # Agregamos los archivos nuevos creados en este nivel\n",
    "                new_files.append(f\"block_nivel{level + 1}_bloque{self.block_counter - 1}.pkl\")\n",
    "    \n",
    "            # Eliminamos los archivos del nivel anterior\n",
    "            for file in files:\n",
    "                if f\"nivel{level}_\" in file:\n",
    "                    os.remove(os.path.join(self.index_dir, file))\n",
    "            \n",
    "            # Actualizamos la lista de archivos para el siguiente nivel\n",
    "            files = sorted(new_files)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "s = SPIMI()  \n",
    "s.spimi_invert(pairs_df)\n",
    "s.merge()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Todos los diccionarios del nivel 5 están correctamente ordenados.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def check_sorted(index_dir, level):\n",
    "    # Listar todos los archivos del nivel\n",
    "    files = sorted(os.listdir(index_dir))\n",
    "    files_at_level = [file for file in files if f\"nivel{level}\" in file]\n",
    "    \n",
    "    # Leer cada archivo del nivel\n",
    "    for file in files_at_level:\n",
    "        with open(os.path.join(index_dir, file), \"rb\") as f:\n",
    "            dictionary = pickle.load(f)\n",
    "            # Comprobar si el diccionario está ordenado\n",
    "            last_term = None\n",
    "            for term in dictionary:\n",
    "                if last_term and last_term > term:\n",
    "                    print(f\"El diccionario en {file} no está ordenado correctamente.\")\n",
    "                    return False\n",
    "                last_term = term\n",
    "    print(f\"Todos los diccionarios del nivel {level} están correctamente ordenados.\")\n",
    "    return True\n",
    "\n",
    "check_sorted(\"index_blocks\", s.totalLevels - 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Contenido de merged_nivel5_bloque32.pkl:\n",
      "Term: dedicar -> Postings: {10: 2}\n",
      "Term: del -> Postings: {11: 1}\n",
      "Term: dell -> Postings: {10: 1}\n",
      "Term: descontrola -> Postings: {11: 2}\n",
      "Term: desea -> Postings: {11: 1}\n",
      "Term: diablo -> Postings: {11: 1}\n",
      "Term: disfruta -> Postings: {11: 1}\n",
      "Term: dond -> Postings: {11: 1}\n",
      "Term: duro -> Postings: {11: 1}\n",
      "Term: ella -> Postings: {11: 3}\n",
      "Term: en -> Postings: {11: 2}\n",
      "Term: eso -> Postings: {11: 2}\n",
      "Term: estamo -> Postings: {11: 2}\n",
      "Term: eto -> Postings: {11: 2}\n",
      "Term: fino -> Postings: {10: 1}\n",
      "Term: fuego -> Postings: {11: 1}\n",
      "Term: futuroo -> Postings: {11: 1}\n",
      "Term: gloria -> Postings: {11: 2}\n",
      "Term: gusta -> Postings: {11: 1}\n",
      "Term: gustan -> Postings: {11: 1}\n",
      "Term: hago -> Postings: {11: 1}\n",
      "Term: jugueton -> Postings: {11: 1}\n",
      "Term: las -> Postings: {11: 1}\n",
      "Term: le -> Postings: {10: 1, 11: 4}\n",
      "Term: lleva -> Postings: {11: 2}\n",
      "Term: lo -> Postings: {11: 1}\n",
      "Term: loca -> Postings: {11: 2}\n",
      "Term: los -> Postings: {11: 1}\n",
      "Term: mami -> Postings: {11: 1}\n",
      "Term: mi -> Postings: {11: 2}\n",
      "Term: muer -> Postings: {11: 1}\n",
      "Term: musiica -> Postings: {11: 1}\n",
      "Term: nada -> Postings: {11: 2}\n",
      "Term: noch -> Postings: {11: 1}\n",
      "Term: nostr -> Postings: {10: 1}\n",
      "Term: nunca -> Postings: {11: 2}\n",
      "Term: ombr -> Postings: {10: 1}\n",
      "Term: ore -> Postings: {10: 1}\n",
      "Term: pare -> Postings: {11: 1}\n",
      "Term: pege -> Postings: {11: 1}\n",
      "Term: perdi -> Postings: {10: 1}\n",
      "Term: por -> Postings: {11: 1}\n",
      "Term: porqu -> Postings: {11: 1}\n",
      "Term: posicion -> Postings: {11: 1}\n",
      "Term: provo -> Postings: {10: 3}\n",
      "Term: pum -> Postings: {11: 6}\n",
      "Term: quando -> Postings: {10: 1}\n",
      "Term: quello -> Postings: {10: 3}\n",
      "Term: quier -> Postings: {11: 3}\n",
      "Term: ratata -> Postings: {11: 2}\n",
      "Term: sale -> Postings: {10: 1}\n",
      "Term: sea -> Postings: {11: 1}\n",
      "Term: sentido -> Postings: {11: 2}\n",
      "Term: siento -> Postings: {11: 2}\n",
      "Term: sola -> Postings: {11: 2}\n",
      "Term: sole -> Postings: {10: 1}\n",
      "Term: son -> Postings: {11: 1}\n",
      "Term: ta -> Postings: {11: 1}\n",
      "Term: te -> Postings: {11: 1}\n",
      "Term: ti -> Postings: {10: 2}\n",
      "Term: tiguer -> Postings: {11: 1}\n",
      "Term: toda -> Postings: {11: 1}\n",
      "Term: tra -> Postings: {10: 1}\n",
      "Term: tu -> Postings: {11: 3}\n",
      "Term: tum -> Postings: {11: 1}\n",
      "Term: una -> Postings: {11: 4}\n",
      "Term: uoh -> Postings: {11: 3}\n",
      "Term: vaina -> Postings: {11: 4}\n",
      "Term: vida -> Postings: {11: 2}\n",
      "Term: volevo -> Postings: {10: 2}\n",
      "Term: voy -> Postings: {11: 1}\n",
      "Term: yeah -> Postings: {11: 1}\n",
      "Term: yo -> Postings: {11: 4}\n",
      "Term: è -> Postings: {10: 3}\n",
      "\n",
      "Todos los diccionarios del nivel 5 están correctamente ordenados.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def check_sorted_and_display(index_dir, level):\n",
    "    # Listar todos los archivos del nivel\n",
    "    files = sorted(os.listdir(index_dir))\n",
    "    files_at_level = [file for file in files if f\"nivel{level}\" in file]\n",
    "    \n",
    "    # Leer cada archivo del nivel\n",
    "    for file in files_at_level:\n",
    "        file_path = os.path.join(index_dir, file)\n",
    "        with open(file_path, \"rb\") as f:\n",
    "            dictionary = pickle.load(f)\n",
    "            \n",
    "            # Mostrar el contenido del diccionario (términos y postings)\n",
    "            print(f\"\\nContenido de {file}:\")\n",
    "            for term, postings in dictionary.items():\n",
    "                print(f\"Term: {term} -> Postings: {postings}\")\n",
    "            \n",
    "            # Comprobar si el diccionario está ordenado\n",
    "            last_term = None\n",
    "            for term in dictionary:\n",
    "                if last_term and last_term > term:\n",
    "                    print(f\"El diccionario en {file} no está ordenado correctamente.\")\n",
    "                    return False\n",
    "                last_term = term\n",
    "\n",
    "    print(f\"\\nTodos los diccionarios del nivel {level} están correctamente ordenados.\")\n",
    "    return True\n",
    "\n",
    "# Llamar a la función para verificar y mostrar el contenido\n",
    "check_sorted_and_display(\"index_blocks\", s.totalLevels - 1)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
