{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Proyecto jiji"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: kagglehub in c:\\users\\ce mar\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (0.3.4)\n",
      "Requirement already satisfied: tqdm in c:\\users\\ce mar\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from kagglehub) (4.66.1)\n",
      "Requirement already satisfied: requests in c:\\users\\ce mar\\appdata\\roaming\\python\\python310\\site-packages (from kagglehub) (2.28.1)\n",
      "Requirement already satisfied: packaging in c:\\users\\ce mar\\appdata\\roaming\\python\\python310\\site-packages (from kagglehub) (23.2)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in c:\\users\\ce mar\\appdata\\roaming\\python\\python310\\site-packages (from requests->kagglehub) (2.1.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\ce mar\\appdata\\roaming\\python\\python310\\site-packages (from requests->kagglehub) (2022.12.7)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\ce mar\\appdata\\roaming\\python\\python310\\site-packages (from requests->kagglehub) (1.26.13)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\ce mar\\appdata\\roaming\\python\\python310\\site-packages (from requests->kagglehub) (3.4)\n",
      "Requirement already satisfied: colorama in c:\\users\\ce mar\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from tqdm->kagglehub) (0.4.6)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip available: 22.2.1 -> 24.3.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "!pip install kagglehub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to C:\\Users\\Ce\n",
      "[nltk_data]     mar\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "import numpy as np\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "nltk.download('punkt')\n",
    "import regex as re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "stemmer = SnowballStemmer('english')\n",
    "import os\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Path to dataset files: C:\\Users\\Ce mar\\.cache\\kagglehub\\datasets\\imuhammad\\audio-features-and-lyrics-of-spotify-songs\\versions\\1\n"
     ]
    }
   ],
   "source": [
    "import kagglehub\n",
    "path = kagglehub.dataset_download(\"imuhammad/audio-features-and-lyrics-of-spotify-songs\")\n",
    "print(\"Path to dataset files:\", path)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"stopwords-en.txt\", encoding=\"latin1\") as file:\n",
    "   stoplist = [line.rstrip().lower() for line in file]\n",
    "stoplist += ['?', '-', '.', ':', ',', '!', ';']\n",
    "\n",
    "def preprocesamiento(texto, stemming=True):\n",
    "  words = []\n",
    "  if not isinstance(texto, str):\n",
    "    return words\n",
    "  texto = texto.lower()\n",
    "  texto = re.sub(r'[^a-zA-Z0-9_À-ÿ]', ' ', texto)\n",
    "  # tokenizar\n",
    "  words = nltk.word_tokenize(texto, language='spanish')\n",
    "  # filtrar stopwords\n",
    "  words = [word for word in words if word not in stoplist]\n",
    "  # reducir palabras (stemming)\n",
    "  if stemming:\n",
    "      words = [stemmer.stem(word) for word in words]\n",
    "  return words\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Ce mar\\.cache\\kagglehub\\datasets\\imuhammad\\audio-features-and-lyrics-of-spotify-songs\\versions\\1\\spotify_songs.csv\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>track_id</th>\n",
       "      <th>track_name</th>\n",
       "      <th>track_artist</th>\n",
       "      <th>lyrics</th>\n",
       "      <th>track_popularity</th>\n",
       "      <th>track_album_id</th>\n",
       "      <th>track_album_name</th>\n",
       "      <th>track_album_release_date</th>\n",
       "      <th>playlist_name</th>\n",
       "      <th>playlist_id</th>\n",
       "      <th>...</th>\n",
       "      <th>loudness</th>\n",
       "      <th>mode</th>\n",
       "      <th>speechiness</th>\n",
       "      <th>acousticness</th>\n",
       "      <th>instrumentalness</th>\n",
       "      <th>liveness</th>\n",
       "      <th>valence</th>\n",
       "      <th>tempo</th>\n",
       "      <th>duration_ms</th>\n",
       "      <th>language</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0017A6SJgTbfQVU2EtsPNo</td>\n",
       "      <td>Pangarap</td>\n",
       "      <td>Barbie's Cradle</td>\n",
       "      <td>Minsan pa Nang ako'y napalingon Hindi ko alam ...</td>\n",
       "      <td>41</td>\n",
       "      <td>1srJQ0njEQgd8w4XSqI4JQ</td>\n",
       "      <td>Trip</td>\n",
       "      <td>2001-01-01</td>\n",
       "      <td>Pinoy Classic Rock</td>\n",
       "      <td>37i9dQZF1DWYDQ8wBxd7xt</td>\n",
       "      <td>...</td>\n",
       "      <td>-10.068</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0236</td>\n",
       "      <td>0.27900</td>\n",
       "      <td>0.01170</td>\n",
       "      <td>0.0887</td>\n",
       "      <td>0.566</td>\n",
       "      <td>97.091</td>\n",
       "      <td>235440</td>\n",
       "      <td>tl</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>004s3t0ONYlzxII9PLgU6z</td>\n",
       "      <td>I Feel Alive</td>\n",
       "      <td>Steady Rollin</td>\n",
       "      <td>The trees, are singing in the wind The sky blu...</td>\n",
       "      <td>28</td>\n",
       "      <td>3z04Lb9Dsilqw68SHt6jLB</td>\n",
       "      <td>Love &amp; Loss</td>\n",
       "      <td>2017-11-21</td>\n",
       "      <td>Hard Rock Workout</td>\n",
       "      <td>3YouF0u7waJnolytf9JCXf</td>\n",
       "      <td>...</td>\n",
       "      <td>-4.739</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0442</td>\n",
       "      <td>0.01170</td>\n",
       "      <td>0.00994</td>\n",
       "      <td>0.3470</td>\n",
       "      <td>0.404</td>\n",
       "      <td>135.225</td>\n",
       "      <td>373512</td>\n",
       "      <td>en</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>00chLpzhgVjxs1zKC9UScL</td>\n",
       "      <td>Poison</td>\n",
       "      <td>Bell Biv DeVoe</td>\n",
       "      <td>NA Yeah, Spyderman and Freeze in full effect U...</td>\n",
       "      <td>0</td>\n",
       "      <td>6oZ6brjB8x3GoeSYdwJdPc</td>\n",
       "      <td>Gold</td>\n",
       "      <td>2005-01-01</td>\n",
       "      <td>Back in the day - R&amp;B, New Jack Swing, Swingbe...</td>\n",
       "      <td>3a9y4eeCJRmG9p4YKfqYIx</td>\n",
       "      <td>...</td>\n",
       "      <td>-7.504</td>\n",
       "      <td>0</td>\n",
       "      <td>0.2160</td>\n",
       "      <td>0.00432</td>\n",
       "      <td>0.00723</td>\n",
       "      <td>0.4890</td>\n",
       "      <td>0.650</td>\n",
       "      <td>111.904</td>\n",
       "      <td>262467</td>\n",
       "      <td>en</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>00cqd6ZsSkLZqGMlQCR0Zo</td>\n",
       "      <td>Baby It's Cold Outside (feat. Christina Aguilera)</td>\n",
       "      <td>CeeLo Green</td>\n",
       "      <td>I really can't stay Baby it's cold outside I'v...</td>\n",
       "      <td>41</td>\n",
       "      <td>3ssspRe42CXkhPxdc12xcp</td>\n",
       "      <td>CeeLo's Magic Moment</td>\n",
       "      <td>2012-10-29</td>\n",
       "      <td>Christmas Soul</td>\n",
       "      <td>6FZYc2BvF7tColxO8PBShV</td>\n",
       "      <td>...</td>\n",
       "      <td>-5.819</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0341</td>\n",
       "      <td>0.68900</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.0664</td>\n",
       "      <td>0.405</td>\n",
       "      <td>118.593</td>\n",
       "      <td>243067</td>\n",
       "      <td>en</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>00emjlCv9azBN0fzuuyLqy</td>\n",
       "      <td>Dumb Litty</td>\n",
       "      <td>KARD</td>\n",
       "      <td>Get up out of my business You don't keep me fr...</td>\n",
       "      <td>65</td>\n",
       "      <td>7h5X3xhh3peIK9Y0qI5hbK</td>\n",
       "      <td>KARD 2nd Digital Single ‘Dumb Litty’</td>\n",
       "      <td>2019-09-22</td>\n",
       "      <td>K-Party Dance Mix</td>\n",
       "      <td>37i9dQZF1DX4RDXswvP6Mj</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.993</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0409</td>\n",
       "      <td>0.03700</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.1380</td>\n",
       "      <td>0.240</td>\n",
       "      <td>130.018</td>\n",
       "      <td>193160</td>\n",
       "      <td>en</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 25 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                 track_id                                         track_name  \\\n",
       "0  0017A6SJgTbfQVU2EtsPNo                                           Pangarap   \n",
       "1  004s3t0ONYlzxII9PLgU6z                                       I Feel Alive   \n",
       "2  00chLpzhgVjxs1zKC9UScL                                             Poison   \n",
       "3  00cqd6ZsSkLZqGMlQCR0Zo  Baby It's Cold Outside (feat. Christina Aguilera)   \n",
       "4  00emjlCv9azBN0fzuuyLqy                                         Dumb Litty   \n",
       "\n",
       "      track_artist                                             lyrics  \\\n",
       "0  Barbie's Cradle  Minsan pa Nang ako'y napalingon Hindi ko alam ...   \n",
       "1    Steady Rollin  The trees, are singing in the wind The sky blu...   \n",
       "2   Bell Biv DeVoe  NA Yeah, Spyderman and Freeze in full effect U...   \n",
       "3      CeeLo Green  I really can't stay Baby it's cold outside I'v...   \n",
       "4             KARD  Get up out of my business You don't keep me fr...   \n",
       "\n",
       "   track_popularity          track_album_id  \\\n",
       "0                41  1srJQ0njEQgd8w4XSqI4JQ   \n",
       "1                28  3z04Lb9Dsilqw68SHt6jLB   \n",
       "2                 0  6oZ6brjB8x3GoeSYdwJdPc   \n",
       "3                41  3ssspRe42CXkhPxdc12xcp   \n",
       "4                65  7h5X3xhh3peIK9Y0qI5hbK   \n",
       "\n",
       "                       track_album_name track_album_release_date  \\\n",
       "0                                  Trip               2001-01-01   \n",
       "1                           Love & Loss               2017-11-21   \n",
       "2                                  Gold               2005-01-01   \n",
       "3                  CeeLo's Magic Moment               2012-10-29   \n",
       "4  KARD 2nd Digital Single ‘Dumb Litty’               2019-09-22   \n",
       "\n",
       "                                       playlist_name             playlist_id  \\\n",
       "0                                 Pinoy Classic Rock  37i9dQZF1DWYDQ8wBxd7xt   \n",
       "1                                  Hard Rock Workout  3YouF0u7waJnolytf9JCXf   \n",
       "2  Back in the day - R&B, New Jack Swing, Swingbe...  3a9y4eeCJRmG9p4YKfqYIx   \n",
       "3                                     Christmas Soul  6FZYc2BvF7tColxO8PBShV   \n",
       "4                                  K-Party Dance Mix  37i9dQZF1DX4RDXswvP6Mj   \n",
       "\n",
       "   ... loudness mode  speechiness  acousticness  instrumentalness  liveness  \\\n",
       "0  ...  -10.068    1       0.0236       0.27900           0.01170    0.0887   \n",
       "1  ...   -4.739    1       0.0442       0.01170           0.00994    0.3470   \n",
       "2  ...   -7.504    0       0.2160       0.00432           0.00723    0.4890   \n",
       "3  ...   -5.819    0       0.0341       0.68900           0.00000    0.0664   \n",
       "4  ...   -1.993    1       0.0409       0.03700           0.00000    0.1380   \n",
       "\n",
       "   valence    tempo  duration_ms  language  \n",
       "0    0.566   97.091       235440        tl  \n",
       "1    0.404  135.225       373512        en  \n",
       "2    0.650  111.904       262467        en  \n",
       "3    0.405  118.593       243067        en  \n",
       "4    0.240  130.018       193160        en  \n",
       "\n",
       "[5 rows x 25 columns]"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lista_ = os.listdir(path)\n",
    "songs = os.path.join(path, lista_[0])\n",
    "print(songs)\n",
    "dataset = pd.read_csv(songs)\n",
    "dataset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Get up out of my business You don't keep me from turning up 모두 다 여긴 Witness 넌 바른 척하는 Criminal 찔리는 걸 You feel it 너 네가 뭔데 왜 솔직하지 못해 넌 다를 건 대체 뭔데 aye Bad boy 그래 난 Bad boy 생각해 네가 하고픈 대로 That boy 그래 난 That boy 모르면 믿고 싶은 대로 Nothing ‘bout me nada you don't know Nothing ‘bout me not a thing 욕을 하는 이윤 다 있는 거야 나 아닌 너 안에 Cause we Party yeah we party we don't worry ‘bout the drama no Body Everybody 우린 아침까지 Wilding up 신경 쓸건 따로 아 몰라 오늘 다 집어치워 눈치 보지 말고 Volume up up up We go dumb dumb dumb dumb 누가 뭐래도 Litty dumb Litty dumb litty Bang the drum 갈 때까지 Baby stupid Stupid dumb dumb dumb dumb 누가 뭐래도 Litty dumb Litty dumb litty Bang the drum 갈 때까지 Baby we go 몰입하니 눈 몰리는 이치 시시콜콜 무료함을 완충해 내키면 곧장 Dumb litty 이 노래가 흐름 Come get it Drippin' 여기 주도권을 Grip than 거리낌 허물어 Let's go party 돌변함은 때 때에 따라 아무렴 어때 까무러치게 더불어 즐겨 사는 거지 My way follow nobody Party yeah we party we don't worry ‘bout the drama no Body Everybody 우린 아침까지 Wilding up 신경 쓸건 따로 아 몰라 오늘 다 집어치워 터지게 더 올려 Volume up up up We go dumb dumb dumb dumb 누가 뭐래도 Litty dumb Litty dumb litty Bang the drum 갈 때까지 Baby stupid Stupid dumb dumb dumb dumb 누가 뭐래도 Litty dumb Litty dumb litty Bang the drum 갈 때까지 Baby we go 숨이 차올라 내 심장을 죄여 이젠 풀어 나의 사슬 And I wake up 오늘 밤 지나 해가 뜰 때까지 계속 Up (Go dumb) 내일 일은 내일 uh 끝이 난 듯 저질러 We go dumb dumb dumb dumb 누가 뭐래도 Litty dumb Litty dumb litty Bang the drum 갈 때까지 Baby stupid Stupid dumb dumb dumb dumb 누가 뭐래도 Litty dumb Litty dumb litty Bang the drum 갈 때까지 Baby we go Litty dumb litty Bang the drum 갈 때까지 Baby we go\n"
     ]
    }
   ],
   "source": [
    "fila_5 = dataset.iloc[4, 3]\n",
    "print(fila_5)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Index</th>\n",
       "      <th>ProcessedText</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>tree</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>sing</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>wind</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>sky</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>blue</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3390</th>\n",
       "      <td>19</td>\n",
       "      <td>yeah</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3391</th>\n",
       "      <td>19</td>\n",
       "      <td>nah</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3392</th>\n",
       "      <td>19</td>\n",
       "      <td>voz</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3393</th>\n",
       "      <td>19</td>\n",
       "      <td>favorita</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3394</th>\n",
       "      <td>19</td>\n",
       "      <td>babi</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3395 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      Index ProcessedText\n",
       "0         1          tree\n",
       "1         1          sing\n",
       "2         1          wind\n",
       "3         1           sky\n",
       "4         1          blue\n",
       "...     ...           ...\n",
       "3390     19          yeah\n",
       "3391     19           nah\n",
       "3392     19           voz\n",
       "3393     19      favorita\n",
       "3394     19          babi\n",
       "\n",
       "[3395 rows x 2 columns]"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# dataset = dataset.head(l)\n",
    "dataset = dataset.head(20)\n",
    "#Procesar los documentos en pares\n",
    "position_text=3\n",
    "pairs = []\n",
    "\n",
    "for i, row in dataset.iterrows():\n",
    "    if(i!=0):\n",
    "        words = preprocesamiento(row.iloc[position_text])\n",
    "        for text in words:\n",
    "            pairs.append((i, text))\n",
    "\n",
    "pairs_df = pd.DataFrame(pairs, columns=['Index', 'ProcessedText'])\n",
    "pairs_df.head(len(pairs_df))\n",
    "\n",
    "\n",
    "# print(pairs_df[pairs_df[\"ProcessedText\"] == \"6\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nRecibe la direccion del folder con los diccionarios del spimi,\\nmodifica los archivos y los sobreescribe para crear el índice\\ninvertido con un índice global\\n'"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pickle\n",
    "import math\n",
    "def Lista_To_Diccionario(dir_bloque):\n",
    "    bloque_dict = {}\n",
    "    try:\n",
    "        with open(dir_bloque, 'rb') as f:\n",
    "            bloque = pickle.load(f)\n",
    "        if len(bloque) != 0:\n",
    "            # print(bloque)\n",
    "            for term, posting in bloque:\n",
    "                term_dict = {}\n",
    "                for doc, tf in posting:\n",
    "                    term_dict[doc] = tf\n",
    "                bloque_dict[term] = term_dict\n",
    "    except EOFError:\n",
    "        print(f\"El archivo {dir_bloque} está vacío o incompleto.\")\n",
    "    return bloque_dict\n",
    "\n",
    "def OrdenarPorBloques(dir_blocks, n_blocks):\n",
    "    for i in range(n_blocks):\n",
    "        file_path = os.path.join(dir_blocks, 'block_{}.pkl'.format(i))\n",
    "        with open(file_path, 'rb') as f:\n",
    "            tuplas_ordenadas = pickle.load(f)\n",
    "        tuplas_ordenadas = sorted(list(tuplas_ordenadas.items()), key=lambda x: x[0])\n",
    "        tuplas_ordenadas = [par for par in tuplas_ordenadas if not par[0].isdigit()]\n",
    "        tuplas_ordenadas = [(term[0], list(term[1].items())) for term in tuplas_ordenadas]\n",
    "        with open(file_path, 'wb') as f:\n",
    "            pickle.dump(tuplas_ordenadas, f)\n",
    "\n",
    "def mergeSortAux(dir_bloques, l, r):\n",
    "    if l == r:\n",
    "        bloque = leer_bloque(dir_bloques, l)\n",
    "        unicos = set()    \n",
    "        for par in bloque:\n",
    "            unicos.add(par[0])\n",
    "        return list(unicos)\n",
    "    \n",
    "    if (l < r):\n",
    "        mid = int(math.ceil((r + l)/2.0))\n",
    "        unique_l = mergeSortAux(dir_bloques, l, mid - 1)\n",
    "        unique_r = mergeSortAux(dir_bloques, mid, r)\n",
    "        unicos = set()    \n",
    "        for term in unique_l:\n",
    "            unicos.add(term)\n",
    "        for term in unique_r:\n",
    "            unicos.add(term)\n",
    "        unicos = list(unicos)\n",
    "        merge_v2(dir_bloques, l, r, mid, len(unicos))\n",
    "        return list(unicos)\n",
    "        \n",
    "    return []\n",
    "\n",
    "def escribir_bloque(dir_bloques, block, idx_insert_block, buffer_limit = 2000):\n",
    "    with open(os.path.join(dir_bloques, \"block_{}_v2.pkl\".format(idx_insert_block)), 'wb') as f:\n",
    "        pickle.dump(block, f)    \n",
    "def leer_bloque(dir_bloques, it):\n",
    "    file_path = os.path.join(dir_bloques, f\"block_{it}.pkl\")\n",
    "    with open(file_path, \"rb\") as f:\n",
    "        buffer = pickle.load(f)\n",
    "    return buffer\n",
    "def merge_v2(dir_bloques, l, r, mid, num_terms):\n",
    "    idx_insert_block = l\n",
    "    new_block = []\n",
    "    mezclar_n_bloques = r - l + 1\n",
    "    unique_terms_per_block = int(math.ceil(num_terms/mezclar_n_bloques))\n",
    "    unique_terms_current_block = 0\n",
    "\n",
    "    it_l = l\n",
    "    it_r = mid\n",
    "    term_dic_l = leer_bloque(dir_bloques, it_l)\n",
    "    term_dic_r = leer_bloque(dir_bloques, it_r)\n",
    "    \n",
    "    idx_term_l = 0\n",
    "    idx_term_r = 0\n",
    "\n",
    "    idx_doc_l = 0\n",
    "    idx_doc_r = 0\n",
    "    new_block = []\n",
    "    while(it_l < mid and it_r < r + 1):\n",
    "        # print(f\"Toma 2 bloques {it_l} y {it_r} | idx_term_l: \", idx_term_l, \"| len(term_dic_l)\", len(term_dic_l), \"| idx_term_r: \", idx_term_r, \"| len(term_dic_r)\", len(term_dic_r))\n",
    "        while(idx_term_l < len(term_dic_l) and idx_term_r < len(term_dic_r)): # moverme entre palabras de dos bloques\n",
    "            # print(\"Current term_dic_l\", term_dic_l)\n",
    "            # print(\"Current term_dic_r\", term_dic_r)\n",
    "            # print(f\"Toma 2 terminos {term_dic_l[idx_term_l][0]} y {term_dic_r[idx_term_r][0]}\")\n",
    "            new_term = []\n",
    "            if(term_dic_l[idx_term_l][0] < term_dic_r[idx_term_r][0]):\n",
    "                new_term = term_dic_l[idx_term_l]\n",
    "                idx_term_l += 1\n",
    "            elif(term_dic_l[idx_term_l][0] > term_dic_r[idx_term_r][0]):\n",
    "                new_term = term_dic_r[idx_term_r]\n",
    "                idx_term_r += 1\n",
    "            else:\n",
    "                idx_doc_l = 0\n",
    "                idx_doc_r = 0\n",
    "                while(idx_doc_l < len(term_dic_l[idx_term_l][1]) and idx_doc_r < len(term_dic_r[idx_term_r][1])):\n",
    "                    # print(f\"Toma 2 terminos iguales con tf = {term_dic_l[idx_term_l][1]} y {term_dic_r[idx_term_r][1]}\")\n",
    "                    if term_dic_l[idx_term_l][1][idx_doc_l][0] > term_dic_r[idx_term_r][1][idx_doc_r][0]:\n",
    "                        pushear_doc = term_dic_r[idx_term_r][1][idx_doc_r]\n",
    "                        idx_doc_r += 1\n",
    "                    elif term_dic_l[idx_term_l][1][idx_doc_l][0] < term_dic_r[idx_term_r][1][idx_doc_r][0]:\n",
    "                        pushear_doc = term_dic_l[idx_term_l][1][idx_doc_l]\n",
    "                        idx_doc_l += 1\n",
    "                    else:\n",
    "                        pushear_doc = (term_dic_l[idx_term_l][1][idx_doc_l][0], term_dic_l[idx_term_l][1][idx_doc_l][1] + term_dic_r[idx_term_r][1][idx_doc_r][1])\n",
    "                        idx_doc_l += 1\n",
    "                        idx_doc_r += 1\n",
    "                    # print(\"pushear_doc: \", pushear_doc)\n",
    "                    new_term.append(pushear_doc)\n",
    "                while(idx_doc_l < len(term_dic_l[idx_term_l][1])):\n",
    "                    # print(f\"ya no hay documentos de derecha, rellena con izquierda {idx_doc_l}\")\n",
    "                    pushear_doc = term_dic_l[idx_term_l][1][idx_doc_l]\n",
    "                    idx_doc_l += 1\n",
    "                    new_term.append(pushear_doc)\n",
    "                while(idx_doc_r < len(term_dic_r[idx_term_r][1])):\n",
    "                    # print(f\"ya no hay documentos de izquierda, rellena con derecha {idx_doc_l}\")\n",
    "                    pushear_doc = term_dic_r[idx_term_r][1][idx_doc_r]\n",
    "                    idx_doc_r += 1\n",
    "                    new_term.append(pushear_doc)\n",
    "                new_term = (term_dic_l[idx_term_l][0], new_term)\n",
    "                idx_term_r += 1\n",
    "                idx_term_l += 1\n",
    "            new_block.append(new_term)\n",
    "            \n",
    "            unique_terms_current_block += 1\n",
    "            if (unique_terms_current_block == unique_terms_per_block):\n",
    "                escribir_bloque(dir_bloques, new_block, idx_insert_block)\n",
    "                unique_terms_current_block = 0\n",
    "                idx_insert_block += 1\n",
    "                new_block = []\n",
    "        if(len(term_dic_l) == idx_term_l):\n",
    "            if (it_l < mid - 1):\n",
    "                it_l += 1\n",
    "                term_dic_l = leer_bloque(dir_bloques, it_l)\n",
    "                idx_term_l = 0\n",
    "                idx_doc_l = 0\n",
    "                continue\n",
    "            else:\n",
    "                break\n",
    "        if(len(term_dic_r) == idx_term_r):\n",
    "            if (it_r < r):\n",
    "                it_r += 1\n",
    "                term_dic_r = leer_bloque(dir_bloques, it_r)\n",
    "                idx_term_r = 0\n",
    "                idx_doc_r = 0\n",
    "                continue\n",
    "            else:\n",
    "                break\n",
    "        if(it_l == mid | it_r == r + 1):\n",
    "            break\n",
    "    while(it_l < mid):\n",
    "        # print(f\"Se acabaron los bloques de derecha, llena solo izquierda {it_l}\")\n",
    "        term_dic_l = leer_bloque(dir_bloques, it_l)\n",
    "        while(idx_term_l < len(term_dic_l)):\n",
    "            new_block.append(term_dic_l[idx_term_l])\n",
    "            unique_terms_current_block += 1\n",
    "            if (unique_terms_current_block == unique_terms_per_block):\n",
    "                escribir_bloque(dir_bloques, new_block, idx_insert_block)\n",
    "                unique_terms_current_block = 0\n",
    "                idx_insert_block += 1\n",
    "                new_block = []\n",
    "            idx_term_l += 1\n",
    "        idx_term_l = 0\n",
    "        it_l += 1\n",
    "    while(it_r < r + 1):\n",
    "        # print(f\"Se acabaron los bloques de izquierda, llena solo derecha {it_r}\")\n",
    "        term_dic_r = leer_bloque(dir_bloques, it_r)\n",
    "        while(idx_term_r < len(term_dic_r)):\n",
    "            new_block.append(term_dic_r[idx_term_r])\n",
    "            unique_terms_current_block += 1\n",
    "            if (unique_terms_current_block == unique_terms_per_block):\n",
    "                escribir_bloque(dir_bloques, new_block, idx_insert_block)\n",
    "                unique_terms_current_block = 0\n",
    "                idx_insert_block += 1\n",
    "                new_block = []\n",
    "            idx_term_r += 1\n",
    "        idx_term_r = 0\n",
    "        it_r += 1\n",
    "\n",
    "    while(idx_insert_block < r + 1):\n",
    "        if len(new_block) > 0:\n",
    "            escribir_bloque(dir_bloques, new_block, idx_insert_block)\n",
    "        else:\n",
    "            escribir_bloque(dir_bloques, [], idx_insert_block)\n",
    "        new_block = []\n",
    "        idx_insert_block += 1\n",
    "    idx_insert_block = l\n",
    "    for idx_archivo in range(l, r + 1):\n",
    "        nuevo_nombre = os.path.join(dir_bloques, \"block_{}.pkl\".format(idx_archivo))\n",
    "        if os.path.exists(nuevo_nombre):\n",
    "            os.remove(nuevo_nombre)\n",
    "        os.rename(os.path.join(dir_bloques, \"block_{}_v2.pkl\".format(idx_archivo)), nuevo_nombre)\n",
    "def mergeSort(dir_bloques):\n",
    "    bloques_files_dir = os.listdir(os.path.join('./',dir_bloques))\n",
    "    n = len(bloques_files_dir)\n",
    "    mergeSortAux(dir_bloques, 0, n - 1)\n",
    "    \n",
    "def InvertirListasDiccionarios(dir_bloques):\n",
    "    n_bloques = len(os.listdir(os.path.join('./',dir_bloques)))\n",
    "    # print(\"revisa: \", os.path.join('./',dir_bloques), \" con: \", n_blocks,\" bloques\")\n",
    "    for i in range(n_bloques):\n",
    "        bloque_path = os.path.join(dir_bloques,\"block_{}.pkl\".format(i))\n",
    "        # print(\"revisa: \", bloque_path)\n",
    "        with open(bloque_path, 'rb') as f:\n",
    "            bloque = pickle.load(f)\n",
    "        bloque_dict = {}\n",
    "\n",
    "        if len(bloque) != 0:\n",
    "            for term, poosting_list in bloque:\n",
    "                poosting_dict = {}\n",
    "                for doc, tf in poosting_list:\n",
    "                    poosting_dict[doc] = tf\n",
    "                bloque_dict[term] = poosting_dict\n",
    "        with open(bloque_path, 'wb') as f:\n",
    "            pickle.dump(bloque_dict, f)\n",
    "'''\n",
    "Recibe la direccion del folder con los diccionarios del spimi,\n",
    "modifica los archivos y los sobreescribe para crear el índice\n",
    "invertido con un índice global\n",
    "'''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "La cantidad de niveles serán 5\n",
      "Cantidad de bloques generados 23\n",
      "file:  block_0.pkl\n",
      "file:  block_1.pkl\n",
      "file:  block_10.pkl\n",
      "file:  block_11.pkl\n",
      "file:  block_12.pkl\n",
      "file:  block_13.pkl\n",
      "file:  block_14.pkl\n",
      "file:  block_15.pkl\n",
      "file:  block_16.pkl\n",
      "file:  block_17.pkl\n",
      "file:  block_18.pkl\n",
      "file:  block_19.pkl\n",
      "file:  block_2.pkl\n",
      "file:  block_20.pkl\n",
      "file:  block_21.pkl\n",
      "file:  block_22.pkl\n",
      "file:  block_3.pkl\n",
      "file:  block_4.pkl\n",
      "file:  block_5.pkl\n",
      "file:  block_6.pkl\n",
      "file:  block_7.pkl\n",
      "file:  block_8.pkl\n",
      "file:  block_9.pkl\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pickle\n",
    "import sys\n",
    "import math\n",
    "import heapq\n",
    "\n",
    "class SPIMI:\n",
    "    def __init__(self, index_dir=\"index_blocks\"):\n",
    "        self.index_dir = index_dir  \n",
    "        self.block_counter = 0      \n",
    "        self.doc_count = 0  \n",
    "        self.doc_ids = set()         \n",
    "        self.idf = {}\n",
    "        self.length = {}\n",
    "        self.disk_limit = 4000  \n",
    "        self.totalLevels = 0 \n",
    "        self.currentL = 0\n",
    "        self.cuntermerged = 0 \n",
    "\n",
    "        if not os.path.exists(self.index_dir):\n",
    "            os.makedirs(self.index_dir)\n",
    "\n",
    "    def spimi_invert(self, token_stream):\n",
    "        dictionary = {}\n",
    "        for _, row in token_stream.iterrows():\n",
    "            doc_id = row['Index']\n",
    "            token = row['ProcessedText']\n",
    "            self.doc_ids.add(doc_id)\n",
    "            if token not in dictionary:\n",
    "                dictionary[token] = {}  \n",
    "            \n",
    "            if doc_id not in dictionary[token]:\n",
    "                dictionary[token][doc_id] = 1  \n",
    "            else:\n",
    "                dictionary[token][doc_id] += 1  \n",
    "            dictionary_size = sys.getsizeof(dictionary)\n",
    "            if dictionary_size >= self.disk_limit:\n",
    "                self.write_block_to_disk(dictionary, level=0)\n",
    "                dictionary.clear()\n",
    "                \n",
    "        if dictionary:\n",
    "            self.write_block_to_disk(dictionary, level=0)\n",
    "\n",
    "        self.totalLevels = math.ceil(math.log2(self.block_counter))\n",
    "        print(\"La cantidad de niveles serán\", self.totalLevels)\n",
    "        print(\"Cantidad de bloques generados\", self.block_counter)\n",
    "\n",
    "        ##\n",
    "        self.load_index()\n",
    "        self.calculate_idf()\n",
    "        \n",
    "    def write_block_to_disk(self, dictionary, level):\n",
    "        sorted_terms = dict(sorted(dictionary.items())) \n",
    "        # block_{it}\n",
    "        file_path = os.path.join(self.index_dir, f\"block_{self.block_counter}.pkl\")\n",
    "        with open(file_path, \"wb\") as f:\n",
    "            pickle.dump(sorted_terms, f)\n",
    "        \n",
    "        self.block_counter += 1\n",
    "\n",
    "    def load_block(self, filepath):\n",
    "        with open(filepath, \"rb\") as f:\n",
    "            return pickle.load(f)\n",
    "\n",
    "    def save_merged_block(self, merged_data, level):\n",
    "        file_path = os.path.join(self.index_dir, f\"block_{self.block_counter}.pkl\")\n",
    "        with open(file_path, \"wb\") as f:\n",
    "            pickle.dump(merged_data, f)\n",
    "        self.cuntermerged += 1\n",
    "\n",
    "    def retrieval(self, query, k):\n",
    "        self.load_index()  # Cargar el índice\n",
    "        print(\"El index es:\", self.index)\n",
    "        N = len(self.doc_ids)  # Número total de documentos\n",
    "        print(\"Existen \", N, \" documentos\")\n",
    "        scores = [0] * N  # Puntuaciones para cada documento\n",
    "        terms = preprocesamiento(query)  # Función de preprocesamiento para tokenizar\n",
    "        print(\"query: \", query)\n",
    "        print(\"terms: \", terms)\n",
    "        # Calcular el TF-IDF del query\n",
    "        tf_query = {}  # Almacenaremos el TF de los términos de la consulta\n",
    "        for term in terms:\n",
    "            if term in tf_query.keys():\n",
    "                tf_query[term] += 1\n",
    "            else:\n",
    "                tf_query[term] = 1\n",
    "        print(\"tf_query es: \", tf_query)\n",
    "\n",
    "        tfidf_query = {}\n",
    "        for term, tf in tf_query.items():\n",
    "            tfidf_query\n",
    "            idf_term = 0\n",
    "            if term in self.idf:\n",
    "                idf_term = self.idf[term]\n",
    "            tfidf_query[term] = math.log10(1 + tf) * idf_term\n",
    "        print(\"tfidf_query es: \", tfidf_query)\n",
    "        norm_query = math.sqrt(sum(w_tq**2 for w_tq in tfidf_query.values()))  # Normalización del query\n",
    "\n",
    "        # Aplicar similitud de coseno: Calculamos el puntaje para cada documento\n",
    "        for term, w_tq in tfidf_query.items():\n",
    "            if term in self.index:\n",
    "                for doc, tf_td in self.index[term]:\n",
    "                    w_td = tf_td * self.idf[term]\n",
    "                    # print(\"se añade el peso del doc: \", doc)\n",
    "                    scores[doc - 1] += w_td * w_tq\n",
    "\n",
    "        # Normalizar las puntuaciones de los documentos\n",
    "        for d in range(N):\n",
    "            if self.length.get(d + 1, 0) != 0:\n",
    "                scores[d] /= (self.length[d + 1] * norm_query)  # Normalización documento y consulta\n",
    "\n",
    "        # Ordenar las puntuaciones en orden descendente\n",
    "        result = sorted(enumerate(scores), key=lambda x: x[1], reverse=True)\n",
    "        print(\"result es\", result)\n",
    "\n",
    "        # Devolver los k documentos más relevantes (top-k)\n",
    "        return result[:k]\n",
    "    \n",
    "    def calculate_idf(self):\n",
    "        N = len(self.doc_ids)\n",
    "        for term, postings in self.index.items():\n",
    "            # Número de documentos que contienen el término\n",
    "            doc_freq = len(postings)\n",
    "            # Calcular IDF y almacenar\n",
    "            self.idf[term] = math.log10(N / (1 + doc_freq))\n",
    "        # Calcular la longitud de cada documento\n",
    "        for term, postings in self.index.items():\n",
    "            for doc_id, tf in postings:\n",
    "                if doc_id not in self.length:\n",
    "                    self.length[doc_id] = 0\n",
    "                # Sumar los TF-IDF al cuadrado para la longitud del documento\n",
    "                self.length[doc_id] += (tf * self.idf[term]) ** 2\n",
    "    \n",
    "        # Tomar la raíz cuadrada para completar la longitud de cada documento\n",
    "        for doc_id in self.length.keys():\n",
    "            self.length[doc_id] = math.sqrt(self.length[doc_id])\n",
    "\n",
    "    def load_index(self):\n",
    "        self.index = {}\n",
    "        files = sorted(os.listdir(self.index_dir))\n",
    "        for file in files:\n",
    "            if file.endswith(\".pkl\"):\n",
    "                print(\"file: \",file)\n",
    "                block = self.load_block(os.path.join(self.index_dir, file))\n",
    "                # print(block)\n",
    "                # print(\"block es: \", block)\n",
    "                for term, postings in block.items():\n",
    "                    if term not in self.index.keys():\n",
    "                        self.index[term] = []\n",
    "                    for doc_id, tf in postings.items():\n",
    "                        self.index[term].append((doc_id, tf))\n",
    " \n",
    "    def load_block(self, filepath):\n",
    "        \"\"\" Cargar un bloque del índice desde un archivo. \"\"\"\n",
    "        with open(filepath, \"rb\") as f:\n",
    "            return pickle.load(f)\n",
    "def PreComputarNormas(dir_blocks, n_docs):\n",
    "    n_blocks = len(os.listdir(os.path.join('./',dir_blocks)))\n",
    "    norms = {}\n",
    "    for i in range(n_blocks):\n",
    "        bloque_i_path = os.path.join(dir_blocks, \"block_{}.pkl\".format(i))\n",
    "        with open(bloque_i_path, 'rb') as f:\n",
    "            bloque_i = pickle.load(f)\n",
    "        for term, poosting in bloque_i.items():\n",
    "            for doc, tf in poosting.items():\n",
    "                df = len(poosting)\n",
    "                idf = math.log10(n_docs / (1 + df))\n",
    "                if doc not in norms.keys():\n",
    "                    norms[doc] = 0\n",
    "                norms[doc] += (tf*idf)**2\n",
    "    for doc, peso in norms.items():\n",
    "        norms[doc] = math.sqrt(peso)\n",
    "    n_norm_blocks = int(math.ceil(math.log2(n_docs)))\n",
    "    norm_per_blocks = int(math.ceil(n_docs/n_norm_blocks))\n",
    "    norm_path = \"normas\"\n",
    "    norm_path = os.path.join('./',norm_path)\n",
    "    if not (os.path.exists(norm_path) and os.path.isdir(norm_path)):\n",
    "        os.mkdir(norm_path)\n",
    "    norm_block = []\n",
    "    i_block = 0\n",
    "    for i in range(1, n_docs + 1):\n",
    "        norm_block.append(norms[i])\n",
    "        if len(norm_block) == norm_per_blocks:\n",
    "            with open(os.path.join(norm_path,\"norma_block_{}.pkl\".format(i_block)), 'wb') as f:\n",
    "                pickle.dump(norm_block,f)\n",
    "            i_block += 1\n",
    "            norm_block = []\n",
    "    \n",
    "s = SPIMI()\n",
    "s.spimi_invert(pairs_df)\n",
    "n_docs = s.doc_count\n",
    "n_blocks = s.block_counter\n",
    "dir_blocks = 'index_blocks'\n",
    "OrdenarPorBloques(dir_blocks, n_blocks)\n",
    "mergeSort(dir_blocks)\n",
    "InvertirListasDiccionarios(\"index_blocks\")\n",
    "n_docs = len(dataset) - 1\n",
    "PreComputarNormas(\"index_blocks\", n_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[6.779048832121905, 27.680288053691662, 11.652914857219042, 45.419717724125256, 12.853743086710118, 13.469997320530918, 10.7071659162389, 15.003162950913401, 31.34524043387813, 27.525959687073623, 9.404658021815637, 70.99093152570862, 19.403172075200356, 43.47641982668596, 16.19538497398011, 43.075836997866865]\n"
     ]
    }
   ],
   "source": [
    "n_norm_blocks = len(os.listdir(os.path.join('./', 'normas')))\n",
    "norms = []\n",
    "for i in range(n_norm_blocks):\n",
    "    norm_path = os.path.join('normas', \"norma_block_{}.pkl\".format(i))\n",
    "    with open(norm_path, 'rb') as f:\n",
    "        norms.extend(pickle.load(f))\n",
    "print(norms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'dict_norms' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[52], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;28mlen\u001b[39m(norms))\n\u001b[1;32m----> 2\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;28mlen\u001b[39m(\u001b[43mdict_norms\u001b[49m))\n",
      "\u001b[1;31mNameError\u001b[0m: name 'dict_norms' is not defined"
     ]
    }
   ],
   "source": [
    "print(len(norms))\n",
    "print(len(dict_norms))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(1, 18.20125930451387), (2, 6.573848212298432), (3, 20.37310078882271), (4, 11.846391359351319), (5, 45.46368402681242), (6, 12.7773815631536), (7, 12.888198670728768), (8, 10.371365148942724), (9, 15.003162950913401), (10, 24.550017443747286), (11, 27.218329688855782), (12, 9.403106652644812), (13, 68.45921039284043), (14, 15.531966583251789), (15, 31.07430350579313), (16, 11.935286708520293), (17, 43.0652126018369), (18, 19.637034026914826), (19, 24.30839387700574)]\n"
     ]
    }
   ],
   "source": [
    "dict_norms = s.length\n",
    "dict_norms = sorted(dict_norms.items(), key=lambda x: x[0], reverse=False)\n",
    "print(dict_norms)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "file:  block_0.pkl\n",
      "file:  block_1.pkl\n",
      "file:  block_2.pkl\n",
      "file:  block_3.pkl\n",
      "file:  block_4.pkl\n",
      "file:  block_5.pkl\n",
      "file:  block_6.pkl\n",
      "file:  block_7.pkl\n",
      "file:  block_8.pkl\n",
      "file:  block_9.pkl\n",
      "El index es: {'aaa': [(3, 1)], 'abrazadito': [(13, 4)], 'accordar': [(10, 2)], 'acemo': [(11, 1)], 'actin': [(14, 1)], 'action': [(16, 1)], 'actitud': [(11, 1)], 'ador': [(7, 1)], 'ain': [(7, 7), (14, 5), (17, 2)], 'air': [(7, 1), (17, 1)], 'aisl': [(16, 1)], 'ake': [(1, 1)], 'akin': [(1, 4)], 'ako': [(1, 2)], 'ala': [(11, 3)], 'alam': [(1, 2)], 'aliv': [(2, 2)], 'alla': [(10, 1)], 'alright': [(15, 4)], 'altra': [(10, 1)], 'amanec': [(13, 4)], 'amanecimo': [(15, 1)], 'amarradito': [(13, 2)], 'america': [(10, 1)], 'amo': [(10, 1)], 'amplif': [(16, 1)], 'anch': [(10, 3)], 'ando': [(15, 2)], 'anello': [(10, 1)], 'ang': [(1, 7)], 'angel': [(2, 3), (15, 1)], 'answer': [(4, 1)], 'appeal': [(14, 1)], 'appear': [(7, 1)], 'arm': [(7, 1)], 'aserej': [(10, 1)], 'askin': [(18, 3)], 'así': [(13, 2)], 'atención': [(13, 2)], 'attract': [(16, 1)], 'aunt': [(4, 1)], 'awar': [(7, 1)], 'aww': [(14, 6)], 'ay': [(1, 5), (13, 4), (15, 4)], 'aye': [(5, 1)], 'azucar': [(17, 1)], 'babi': [(4, 8), (5, 7), (7, 2), (15, 6)], 'bacano': [(11, 1)], 'bacio': [(10, 2)], 'backstag': [(16, 1)], 'bad': [(4, 1), (5, 2), (7, 1)], 'baguett': [(7, 1)], 'bailar': [(13, 2), (15, 1)], 'baina': [(11, 1)], 'bajar': [(13, 2)], 'baje': [(11, 1)], 'bajo': [(13, 2)], 'balla': [(10, 2)], 'balliamo': [(10, 1)], 'bang': [(5, 7)], 'barch': [(10, 1)], 'basterebb': [(10, 1)], 'bawat': [(1, 1)], 'bañarno': [(13, 8)], 'beat': [(15, 1)], 'beauti': [(3, 1), (4, 2), (6, 1)], 'becki': [(15, 1)], 'bed': [(7, 1)], 'bein': [(14, 2)], 'believ': [(8, 1)], 'bell': [(3, 1)], 'bella': [(15, 10)], 'bello': [(10, 2)], 'belong': [(7, 1)], 'bench': [(2, 1)], 'bend': [(7, 1)], 'bene': [(10, 2)], 'berata': [(17, 1)], 'besito': [(15, 1)], 'bewar': [(3, 1)], 'bien': [(13, 6)], 'billi': [(10, 1)], 'bit': [(16, 2)], 'bitch': [(16, 1)], 'biv': [(3, 3)], 'black': [(14, 5), (15, 1)], 'blade': [(17, 1)], 'blast': [(14, 1)], 'blind': [(3, 1)], 'blizzard': [(4, 1)], 'blow': [(3, 1), (17, 2)], 'blue': [(2, 1)], 'bocado': [(13, 2)], 'bodi': [(5, 2), (15, 1), (16, 1)], 'book': [(12, 1)], 'boquita': [(13, 18)], 'bother': [(14, 1)], 'bound': [(4, 1)], 'bout': [(5, 4), (7, 1), (14, 1), (19, 1)], 'boy': [(3, 1), (5, 4), (7, 1), (15, 2)], 'brain': [(7, 1)], 'brave': [(6, 4)], 'break': [(3, 1), (4, 1), (8, 3)], 'breakfast': [(16, 1)], 'breakin': [(3, 1)], 'breath': [(6, 1), (7, 1)], 'brian': [(16, 1)], 'broken': [(8, 1)], 'brother': [(4, 1), (14, 7)], 'brown': [(3, 1)], 'brum': [(11, 3)], 'buck': [(14, 1)], 'buena': [(11, 1)], 'bugi': [(10, 1)], 'bumulong': [(1, 1)], 'busi': [(5, 1)], 'bust': [(14, 1)], 'butt': [(3, 2)], 'búscame': [(13, 2)], 'cab': [(4, 1)], 'caiga': [(15, 2)], 'calcio': [(10, 1)], 'cama': [(15, 2)], 'cambi': [(11, 2)], 'camera': [(18, 3)], 'campana': [(13, 2)], 'canzoni': [(10, 2)], 'car': [(7, 1)], 'care': [(2, 1), (14, 1)], 'caress': [(7, 1)], 'carri': [(6, 3)], 'carro': [(11, 1)], 'cart': [(16, 1)], 'catch': [(19, 2)], 'caught': [(4, 1)], 'caution': [(3, 1)], 'ceil': [(17, 4)], 'cervello': [(10, 1)], 'chabli': [(12, 1)], 'chang': [(14, 7)], 'chat': [(18, 1)], 'che': [(10, 10), (15, 1)], 'cheat': [(7, 1)], 'checkin': [(3, 1)], 'chest': [(3, 1), (16, 1)], 'chez': [(15, 5)], 'chicago': [(16, 1)], 'chick': [(17, 1)], 'child': [(7, 1)], 'chill': [(14, 1)], 'chissà': [(10, 2)], 'chit': [(18, 1)], 'chitarra': [(10, 2)], 'chiusi': [(10, 2)], 'choco': [(11, 1)], 'chose': [(7, 1)], 'chulo': [(17, 1)], 'cielo': [(15, 2)], 'cigarett': [(4, 1)], 'cintura': [(15, 5)], 'circl': [(18, 1)], 'clockin': [(3, 1)], 'close': [(2, 1), (14, 1), (19, 2)], 'closer': [(4, 1), (7, 1), (9, 2)], 'cloud': [(9, 4)], 'coat': [(4, 1)], 'coca': [(10, 1)], 'coil': [(7, 1)], 'cola': [(10, 1)], 'cold': [(4, 6)], 'colder': [(9, 2)], 'comin': [(14, 1)], 'como': [(11, 5), (15, 2)], 'conceal': [(14, 1)], 'confond': [(10, 1)], 'confort': [(15, 2)], 'contest': [(15, 1)], 'contigo': [(11, 2), (15, 1)], 'conto': [(10, 3)], 'contro': [(10, 1)], 'convers': [(17, 1)], 'cool': [(14, 1), (17, 1)], 'cop': [(14, 2)], 'corner': [(7, 1)], 'cos': [(10, 5)], 'cosa': [(10, 1)], 'costum': [(16, 1)], 'count': [(9, 2)], 'courag': [(7, 1)], 'crack': [(14, 4)], 'crampin': [(18, 3)], 'crazi': [(16, 1)], 'credi': [(10, 1)], 'cree': [(15, 1)], 'crew': [(3, 2)], 'crime': [(14, 1)], 'crimin': [(5, 1)], 'cross': [(12, 1)], 'cruella': [(16, 1)], 'cruis': [(18, 1)], 'crujient': [(13, 2)], 'crumbl': [(6, 1)], 'cuando': [(11, 10), (13, 2)], 'cubana': [(17, 1)], 'cuerpo': [(13, 2)], 'cumpl': [(15, 1)], 'cura': [(13, 2), (15, 4)], 'cure': [(3, 1)], 'cut': [(3, 1)], 'da': [(10, 6)], 'dai': [(10, 2)], 'dal': [(10, 1)], 'dalla': [(10, 1)], 'dame': [(15, 1)], 'damn': [(14, 1), (18, 2)], 'danc': [(2, 1), (9, 4), (12, 1), (16, 1)], 'darat': [(1, 1)], 'dark': [(14, 1)], 'darkest': [(6, 3)], 'darl': [(6, 1)], 'dart': [(11, 2)], 'day': [(6, 7), (12, 4)], 'dead': [(3, 4), (14, 1)], 'deal': [(14, 1)], 'dealin': [(7, 1)], 'deceiv': [(7, 1)], 'decis': [(8, 1)], 'dedica': [(10, 1)], 'dedicar': [(10, 6)], 'deg': [(10, 1)], 'deja': [(13, 2)], 'dejast': [(15, 4)], 'del': [(10, 1), (11, 1)], 'delici': [(4, 2)], 'dell': [(10, 3)], 'della': [(10, 1)], 'dement': [(13, 4)], 'demon': [(3, 1)], 'depth': [(6, 1)], 'descontrola': [(11, 5)], 'desea': [(11, 2)], 'desir': [(19, 1)], 'despacito': [(13, 2)], 'despair': [(6, 1), (7, 1)], 'destila': [(13, 2)], 'devic': [(16, 1)], 'devil': [(14, 1)], 'devill': [(16, 1)], 'devo': [(3, 2)], 'di': [(10, 3)], 'diablo': [(11, 1)], 'diamond': [(7, 1)], 'dice': [(15, 3)], 'diciamo': [(10, 1)], 'die': [(4, 1), (12, 1)], 'dient': [(13, 2)], 'diferent': [(15, 1)], 'dig': [(12, 3), (16, 1), (17, 1)], 'dijist': [(15, 2)], 'dile': [(15, 2)], 'dimmi': [(10, 2)], 'dio': [(13, 8)], 'dirti': [(10, 1), (15, 1)], 'disapprov': [(6, 1)], 'disfruta': [(11, 2)], 'disgrac': [(14, 1)], 'dist': [(15, 2)], 'distanc': [(6, 1)], 'distant': [(14, 1)], 'dito': [(1, 1)], 'diversi': [(10, 1)], 'divid': [(19, 1)], 'divina': [(15, 1)], 'divorc': [(16, 1)], 'dolc': [(17, 1)], 'dond': [(11, 2)], 'donn': [(10, 1)], 'door': [(4, 1)], 'doowop': [(17, 1)], 'dope': [(14, 2)], 'doubt': [(7, 1)], 'dove': [(17, 1)], 'drama': [(5, 2)], 'dreamin': [(3, 1)], 'dress': [(18, 2)], 'drink': [(4, 2), (18, 3)], 'drippin': [(5, 1)], 'drive': [(3, 1), (7, 1)], 'drivin': [(3, 2)], 'drop': [(4, 2)], 'drug': [(14, 1)], 'drum': [(5, 7)], 'dude': [(17, 1)], 'dumb': [(5, 39)], 'dura': [(10, 1), (15, 12)], 'duro': [(11, 2), (15, 6)], 'dé': [(13, 8)], 'déjà': [(10, 1)], 'día': [(13, 8)], 'díselo': [(15, 1)], 'eas': [(7, 1)], 'easi': [(14, 1)], 'east': [(14, 1)], 'eat': [(14, 1)], 'eilish': [(10, 1)], 'el': [(11, 1), (13, 16), (15, 8)], 'eleanor': [(8, 1)], 'elektra': [(16, 1)], 'ella': [(11, 8), (15, 1)], 'em': [(3, 1), (14, 5), (19, 1)], 'emi': [(16, 1)], 'empezando': [(15, 1)], 'empti': [(7, 1)], 'en': [(11, 6), (13, 12), (15, 7)], 'enamorada': [(15, 1)], 'energi': [(17, 1)], 'enseñart': [(13, 2)], 'entendido': [(15, 1)], 'entero': [(15, 1)], 'entertain': [(16, 13)], 'entrambi': [(10, 1)], 'envida': [(15, 1)], 'era': [(10, 1)], 'eras': [(14, 1)], 'erba': [(10, 1)], 'ere': [(13, 2), (15, 3)], 'esci': [(10, 1)], 'escono': [(10, 1)], 'eso': [(11, 4)], 'esperaban': [(15, 1)], 'esperando': [(15, 2)], 'estamo': [(11, 5)], 'esto': [(15, 1)], 'estoy': [(13, 2)], 'estrella': [(15, 4)], 'está': [(13, 12), (15, 1)], 'eterno': [(10, 1)], 'eto': [(11, 4)], 'even': [(4, 1)], 'evil': [(14, 1)], 'expir': [(12, 1)], 'explico': [(15, 1)], 'ey': [(15, 4)], 'eye': [(3, 1), (4, 1), (7, 1), (19, 4)], 'fa': [(10, 1), (17, 2)], 'facil': [(10, 1)], 'fade': [(7, 1)], 'fai': [(10, 1)], 'fall': [(3, 1), (6, 1), (18, 1)], 'fare': [(10, 1)], 'farlo': [(10, 1)], 'fat': [(17, 1)], 'father': [(4, 1)], 'fatta': [(10, 1)], 'fault': [(8, 1)], 'fear': [(7, 1)], 'feel': [(2, 2), (5, 1), (6, 1), (7, 4), (8, 1), (17, 8)], 'feelin': [(15, 3), (18, 1)], 'fella': [(3, 2)], 'fellow': [(3, 1)], 'femminil': [(10, 1)], 'fenc': [(2, 1)], 'ferrari': [(15, 1)], 'fiesta': [(13, 8)], 'fight': [(6, 3), (14, 1)], 'fill': [(14, 1)], 'fin': [(13, 2)], 'fina': [(15, 6)], 'finché': [(10, 2)], 'finest': [(18, 2)], 'fino': [(10, 3)], 'fireplac': [(4, 1)], 'flama': [(15, 4)], 'flame': [(12, 1)], 'flashin': [(18, 3)], 'fli': [(3, 2), (17, 28)], 'fling': [(17, 3)], 'floor': [(4, 1)], 'flow': [(13, 2)], 'follow': [(5, 1)], 'fool': [(14, 1), (17, 1)], 'forc': [(16, 1)], 'forget': [(3, 1)], 'frame': [(7, 1)], 'freez': [(3, 1), (4, 1)], 'frenar': [(13, 2)], 'freno': [(13, 2)], 'friend': [(12, 1)], 'frighten': [(6, 1)], 'fro': [(3, 1)], 'fuck': [(18, 7)], 'fue': [(13, 4), (15, 3)], 'fuego': [(11, 2), (13, 2)], 'fumando': [(15, 1)], 'funni': [(16, 1)], 'futuroo': [(11, 1)], 'gabbana': [(17, 1)], 'game': [(7, 1), (14, 1)], 'gawin': [(1, 1)], 'gent': [(10, 1), (13, 2)], 'gentlemen': [(16, 1)], 'gettin': [(3, 1)], 'gift': [(17, 4)], 'gioiello': [(10, 1)], 'girl': [(3, 7), (7, 4), (8, 1), (18, 1)], 'giuro': [(10, 1)], 'givin': [(14, 1)], 'gli': [(10, 2)], 'glisten': [(7, 1)], 'gloria': [(11, 4)], 'gon': [(4, 1), (6, 1), (7, 1), (9, 8), (12, 5), (16, 2), (18, 1)], 'goowi': [(17, 1)], 'gosh': [(4, 1)], 'grand': [(4, 1)], 'grave': [(12, 2)], 'gray': [(7, 1)], 'grip': [(5, 1)], 'grita': [(11, 1)], 'groovi': [(17, 1)], 'ground': [(16, 1)], 'groupi': [(16, 1)], 'grow': [(6, 1), (7, 2), (12, 1)], 'grown': [(6, 2), (17, 1)], 'guarda': [(10, 1)], 'gun': [(14, 1)], 'gurl': [(17, 3)], 'gusta': [(11, 2)], 'gustan': [(11, 2)], 'guy': [(19, 2)], 'ha': [(16, 4)], 'habitación': [(15, 2)], 'hacemo': [(11, 1)], 'haga': [(15, 1)], 'hago': [(11, 2)], 'hair': [(4, 1)], 'hand': [(2, 1), (4, 2)], 'hanggang': [(1, 1)], 'hangin': [(1, 1)], 'hard': [(3, 2), (6, 1)], 'harm': [(7, 1)], 'haré': [(15, 1)], 'hasta': [(13, 4), (15, 2)], 'hat': [(4, 1)], 'hate': [(14, 1)], 'hay': [(15, 3)], 'head': [(3, 2)], 'heal': [(14, 1)], 'hear': [(7, 1), (19, 12)], 'heart': [(3, 2), (7, 1), (8, 3), (9, 2), (12, 1), (19, 1)], 'heartfelt': [(7, 1)], 'heaven': [(14, 1)], 'heidi': [(10, 1)], 'hero': [(14, 1)], 'hey': [(2, 2), (4, 1), (14, 1), (16, 5)], 'hice': [(15, 2)], 'hicimo': [(15, 1)], 'hide': [(19, 1)], 'hideaway': [(9, 10)], 'high': [(3, 1)], 'highdrow': [(17, 1)], 'hindi': [(1, 2)], 'hit': [(17, 1)], 'ho': [(3, 1), (10, 2), (18, 6)], 'hoe': [(3, 1)], 'hold': [(4, 2), (6, 1), (7, 2), (8, 2), (9, 8), (14, 1), (18, 2)], 'homi': [(18, 6)], 'hoo': [(3, 1)], 'hope': [(4, 1)], 'hous': [(3, 1)], 'huey': [(14, 2)], 'huh': [(3, 2)], 'hungri': [(14, 1)], 'hunnybun': [(17, 3)], 'hunt': [(12, 1)], 'hurri': [(4, 2)], 'hurt': [(4, 1), (7, 1), (8, 1), (14, 1)], 'huski': [(10, 1)], 'ice': [(4, 1)], 'ickey': [(17, 1)], 'ihhh': [(13, 4)], 'ika': [(1, 1)], 'ikaw': [(1, 1)], 'impli': [(4, 1)], 'importa': [(10, 1)], 'impress': [(7, 1)], 'incostanti': [(10, 1)], 'indecent': [(15, 1)], 'inocent': [(15, 2)], 'iphon': [(18, 3)], 'isang': [(1, 2)], 'ita': [(13, 36)], 'italian': [(17, 1)], 'ito': [(1, 1)], 'iyong': [(1, 4)], 'ja': [(13, 8)], 'jack': [(14, 1)], 'jaja': [(15, 1)], 'jajaja': [(15, 1)], 'japanes': [(16, 1)], 'jazz': [(16, 1)], 'jealous': [(6, 1), (14, 1)], 'jet': [(7, 1)], 'johnni': [(3, 1)], 'jugueton': [(11, 2)], 'jump': [(2, 1)], 'juro': [(13, 2)], 'ka': [(1, 5)], 'kailan': [(1, 1)], 'kay': [(1, 4)], 'kaya': [(1, 2)], 'keepin': [(3, 1)], 'kick': [(17, 1)], 'kid': [(6, 1), (14, 3)], 'kill': [(14, 3)], 'killin': [(18, 1)], 'king': [(9, 4)], 'kiss': [(2, 3), (3, 2)], 'knee': [(4, 1)], 'knock': [(6, 1)], 'know': [(2, 1)], 'ko': [(1, 3)], 'kong': [(1, 1)], 'kung': [(1, 1)], 'kénsel': [(15, 1)], 'labio': [(13, 2)], 'lace': [(7, 1)], 'ladi': [(16, 1)], 'laid': [(3, 1)], 'lamig': [(1, 4)], 'lang': [(1, 3)], 'langit': [(1, 1)], 'las': [(11, 2), (15, 5)], 'latina': [(15, 3)], 'laugh': [(2, 1)], 'lay': [(3, 1), (7, 1), (14, 1)], 'le': [(10, 4), (11, 8), (15, 1)], 'learn': [(6, 1), (14, 2)], 'leather': [(17, 3)], 'leav': [(7, 1)], 'left': [(19, 1)], 'lend': [(4, 1)], 'letal': [(13, 2)], 'lick': [(17, 1)], 'lie': [(4, 1), (7, 1), (17, 1), (19, 9)], 'life': [(4, 1), (6, 4), (8, 1), (14, 1), (18, 1)], 'lift': [(17, 10)], 'light': [(7, 1), (12, 1), (16, 1)], 'lil': [(17, 1)], 'lilingon': [(1, 1)], 'limeston': [(12, 2)], 'limit': [(8, 2)], 'lip': [(2, 1), (4, 2)], 'listen': [(4, 1), (7, 1), (16, 1)], 'litti': [(5, 21)], 'live': [(6, 3), (12, 1), (14, 1)], 'livin': [(14, 1)], 'llama': [(15, 4)], 'llamada': [(15, 2)], 'llamar': [(13, 2)], 'llegaba': [(15, 2)], 'llego': [(13, 8)], 'lleva': [(11, 4)], 'llevara': [(15, 2)], 'llevo': [(11, 1)], 'llevé': [(15, 4)], 'lo': [(11, 3), (13, 4), (15, 4)], 'loca': [(11, 5)], 'locaa': [(11, 1)], 'locat': [(16, 1)], 'loco': [(11, 1)], 'lone': [(2, 1)], 'lontana': [(10, 1)], 'lookin': [(3, 1), (14, 1)], 'los': [(11, 2), (15, 1)], 'loser': [(3, 1)], 'lot': [(16, 1)], 'loui': [(12, 1)], 'love': [(2, 1), (3, 4), (6, 1), (7, 10), (8, 15), (12, 1), (14, 2), (15, 1), (17, 2), (19, 8)], 'lover': [(8, 4), (9, 4)], 'low': [(3, 1)], 'loyal': [(7, 1)], 'lucio': [(10, 1)], 'lucki': [(4, 1)], 'lujuria': [(13, 2)], 'luna': [(15, 6)], 'lust': [(19, 1)], 'luz': [(11, 1)], 'líder': [(15, 3)], 'mai': [(10, 2)], 'maiden': [(4, 1)], 'makin': [(14, 2)], 'male': [(10, 1)], 'malibu': [(10, 1)], 'maluma': [(15, 6)], 'mama': [(14, 1), (17, 1)], 'mamacita': [(15, 4)], 'mami': [(11, 2), (15, 1)], 'manbo': [(11, 1)], 'mangarap': [(1, 1)], 'mano': [(10, 1)], 'mantien': [(13, 2)], 'manzana': [(13, 4)], 'maradona': [(10, 1)], 'marat': [(1, 1)], 'mare': [(10, 1)], 'marea': [(13, 8)], 'mari': [(15, 1)], 'maschi': [(10, 1)], 'matter': [(7, 1)], 'meant': [(8, 3)], 'meet': [(3, 1)], 'meglio': [(10, 2)], 'mellow': [(3, 1)], 'melt': [(7, 1)], 'menu': [(16, 1)], 'merchandis': [(16, 1)], 'met': [(7, 1)], 'metrica': [(10, 1)], 'mga': [(1, 5)], 'mi': [(10, 5), (11, 7), (13, 2), (15, 6)], 'mia': [(10, 2)], 'michael': [(3, 1)], 'michell': [(12, 1)], 'middl': [(14, 1)], 'midnight': [(2, 1)], 'miel': [(13, 2)], 'mike': [(3, 1)], 'miley': [(10, 1)], 'mill': [(10, 3)], 'mind': [(3, 4), (4, 2), (7, 1)], 'minsan': [(1, 1)], 'mio': [(10, 1)], 'mira': [(15, 1)], 'mis': [(13, 2)], 'misplac': [(14, 1)], 'mistak': [(6, 1)], 'mma': [(7, 1)], 'mmm': [(18, 2)], 'mobil': [(14, 1)], 'moda': [(10, 1)], 'modert': [(13, 8)], 'mold': [(7, 1)], 'moment': [(7, 1), (9, 2)], 'momento': [(10, 1)], 'mon': [(16, 3)], 'money': [(3, 1), (7, 1), (17, 6), (18, 2)], 'mong': [(1, 4)], 'monto': [(15, 1)], 'mood': [(17, 1)], 'morder': [(13, 4)], 'mordidita': [(13, 36)], 'morn': [(14, 1)], 'mother': [(4, 1), (14, 1)], 'mountain': [(12, 1)], 'mouth': [(14, 1)], 'movimiento': [(15, 1)], 'movin': [(3, 1)], 'muer': [(11, 2)], 'muli': [(1, 1)], 'mundo': [(15, 1)], 'musiica': [(11, 1)], 'má': [(15, 5)], 'más': [(15, 29)], 'nabitawan': [(1, 1)], 'nada': [(5, 1), (11, 4)], 'nadi': [(11, 1)], 'nah': [(18, 3)], 'nakapagbigay': [(1, 4)], 'name': [(12, 1)], 'nang': [(1, 1)], 'napalingon': [(1, 1)], 'natur': [(13, 2), (17, 3)], 'neglect': [(7, 1)], 'negro': [(14, 1)], 'nei': [(10, 1)], 'neighbor': [(4, 1)], 'nella': [(10, 1)], 'nere': [(10, 1)], 'netflix': [(15, 1)], 'ngiti': [(1, 4)], 'nice': [(4, 2)], 'nigga': [(14, 1)], 'night': [(3, 1), (7, 1), (9, 8), (12, 2), (16, 1)], 'ninguna': [(15, 1)], 'niña': [(13, 2)], 'noch': [(11, 2), (13, 8)], 'nostr': [(10, 1)], 'nott': [(10, 1)], 'notti': [(10, 2)], 'nulla': [(10, 1)], 'nunca': [(11, 4)], 'occhi': [(10, 3)], 'odd': [(6, 1)], 'ogni': [(10, 1)], 'olvid': [(15, 1)], 'ombr': [(10, 1)], 'ond': [(10, 1)], 'ooh': [(6, 1), (15, 1), (17, 4), (18, 16)], 'oowi': [(17, 1)], 'oper': [(14, 1)], 'ore': [(10, 3)], 'orillita': [(13, 8)], 'orizzont': [(10, 1)], 'orlean': [(16, 1)], 'outrag': [(16, 1)], 'outta': [(16, 1)], 'pace': [(4, 1)], 'pack': [(14, 1)], 'paid': [(3, 1), (14, 1)], 'pain': [(7, 1)], 'pal': [(11, 1)], 'pangarap': [(1, 6)], 'pansin': [(1, 2)], 'para': [(13, 6)], 'parcero': [(15, 1)], 'pare': [(11, 2)], 'parec': [(13, 2)], 'park': [(2, 1), (3, 1)], 'parol': [(10, 2)], 'part': [(10, 1)], 'parti': [(5, 5), (10, 3), (15, 1)], 'pass': [(16, 1)], 'passin': [(18, 3)], 'pastel': [(13, 2)], 'payback': [(14, 1)], 'peac': [(14, 1)], 'pege': [(11, 2)], 'pelé': [(10, 1)], 'penitentiari': [(14, 1)], 'pensar': [(13, 2)], 'penso': [(10, 1)], 'pentot': [(10, 1)], 'peopl': [(14, 2)], 'pequemo': [(13, 4)], 'perché': [(10, 3)], 'perdi': [(10, 3)], 'perdiamo': [(10, 1)], 'perfect': [(6, 1), (7, 1)], 'perform': [(16, 1)], 'persist': [(7, 1)], 'phone': [(14, 1)], 'pica': [(13, 12)], 'piec': [(16, 1)], 'piel': [(13, 2)], 'pijama': [(15, 1)], 'pill': [(16, 1)], 'pimp': [(14, 1)], 'più': [(10, 6)], 'plane': [(17, 1)], 'plant': [(7, 1)], 'play': [(3, 1), (7, 1), (9, 4), (12, 1), (14, 1)], 'playa': [(17, 1)], 'pleasur': [(16, 1)], 'plenti': [(4, 1)], 'pneumonia': [(4, 1)], 'poetica': [(10, 1)], 'poi': [(10, 5)], 'poison': [(3, 25)], 'polemica': [(10, 1)], 'polic': [(14, 1)], 'polit': [(17, 1)], 'pone': [(13, 6)], 'poniendo': [(15, 1)], 'pool': [(17, 1)], 'poor': [(14, 1)], 'por': [(11, 1), (15, 1)], 'porqu': [(11, 2), (15, 4)], 'portion': [(3, 1)], 'posicion': [(11, 2)], 'potrei': [(10, 1)], 'pour': [(4, 1)], 'poverti': [(14, 1)], 'power': [(3, 1)], 'pray': [(7, 1)], 'precious': [(8, 1)], 'prefiero': [(15, 1)], 'presid': [(14, 1)], 'presión': [(13, 2)], 'pretti': [(15, 1), (16, 1)], 'pride': [(4, 1)], 'pro': [(3, 1)], 'probada': [(15, 3)], 'probart': [(15, 3)], 'problema': [(15, 1)], 'promis': [(7, 2), (9, 4)], 'propuesta': [(15, 1)], 'protect': [(7, 1)], 'prove': [(7, 1)], 'provo': [(10, 7)], 'pull': [(14, 1), (16, 1)], 'pum': [(11, 6)], 'pupila': [(13, 2)], 'purs': [(14, 1)], 'puso': [(13, 4)], 'quando': [(10, 7)], 'quedó': [(15, 1)], 'queen': [(7, 1), (9, 4)], 'quell': [(10, 1)], 'quelli': [(10, 1)], 'quello': [(10, 7)], 'querían': [(15, 1)], 'questa': [(10, 1)], 'questo': [(10, 2)], 'qui': [(10, 6)], 'quick': [(17, 1)], 'quier': [(11, 7), (15, 12)], 'quiero': [(13, 2)], 'qué': [(15, 10)], 'race': [(14, 1)], 'racist': [(14, 1)], 'rais': [(14, 1)], 'ralph': [(3, 1)], 'rare': [(7, 1)], 'rat': [(14, 1)], 'ratata': [(11, 5)], 'raven': [(17, 1)], 'rawhid': [(17, 3)], 'razzamatazz': [(16, 1)], 'readi': [(3, 4), (14, 1), (16, 2)], 'real': [(7, 1), (13, 2), (14, 2), (17, 2)], 'realiz': [(19, 1)], 'reason': [(6, 1), (7, 1)], 'recompensaba': [(15, 2)], 'record': [(4, 1)], 'relationship': [(3, 1)], 'repitamo': [(15, 1)], 'requisito': [(15, 1)], 'resist': [(16, 1)], 'rest': [(3, 1)], 'resta': [(10, 2)], 'reykon': [(15, 7)], 'rico': [(13, 2), (15, 1)], 'ride': [(7, 1), (17, 3)], 'ridendo': [(10, 1)], 'rigbi': [(8, 1)], 'rimanendo': [(10, 2)], 'riot': [(16, 1)], 'risvegliano': [(10, 1)], 'ritorni': [(10, 2)], 'ritorno': [(10, 1)], 'roar': [(4, 1)], 'rock': [(16, 2)], 'rockin': [(17, 1)], 'roll': [(16, 1)], 'romeo': [(15, 1)], 'ron': [(3, 1)], 'rough': [(14, 1)], 'royal': [(7, 1)], 'rule': [(7, 1)], 'rumour': [(6, 1)], 'run': [(3, 1), (12, 3)], 'runnin': [(3, 1)], 'rush': [(14, 1)], 'sabe': [(11, 1), (15, 2)], 'sack': [(17, 1)], 'sad': [(7, 1)], 'sale': [(10, 6)], 'salir': [(15, 5)], 'salvaj': [(13, 2)], 'sana': [(1, 1)], 'sandali': [(1, 2)], 'sangu': [(10, 1)], 'satisfi': [(7, 2)], 'sayin': [(3, 2)], 'sayo': [(1, 1)], 'scare': [(6, 1), (7, 1)], 'scene': [(16, 1)], 'schemin': [(3, 2)], 'scorrer': [(10, 1)], 'screamin': [(3, 1)], 'screen': [(17, 3)], 'scritt': [(10, 2)], 'scurri': [(4, 1)], 'scusi': [(10, 2)], 'sea': [(11, 2), (12, 1)], 'secondo': [(10, 2)], 'secret': [(14, 1), (19, 12)], 'seed': [(7, 1)], 'seguo': [(10, 1)], 'sei': [(10, 5)], 'sell': [(16, 1)], 'sellin': [(14, 1)], 'sellout': [(16, 1)], 'semana': [(13, 2), (15, 1)], 'sembra': [(10, 1)], 'sempr': [(10, 1)], 'sens': [(3, 1), (4, 1)], 'sensazion': [(10, 1)], 'sentido': [(11, 4)], 'sentimento': [(10, 2)], 'sentir': [(13, 2)], 'sepa': [(15, 1)], 'sere': [(10, 1)], 'seren': [(10, 1)], 'servono': [(10, 1)], 'set': [(12, 1), (17, 1)], 'sexi': [(17, 1)], 'sha': [(13, 2)], 'shake': [(3, 1)], 'shape': [(18, 1)], 'share': [(7, 1), (14, 1)], 'shift': [(17, 4)], 'shine': [(7, 1)], 'ship': [(14, 1)], 'shit': [(7, 1), (16, 1)], 'shore': [(4, 1)], 'shori': [(11, 1)], 'shot': [(14, 1)], 'shoulder': [(6, 3)], 'sicuro': [(10, 1)], 'sient': [(15, 1)], 'siento': [(11, 4)], 'siero': [(10, 1)], 'sight': [(16, 1)], 'sign': [(2, 1)], 'simpli': [(4, 1), (7, 1)], 'sin': [(13, 2), (15, 3)], 'sing': [(2, 1), (7, 1), (16, 1)], 'singl': [(6, 2)], 'sir': [(4, 1)], 'sister': [(4, 1), (7, 1), (14, 1)], 'sit': [(7, 1), (12, 1)], 'situat': [(3, 1)], 'skill': [(14, 1)], 'skull': [(12, 1)], 'sky': [(2, 1), (8, 1)], 'sleazi': [(14, 1)], 'sleep': [(19, 6)], 'slick': [(3, 2)], 'slow': [(3, 1)], 'smack': [(14, 1)], 'smile': [(2, 3), (3, 2)], 'smokin': [(14, 1)], 'snatch': [(14, 1)], 'sofa': [(7, 1)], 'soft': [(2, 2)], 'sogni': [(10, 1)], 'soil': [(7, 1)], 'sola': [(11, 5)], 'soldier': [(6, 7), (7, 1)], 'sole': [(10, 3)], 'solo': [(10, 3)], 'son': [(11, 2), (16, 1)], 'song': [(7, 1)], 'sono': [(10, 1)], 'sonó': [(13, 2)], 'sorrow': [(4, 1)], 'soul': [(7, 1)], 'sound': [(14, 1), (16, 1)], 'soy': [(11, 1), (15, 3)], 'spark': [(12, 1)], 'special': [(7, 1)], 'spell': [(4, 1)], 'spend': [(7, 1), (8, 1)], 'spiagg': [(10, 1)], 'spirit': [(7, 1)], 'spite': [(8, 1)], 'spog': [(10, 1)], 'spyderman': [(3, 1)], 'stack': [(18, 1)], 'star': [(9, 2), (17, 4)], 'starlight': [(4, 1)], 'start': [(3, 2), (4, 1), (8, 2), (14, 2)], 'stato': [(10, 1)], 'stay': [(4, 3), (7, 1), (9, 4), (14, 2)], 'stayin': [(14, 1)], 'steal': [(3, 1)], 'step': [(14, 1), (18, 4)], 'stiamo': [(10, 1)], 'stickel': [(16, 1)], 'sticki': [(17, 1)], 'stimul': [(7, 1)], 'stomach': [(14, 1)], 'stood': [(3, 1)], 'storm': [(4, 1)], 'straight': [(7, 1)], 'strang': [(3, 1)], 'stranger': [(14, 1)], 'strap': [(14, 1)], 'street': [(14, 1)], 'strengthen': [(7, 1)], 'strong': [(7, 1)], 'stumbl': [(6, 1)], 'stupid': [(5, 6)], 'style': [(16, 1), (18, 3)], 'sudar': [(13, 2)], 'sudor': [(13, 2)], 'suga': [(17, 32)], 'sugar': [(17, 5)], 'sul': [(10, 1)], 'sun': [(2, 1)], 'superfli': [(17, 1)], 'suppos': [(7, 1), (14, 1)], 'sur': [(11, 1)], 'surviv': [(14, 1)], 'suspici': [(4, 1)], 'swe': [(16, 1)], 'swear': [(6, 1)], 'sweet': [(17, 4)], 'swell': [(4, 1)], 'sé': [(15, 1)], 'sólo': [(15, 6)], 'ta': [(11, 1), (14, 7)], 'take': [(14, 2)], 'takin': [(3, 1)], 'talk': [(4, 1), (19, 6)], 'talkin': [(19, 18)], 'tanong': [(1, 1)], 'tanqu': [(11, 1)], 'tat': [(14, 4)], 'te': [(10, 3), (11, 1), (13, 10), (15, 12)], 'tear': [(6, 1)], 'telegram': [(10, 1)], 'tellin': [(7, 1)], 'tempo': [(10, 1)], 'tender': [(8, 8)], 'tensión': [(13, 4)], 'tenso': [(15, 1)], 'tequila': [(13, 2)], 'thang': [(17, 1)], 'think': [(12, 6)], 'thrill': [(4, 1), (16, 1)], 'throne': [(12, 1)], 'throw': [(18, 1)], 'ti': [(10, 11), (15, 2)], 'tien': [(11, 1), (13, 2), (15, 1)], 'tiffani': [(16, 1)], 'tight': [(17, 1)], 'tiguer': [(11, 2)], 'time': [(3, 1), (6, 1), (7, 3), (8, 2), (14, 4), (16, 1), (17, 1), (18, 2), (19, 4)], 'tingin': [(1, 1)], 'tinig': [(1, 4)], 'tire': [(12, 1), (14, 1)], 'toda': [(11, 2), (13, 8), (15, 1)], 'todito': [(13, 8)], 'todo': [(15, 3)], 'told': [(7, 1), (19, 2)], 'tom': [(15, 4)], 'tomorrow': [(4, 1)], 'tone': [(17, 1)], 'tongu': [(18, 2)], 'tonight': [(14, 1)], 'tool': [(14, 1)], 'totoo': [(1, 1)], 'touch': [(4, 1), (7, 1), (14, 1)], 'tour': [(16, 1)], 'toy': [(7, 1)], 'tra': [(10, 2)], 'track': [(18, 1)], 'traje': [(13, 2)], 'trama': [(15, 1)], 'treat': [(7, 1), (14, 1), (17, 1)], 'tree': [(2, 2)], 'tremenda': [(15, 4)], 'tri': [(7, 1)], 'tribù': [(10, 1)], 'trigger': [(14, 1)], 'tropic': [(4, 1)], 'troppo': [(10, 1)], 'trovarl': [(10, 1)], 'trovato': [(10, 1)], 'true': [(7, 2), (16, 1)], 'trust': [(3, 2), (14, 1)], 'tryna': [(7, 1)], 'tu': [(10, 2), (11, 5), (13, 22), (15, 5)], 'tum': [(11, 3)], 'tus': [(13, 4), (15, 1)], 'tutta': [(10, 1)], 'tutto': [(10, 1)], 'tutugon': [(1, 1)], 'tú': [(15, 12)], 'uh': [(3, 1), (5, 1), (15, 14), (18, 1)], 'ulit': [(1, 1)], 'ultim': [(17, 4)], 'una': [(10, 5), (11, 10), (13, 36), (15, 4)], 'understand': [(7, 1)], 'unit': [(9, 2)], 'uoh': [(11, 3)], 'vain': [(12, 1)], 'vaina': [(11, 9)], 'vale': [(10, 2)], 'vali': [(10, 1)], 'vamo': [(13, 10)], 'vampir': [(12, 1)], 'vampiro': [(13, 2)], 'vena': [(10, 1)], 'vendita': [(10, 1)], 'ver': [(13, 2)], 'verità': [(10, 1)], 'vestido': [(13, 2)], 'vibe': [(18, 1)], 'vicina': [(10, 1)], 'vicious': [(4, 1)], 'vida': [(11, 5)], 'viendo': [(15, 1)], 'viern': [(15, 1)], 'vinil': [(10, 1)], 'viril': [(10, 1)], 'voic': [(7, 1)], 'void': [(7, 1)], 'volevo': [(10, 6)], 'volt': [(10, 1)], 'volum': [(5, 2)], 'vos': [(15, 1)], 'voy': [(11, 2)], 'vuelto': [(15, 1)], 'wait': [(7, 6)], 'wake': [(5, 1), (14, 1)], 'wall': [(3, 1)], 'wan': [(6, 5), (7, 2), (16, 1), (18, 3)], 'war': [(14, 4)], 'warm': [(4, 1)], 'warn': [(3, 1)], 'wassup': [(3, 1)], 'wast': [(14, 1)], 'watch': [(14, 1), (17, 3)], 'wave': [(4, 1)], 'weak': [(7, 1)], 'wealth': [(7, 1)], 'wee': [(17, 4)], 'welfar': [(14, 1)], 'white': [(14, 1)], 'wild': [(5, 2)], 'wind': [(2, 1)], 'window': [(4, 1)], 'winner': [(3, 1)], 'wipe': [(19, 1)], 'wit': [(5, 1)], 'woah': [(16, 1), (18, 3)], 'woman': [(7, 2)], 'woo': [(6, 1)], 'workin': [(14, 1)], 'worri': [(4, 1), (5, 2), (7, 1), (14, 1), (17, 1), (19, 1)], 'wors': [(14, 1)], 'worth': [(6, 3), (14, 1)], 'worthwhil': [(7, 1)], 'wou': [(11, 3)], 'wow': [(15, 3)], 'wrap': [(7, 1)], 'write': [(12, 1)], 'wrong': [(3, 2), (7, 1), (17, 1)], 'wuh': [(15, 1)], 'wuoh': [(15, 3)], 'yah': [(11, 1)], 'yata': [(1, 1)], 'yeah': [(3, 3), (5, 2), (6, 7), (11, 1), (14, 6), (16, 1), (18, 32)], 'yo': [(3, 3), (11, 8), (13, 2), (15, 8), (17, 1)], 'york': [(16, 1)], 'zona': [(15, 2)], 'zone': [(17, 1)], 'è': [(10, 8)]}\n",
      "Existen  19  documentos\n",
      "query:  NA Yeah, Spyderman and Freeze in full effect Uh-huh You ready, Ron? I'm ready You ready, Biv? I'm ready, Slick, are you? Oh, yeah, break it down NA Girl, I, must (warn you) I sense something strange in my mind Situation is (serious) Let's cure it cause we're running out of time It's oh, so (beautiful) Relationships they seem from the start It's all so (deadly) When love is not together from the heart It's drivin' me out of my mind! That's why it's HARD for me to find Can't get it out of my head! Miss her, kiss her, love her(Wrong move you're dead!) That girl is (poison)...Never trust a big butt and smile That girl is (poison)..(\"POISON!!\") NA (-caution) Before I start to meet a fly girl, you know? Cause in some (portions) You'll think she's the best thing in the world She's so - (fly) She'll drive you right out of your mind And steal your heart when you're blind Beware she's schemin', she'll make you think you're dreamin' YOU'LL fall in love and you'll be screamin', demon, HOO.. Poison, deadly, movin' in slow Lookin for a mellow fellow like DeVoe Gettin paid, laid, so better lay low Schemin on house, money, and the whole show The low pro ho she'll be cut like an aaa-FRO See what you're sayin', huh, she's a winner to you But I know she's a loser (How do you know?) Me and the crew used to do her! \"POISON!\" \"POISON!\" \"POISON!\" \"POISON!\" \"POISON!\" \"POISON!\" \"POISON!\" \"POISON! \"POISON!\" \"POISON!\" \"POISON!\" \"POISON! \"POISON!\" \"POISON!\" \"POISON!\" \"POISON! I was at the park, shake, breakin and takin 'em all And that night, I played the wall Checkin' out the fellas, the highs and lows Keepin' one eye open, still clockin' the hoes There was one particular girl that stood out from the rest Poison as can be, the high power chest Michael Biv here and I'm runnin' the show Bell, Biv DeVoe ..now you know! Yo, Slick, blow.. It's drivin' me out of my mind! That's why it's HARD for me to find Can't get it out of my head! Miss her, kiss her, love her(Wrong move you're dead!) That girl is (poison)...Never trust a big butt and smile That girl is (poison)..(\"POISON!!\") Yo' fellas, that was my end of.. You know what I'm sayin', Mike? Yeah, B.B.D. in full effect Yo', wassup to Ralph T and Johnny G And I can't forget about my boy, B. Brown And the whole NE crew Poison.. NA\n",
      "terms:  ['yeah', 'spyderman', 'freez', 'uh', 'huh', 'readi', 'ron', 'readi', 'readi', 'biv', 'readi', 'slick', 'yeah', 'break', 'girl', 'warn', 'sens', 'strang', 'mind', 'situat', 'cure', 'run', 'time', 'beauti', 'relationship', 'start', 'dead', 'love', 'heart', 'drivin', 'mind', 'hard', 'head', 'kiss', 'love', 'wrong', 'dead', 'girl', 'poison', 'trust', 'butt', 'smile', 'girl', 'poison', 'poison', 'caution', 'start', 'meet', 'fli', 'girl', 'portion', 'fli', 'drive', 'mind', 'steal', 'heart', 'blind', 'bewar', 'schemin', 'dreamin', 'fall', 'love', 'screamin', 'demon', 'hoo', 'poison', 'dead', 'movin', 'slow', 'lookin', 'mellow', 'fellow', 'devo', 'gettin', 'paid', 'laid', 'lay', 'schemin', 'hous', 'money', 'pro', 'ho', 'cut', 'aaa', 'fro', 'sayin', 'huh', 'winner', 'loser', 'crew', 'poison', 'poison', 'poison', 'poison', 'poison', 'poison', 'poison', 'poison', 'poison', 'poison', 'poison', 'poison', 'poison', 'poison', 'poison', 'poison', 'park', 'shake', 'breakin', 'takin', 'em', 'night', 'play', 'wall', 'checkin', 'fella', 'high', 'low', 'keepin', 'eye', 'clockin', 'hoe', 'girl', 'stood', 'rest', 'poison', 'power', 'chest', 'michael', 'biv', 'runnin', 'bell', 'biv', 'devo', 'yo', 'slick', 'blow', 'drivin', 'mind', 'hard', 'head', 'kiss', 'love', 'wrong', 'dead', 'girl', 'poison', 'trust', 'butt', 'smile', 'girl', 'poison', 'poison', 'yo', 'fella', 'sayin', 'mike', 'yeah', 'yo', 'wassup', 'ralph', 'johnni', 'forget', 'boy', 'brown', 'crew', 'poison']\n",
      "tf_query es:  {'yeah': 3, 'spyderman': 1, 'freez': 1, 'uh': 1, 'huh': 2, 'readi': 4, 'ron': 1, 'biv': 3, 'slick': 2, 'break': 1, 'girl': 7, 'warn': 1, 'sens': 1, 'strang': 1, 'mind': 4, 'situat': 1, 'cure': 1, 'run': 1, 'time': 1, 'beauti': 1, 'relationship': 1, 'start': 2, 'dead': 4, 'love': 4, 'heart': 2, 'drivin': 2, 'hard': 2, 'head': 2, 'kiss': 2, 'wrong': 2, 'poison': 25, 'trust': 2, 'butt': 2, 'smile': 2, 'caution': 1, 'meet': 1, 'fli': 2, 'portion': 1, 'drive': 1, 'steal': 1, 'blind': 1, 'bewar': 1, 'schemin': 2, 'dreamin': 1, 'fall': 1, 'screamin': 1, 'demon': 1, 'hoo': 1, 'movin': 1, 'slow': 1, 'lookin': 1, 'mellow': 1, 'fellow': 1, 'devo': 2, 'gettin': 1, 'paid': 1, 'laid': 1, 'lay': 1, 'hous': 1, 'money': 1, 'pro': 1, 'ho': 1, 'cut': 1, 'aaa': 1, 'fro': 1, 'sayin': 2, 'winner': 1, 'loser': 1, 'crew': 2, 'park': 1, 'shake': 1, 'breakin': 1, 'takin': 1, 'em': 1, 'night': 1, 'play': 1, 'wall': 1, 'checkin': 1, 'fella': 2, 'high': 1, 'low': 1, 'keepin': 1, 'eye': 1, 'clockin': 1, 'hoe': 1, 'stood': 1, 'rest': 1, 'power': 1, 'chest': 1, 'michael': 1, 'runnin': 1, 'bell': 1, 'yo': 3, 'blow': 1, 'mike': 1, 'wassup': 1, 'ralph': 1, 'johnni': 1, 'forget': 1, 'boy': 1, 'brown': 1}\n",
      "tfidf_query es:  {'yeah': 0.16782639057229848, 'spyderman': 0.2943241326606739, 'freez': 0.24131538171067718, 'uh': 0.15069632342122066, 'huh': 0.46649271332444703, 'readi': 0.47298853525364926, 'ron': 0.2943241326606739, 'biv': 0.5886482653213478, 'slick': 0.38247583085863524, 'break': 0.20370507437121738, 'girl': 0.45208897026366196, 'warn': 0.2943241326606739, 'sens': 0.24131538171067718, 'strang': 0.2943241326606739, 'mind': 0.40525134304120436, 'situat': 0.2943241326606739, 'cure': 0.2943241326606739, 'run': 0.24131538171067718, 'time': 0.07145275545244822, 'beauti': 0.20370507437121738, 'relationship': 0.2943241326606739, 'start': 0.23884802161918203, 'dead': 0.47298853525364926, 'love': 0.11519742021070949, 'heart': 0.2069062853511706, 'drivin': 0.38247583085863524, 'hard': 0.3228649040849938, 'head': 0.38247583085863524, 'kiss': 0.3228649040849938, 'wrong': 0.27662707708369105, 'poison': 1.1342884047912456, 'trust': 0.3228649040849938, 'butt': 0.38247583085863524, 'smile': 0.3228649040849938, 'caution': 0.2943241326606739, 'meet': 0.2943241326606739, 'fli': 0.38247583085863524, 'portion': 0.2943241326606739, 'drive': 0.24131538171067718, 'steal': 0.2943241326606739, 'blind': 0.2943241326606739, 'bewar': 0.2943241326606739, 'schemin': 0.46649271332444703, 'dreamin': 0.2943241326606739, 'fall': 0.20370507437121738, 'screamin': 0.2943241326606739, 'demon': 0.2943241326606739, 'hoo': 0.2943241326606739, 'movin': 0.2943241326606739, 'slow': 0.2943241326606739, 'lookin': 0.24131538171067718, 'mellow': 0.2943241326606739, 'fellow': 0.2943241326606739, 'devo': 0.46649271332444703, 'gettin': 0.2943241326606739, 'paid': 0.24131538171067718, 'laid': 0.2943241326606739, 'lay': 0.20370507437121738, 'hous': 0.2943241326606739, 'money': 0.17453225357560578, 'pro': 0.2943241326606739, 'ho': 0.20370507437121738, 'cut': 0.2943241326606739, 'aaa': 0.2943241326606739, 'fro': 0.2943241326606739, 'sayin': 0.38247583085863524, 'winner': 0.2943241326606739, 'loser': 0.2943241326606739, 'crew': 0.38247583085863524, 'park': 0.24131538171067718, 'shake': 0.2943241326606739, 'breakin': 0.2943241326606739, 'takin': 0.2943241326606739, 'em': 0.17453225357560578, 'night': 0.15069632342122066, 'play': 0.15069632342122066, 'wall': 0.2943241326606739, 'checkin': 0.2943241326606739, 'fella': 0.38247583085863524, 'high': 0.2943241326606739, 'low': 0.2943241326606739, 'keepin': 0.2943241326606739, 'eye': 0.17453225357560578, 'clockin': 0.2943241326606739, 'hoe': 0.2943241326606739, 'stood': 0.2943241326606739, 'rest': 0.2943241326606739, 'power': 0.2943241326606739, 'chest': 0.24131538171067718, 'michael': 0.2943241326606739, 'runnin': 0.2943241326606739, 'bell': 0.2943241326606739, 'yo': 0.1953751449424479, 'blow': 0.24131538171067718, 'mike': 0.2943241326606739, 'wassup': 0.2943241326606739, 'ralph': 0.2943241326606739, 'johnni': 0.2943241326606739, 'forget': 0.2943241326606739, 'boy': 0.17453225357560578, 'brown': 0.2943241326606739}\n",
      "result es [(2, 0.885632394783562), (1, 0.0719356986071067), (16, 0.07105964094027524), (6, 0.058213740034873326), (13, 0.05177414725648684), (17, 0.04742649681267965), (7, 0.043906745878282916), (3, 0.039074770924675706), (11, 0.03022026263088757), (15, 0.025301653978586646), (8, 0.022444440838717578), (5, 0.020835982211837713), (14, 0.01782969312468778), (18, 0.010413394541608924), (10, 0.006318549274950293), (4, 0.003917827045626913), (9, 0.0034861522619269356), (12, 0.0005750044510052899), (0, 0.0)]\n",
      "Documento 2  con similitud:  0.885632394783562\n",
      "Documento 1  con similitud:  0.0719356986071067\n",
      "Documento 16  con similitud:  0.07105964094027524\n",
      "Documento 6  con similitud:  0.058213740034873326\n",
      "Documento 13  con similitud:  0.05177414725648684\n",
      "_____\n"
     ]
    }
   ],
   "source": [
    "def mostrarDocumentos(result):\n",
    "    # print(result)\n",
    "    for doc, score in result:\n",
    "        print(\"Documento\", doc, \" con similitud: \", score)\n",
    "    print(\"_____\")   \n",
    "    \n",
    "query = dataset.iloc[3,3]\n",
    "top_k = 5\n",
    "result = s.retrieval(query, top_k)\n",
    "mostrarDocumentos(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
